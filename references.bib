
@inproceedings{10.1145/1068009.1068210,
  title = {Automatic Feature Selection in Neuroevolution},
  booktitle = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
  author = {Whiteson, Shimon and Stone, Peter and Stanley, Kenneth O. and Miikkulainen, Risto and Kohl, Nate},
  year = {2005},
  series = {{{GECCO}} '05},
  pages = {1225--1232},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1068009.1068210},
  abstract = {Feature selection is the process of finding the set of inputs to a machine learning algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to automate feature selection rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously, FS-NEAT addresses the feature selection problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the feature selection task it faces is made increasingly difficult.},
  isbn = {1-59593-010-8},
  keywords = {feature selection,genetic algorithms,neural networks},
  file = {/home/disc/p.templier/Zotero/storage/5ZN5QZJS/Whiteson et al. - 2005 - Automatic feature selection in neuroevolution.pdf}
}

@article{adamsDynamicMultidrugTherapies2004,
  title = {Dynamic {{Multidrug Therapies}} for {{HIV}}: {{Optimal}} and {{STI Control Approaches}}},
  shorttitle = {Dynamic {{Multidrug Therapies}} for {{HIV}}},
  author = {Adams, B and Banks, H. and Kwon, Hee-Dae and Tran, Hien},
  year = {2004},
  month = sep,
  journal = {Mathematical biosciences and engineering : MBE},
  volume = {1},
  pages = {223--41},
  doi = {10.3934/mbe.2004.1.223},
  abstract = {We formulate a dynamic mathematical model that describes the interaction of the immune system with the human immunodeficiency virus (HIV) and that permits drug "cocktail " therapies. We derive HIV therapeutic strategies by formulating and analyzing an optimal control problem using two types of dynamic treatments representing reverse transcriptase (RT) in hibitors and protease inhibitors (PIs). Continuous optimal therapies are found by solving the corresponding optimality systems. In addition, using ideas from dynamic programming, we formulate and derive suboptimal structured treatment interruptions (STI)in antiviral therapy that include drug-free periods of immune-mediated control of HIV. Our numerical results support a scenario in which STI therapies can lead to long-term control of HIV by the immune response system after discontinuation of therapy.},
  file = {/home/disc/p.templier/Zotero/storage/QWAC2LIJ/Adams et al. - 2004 - Dynamic Multidrug Therapies for HIV Optimal and S.pdf}
}

@article{ahmadizarArtificialNeuralNetwork2014,
  title = {Artificial Neural Network Development by Means of a Novel Combination of Grammatical Evolution and Genetic Algorithm},
  author = {Ahmadizar, Fardin and Soltanian, Khabat and AkhlaghianTab, Fardin and Tsoulos, Ioannis},
  year = {2014},
  month = nov,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {39},
  pages = {1--13},
  publisher = {{Pergamon}},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2014.11.003},
  abstract = {The most important problems with exploiting artificial neural networks (ANNs) are to design the network topology, which usually requires an excessive \ldots},
  langid = {english},
  keywords = {Adaptive penalty approach,Classification problems,Genetic algorithm,Grammatical evolution,Neural networks},
  file = {/home/disc/p.templier/Zotero/storage/GEKZBRE2/Ahmadizar et al. - 2015 - Artificial neural network development by means of .pdf;/home/disc/p.templier/Zotero/storage/F75U39VX/S0952197614002759.html;/home/disc/p.templier/Zotero/storage/SJLMZDID/S0952197614002759.html}
}

@article{akimotoAnalysisRuntimeOptimization2015,
  title = {Analysis of Runtime of Optimization Algorithms for Noisy Functions over Discrete Codomains},
  author = {Akimoto, Youhei and {Astete-Morales}, Sandra and Teytaud, Olivier},
  year = {2015},
  month = nov,
  journal = {Theoretical Computer Science},
  volume = {605},
  pages = {42--50},
  issn = {03043975},
  doi = {10.1016/j.tcs.2015.04.008},
  abstract = {We consider in this work the application of optimization algorithms to problems over discrete codomains corrupted by additive unbiased noise. We propose a modification of the algorithms by repeating the fitness evaluation of the noisy function sufficiently so that, with a fix probability, the function evaluation on the noisy case is identical to the true value.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/5CXZ4FDC/Akimoto et al. - 2015 - Analysis of runtime of optimization algorithms for.pdf}
}

@article{akimotoTheoreticalFoundationCMAES2012,
  title = {Theoretical {{Foundation}} for {{CMA-ES}} from {{Information Geometry Perspective}}},
  author = {Akimoto, Youhei and Nagata, Yuichi and Ono, Isao and Kobayashi, Shigenobu},
  year = {2012},
  month = dec,
  journal = {Algorithmica},
  volume = {64},
  number = {4},
  pages = {698--716},
  issn = {0178-4617, 1432-0541},
  doi = {10.1007/s00453-011-9564-8},
  abstract = {This paper explores the theoretical basis of the covariance matrix adaptation evolution strategy (CMA-ES) from the information geometry viewpoint. To establish a theoretical foundation for the CMA-ES, we focus on a geometric structure of a Riemannian manifold of probability distributions equipped with the Fisher metric. We define a function on the manifold which is the expectation of fitness over the sampling distribution, and regard the goal of update of the parameters of sampling distribution in the CMA-ES as maximization of the expected fitness. We investigate the steepest ascent learning for the expected fitness maximization, where the steepest ascent direction is given by the natural gradient, which is the product of the inverse of the Fisher information matrix and the conventional gradient of the function.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/7UPGHGCD/Akimoto et al. - 2012 - Theoretical Foundation for CMA-ES from Information.pdf}
}

@article{akroutDeepLearningWeight2019,
  title = {Deep {{Learning}} without {{Weight Transport}}},
  author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C. and Lillicrap, Timothy and Tweed, Douglas},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.05391 [cs, stat]},
  eprint = {1904.05391},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms - a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 - both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring.Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/LIEC8MYQ/Akrout et al. - 2019 - Deep Learning without Weight Transport.pdf;/home/disc/p.templier/Zotero/storage/BYDD38PT/1904.html}
}

@article{albanieStoppingGANViolence2017,
  title = {Stopping {{GAN Violence}}: {{Generative Unadversarial Networks}}},
  shorttitle = {Stopping {{GAN Violence}}},
  author = {Albanie, Samuel and Ehrhardt, S{\'e}bastien and Henriques, Jo{\~a}o F.},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.02528 [cs, stat]},
  eprint = {1703.02528},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (NoN) violence popularised by Generative Adversarial Networks have yet to be addressed. In this work, we quantify the financial, social, spiritual, cultural, grammatical and dermatological impact of this aggression and address the issue by proposing a more peaceful approach which we term Generative Unadversarial Networks (GUNs). Under this framework, we simultaneously train two models: a generator G that does its best to capture whichever data distribution it feels it can manage, and a motivator M that helps G to achieve its dream. Fighting is strictly verboten and both models evolve by learning to respect their differences. The framework is both theoretically and electrically grounded in game theory, and can be viewed as a winner-shares-all two-player game in which both players work as a team to achieve the best score. Experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. Our work builds on a rich history of carefully argued position-papers, published as anonymous YouTube comments, which prove that the optimal solution to NoN violence is more GUNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/FUZYIXAE/Albanie et al. - 2017 - Stopping GAN Violence Generative Unadversarial Ne.pdf;/home/disc/p.templier/Zotero/storage/5V8U2WGJ/1703.html}
}

@book{aliContemporaryChallengesSolutions2013,
  title = {Contemporary {{Challenges}} and {{Solutions}} in {{Applied Artificial Intelligence}}},
  editor = {Ali, Moonis and Bosse, Tibor and Hindriks, Koen V. and Hoogendoorn, Mark and Jonker, Catholijn M. and Treur, Jan},
  year = {2013},
  series = {Studies in {{Computational Intelligence}}},
  volume = {489},
  publisher = {{Springer International Publishing}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-319-00651-2},
  isbn = {978-3-319-00650-5 978-3-319-00651-2},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/4BLUPVXE/Ali et al. - 2013 - Contemporary Challenges and Solutions in Applied A.pdf}
}

@article{antoniouMetametalearningNeuralArchitecture,
  title = {Meta-Meta-Learning for {{Neural Architecture Search}} through {{arXiv Descent}}},
  author = {Antoniou, Antreas and Pawlowski, Nick and Turner, Jack and Owers, James and Mellor, Joseph and Crowley, Elliot J},
  pages = {4},
  abstract = {Recent work in meta-learning has set the deep learning community alight. From minute gains on few-shot learning tasks, to discovering architectures that are slightly better than chance, to solving intelligence itself1, meta-learning is proving a popular solution to every conceivable problem ever conceivably conceived ever. In this paper we venture deeper into the computational insanity that is meta-learning, and potentially risk exiting the simulation of reality itself, by attempting to metalearn at a third learning level. We showcase the resulting approach\textemdash which we call meta-meta-learning\textemdash for neural architecture search. Crucially, instead of meta-learning a neural architecture differentiably as in DARTS (Liu et al., 2018) we meta-meta-learn an architecture by searching through arXiv. This arXiv descent is GPU-free and only requires a handful of graduate students. Further, we introduce a regulariser, called college-dropout, which works by randomly removing a single graduate student from our system. As a consequence, procrastination levels decrease significantly, due to the increased workload and sense of responsibility each student attains. The code for our experiments is publicly available at . Edit: we have decided not to release our code as we are concerned that it may be used for malicious purposes.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/7KVK7KCV/Antoniou et al. - Meta-meta-learning for Neural Architecture Search .pdf}
}

@article{arnoldComparisonEvolutionStrategies,
  title = {A {{Comparison}} of {{Evolution Strategies}} with {{Other Direct Search Methods}} in the {{Presence}} of {{Noise}}},
  author = {Arnold, Dirk V},
  pages = {25},
  abstract = {Evolution strategies are general, nature-inspired heuristics for search and optimization. Due to their use of populations of candidate solutions and their advanced adaptation schemes, there is a common belief that evolution strategies are especially useful for optimization in the presence of noise. Empirical evidence as well as a number of theoretical findings with respect to the performance of evolution strategies on a class of spherical objective functions disturbed by Gaussian noise support that belief. However, little is known with respect to the capabilities in the presence of noise of evolution strategies relative to those of other direct optimization strategies. In the present paper, theoretical results with respect to the performance of evolution strategies in the presence of Gaussian noise are summarized and discussed. Then, the performance of evolution strategies is compared empirically with that of several other direct optimization strategies in the noisy, spherical environment that the theoretical results have been obtained in. Due to the simplicity of that environment, the results are easily interpretable and can serve to reveal the respective strengths and weaknesses of the algorithms. It is seen that for low levels of noise, most of the strategies exhibit similar degrees of efficiency. For higher levels of noise, their step length adaptation scheme affords evolution strategies a greater degree of robustness than the other algorithms tested.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/8EQUJFA2/Arnold - A Comparison of Evolution Strategies with Other Di.pdf}
}

@article{article,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Christopher and Guez, Arthur and Sifre, Laurent and Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  pages = {484--489},
  doi = {10.1038/nature16961}
}

@article{arulkumaranAlphaStarEvolutionaryComputation2019,
  title = {{{AlphaStar}}: {{An Evolutionary Computation Perspective}}},
  shorttitle = {{{AlphaStar}}},
  author = {Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
  year = {2019},
  month = jul,
  journal = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  eprint = {1902.01724},
  eprinttype = {arxiv},
  pages = {314--315},
  doi = {10.1145/3319619.3321894},
  abstract = {In January 2019, DeepMind revealed AlphaStar to the world\textemdash the first artificial intelligence (AI) system to beat a professional player at the game of StarCraft II\textemdash representing a milestone in the progress of AI. AlphaStar draws on many areas of AI research, including deep learning, reinforcement learning, game theory, and evolutionary computation (EC). In this paper we analyze AlphaStar primarily through the lens of EC, presenting a new look at the system and relating it to many concepts in the field. We highlight some of its most interesting aspects\textemdash the use of Lamarckian evolution, competitive co-evolution, and quality diversity. In doing so, we hope to provide a bridge between the wider EC community and one of the most significant AI systems developed in recent times.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/FYFY3LZ8/Arulkumaran et al. - 2019 - AlphaStar An Evolutionary Computation Perspective.pdf}
}

@article{auerFinitetimeAnalysisMultiarmed,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo},
  pages = {22},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/M8C3DQG9/Auer et Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf}
}

@inproceedings{augerEvolutionStrategiesRelated2008,
  title = {Evolution Strategies and Related Estimation of Distribution Algorithms},
  booktitle = {Proceedings of the 2008 {{GECCO}} Conference Companion on {{Genetic}} and Evolutionary Computation - {{GECCO}} '08},
  author = {Auger, Anne and Hansen, Nikolaus},
  year = {2008},
  pages = {2727},
  publisher = {{ACM Press}},
  address = {{Atlanta, GA, USA}},
  doi = {10.1145/1388969.1389076},
  isbn = {978-1-60558-131-6},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/H5LSD7KV/Auger et Hansen - 2008 - Evolution strategies and related estimation of dis.pdf}
}

@inproceedings{augerRestartCMAEvolution2005,
  title = {A {{Restart CMA Evolution Strategy With Increasing Population Size}}},
  booktitle = {2005 {{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {Auger, A. and Hansen, N.},
  year = {2005},
  volume = {2},
  pages = {1769--1776},
  publisher = {{IEEE}},
  address = {{Edinburgh, Scotland, UK}},
  doi = {10.1109/CEC.2005.1554902},
  abstract = {In this paper we introduce a restart-CMAevolution strategy, where the population size is increased for each restart (IPOP). By increasing the population size the search characteristic becomes more global after each restart. The IPOP-CMA-ES is evaluated on the test suit of 25 functions designed for the special session on real-parameter optimization of CEC 2005. Its performance is compared to a local restart strategy with constant small population size. On unimodal functions the performance is similar. On multi-modal functions the local restart strategy significantly outperforms IPOP in 4 test cases whereas IPOP performs significantly better in 29 out of 60 tested cases.},
  isbn = {978-0-7803-9363-9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/5LLH3X2L/Auger et Hansen - 2005 - A Restart CMA Evolution Strategy With Increasing P.pdf}
}

@article{badiaAgent57OutperformingAtari2020,
  title = {Agent57: {{Outperforming}} the {{Atari Human Benchmark}}},
  shorttitle = {Agent57},
  author = {Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.13350 [cs, stat]},
  eprint = {2003.13350},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/4H3YPJGE/Badia et al. - 2020 - Agent57 Outperforming the Atari Human Benchmark.pdf;/home/disc/p.templier/Zotero/storage/VB4CRTX4/Badia et al. - 2020 - Agent57 Outperforming the Atari Human Benchmark.pdf;/home/disc/p.templier/Zotero/storage/ENLF7IHV/2003.html}
}

@inproceedings{baierNoveltyMCTS2021,
  title = {Novelty and {{MCTS}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Baier, Hendrik and Kaisers, Michael},
  year = {2021},
  month = jul,
  series = {{{GECCO}} '21},
  pages = {1483--1487},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3449726.3463217},
  abstract = {Novelty search has become a popular technique in different fields such as evolutionary computing, classical AI planning, and deep reinforcement learning. Searching for novelty instead of, or in addition to, directly maximizing the search objective, aims at avoiding dead ends and local minima, and overall improving exploration. We propose and test the integration of novelty into Monte Carlo Tree Search (MCTS), a state-of-the-art framework for online RL planning, by linearly combining value estimates with novelty scores during the selection phase of MCTS. Three different novelty measures are adapted from the literature, integrated into MCTS, and tested in four different board games. The initial results are promising and point towards potential for novelty as "online generalization for uncertainty" in more challenging search settings.},
  isbn = {978-1-4503-8351-6},
  keywords = {game tree search,Monte Carlo tree search,novelty,novelty search},
  file = {/home/disc/p.templier/Zotero/storage/IYWLQPZV/Baier et Kaisers - 2021 - Novelty and MCTS.pdf}
}

@book{bakerkarlWayGo,
  title = {The {{Way}} to {{Go}}},
  author = {Baker, Karl},
  file = {/home/disc/p.templier/Zotero/storage/9Y8Z3J8D/W2Go8x11.pdf}
}

@article{baldominosEvolutionaryConvolutionalNeural2018,
  title = {Evolutionary {{Convolutional Neural Networks}}: An {{Application}} to {{Handwriting Recognition}}},
  shorttitle = {Evolutionary {{Convolutional Neural Networks}}},
  author = {Baldominos, Alejandro and S{\'a}ez, Yago and Isasi, Pedro},
  year = {2018},
  month = mar,
  journal = {Neurocomputing},
  volume = {283},
  pages = {38},
  doi = {10.1016/j.neucom.2017.12.049},
  abstract = {Convolutional neural networks (CNNs) have been used over the past years to solve many different artificial intelligence (AI) problems, providing significant advances in some domains and leading to state-of-the-art results. However, the topologies of CNNs involve many different parameters, and in most cases, their design remains a manual process that involves effort and a significant amount of trial and error.In this work, we have explored the application of neuroevolution to the automatic design of CNN topologies, introducing a common framework for this task and developing two novel solutions based on genetic algorithms and grammatical evolution. We have evaluated our proposal using the MNIST dataset for handwritten digit recognition, achieving a result that is highly competitive with the state-of-the-art without any kind of data augmentation or preprocessing. When misclassified samples are carefully observed, it is found that most of them involve handwritten digits that are difficult to recognize even by a human.},
  file = {/home/disc/p.templier/Zotero/storage/VYRCV675/Baldominos et al. - 2018 - Evolutionary Convolutional Neural Networks an App.pdf}
}

@incollection{baluja1995removing,
  title = {Removing the Genetics from the Standard Genetic Algorithm},
  booktitle = {Proceedings of the \#1 International Conference on Machine Learning ({{ICML}} \#2){{12th1995}}},
  author = {Baluja, Shumeet and Caruana, Rich},
  year = {1995},
  pages = {38--46},
  publisher = {{Elsevier}}
}

@incollection{Banzhaf2003,
  title = {Artificial Regulatory Networks and Genetic Programming},
  booktitle = {Genetic Programming Theory and Practice},
  author = {Banzhaf, W.},
  editor = {Riolo, Rick and Worzel, Bill},
  year = {2003},
  pages = {43--61},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8983-3_4},
  abstract = {An artificial regulatory network able to reproduce a number of phenomena found in natural genetic regulatory networks (such as heterochrony, evolution, stability and variety of network behavior) is proposed. The connection to a new genetic representation for Genetic Programming is outlined.},
  isbn = {978-1-4419-8983-3},
  file = {/home/disc/p.templier/Zotero/storage/ZAW5Z43C/4_String GRN.pdf}
}

@inproceedings{barrAutomatedSoftwareTransplantation2015,
  title = {Automated Software Transplantation},
  booktitle = {Proceedings of the 2015 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Barr, Earl T. and Harman, Mark and Jia, Yue and Marginean, Alexandru and Petke, Justyna},
  year = {2015},
  month = jul,
  pages = {257--269},
  publisher = {{ACM}},
  address = {{Baltimore MD USA}},
  doi = {10.1145/2771783.2771796},
  abstract = {Automated transplantation would open many exciting avenues for software development: suppose we could autotransplant code from one system into another, entirely unrelated, system. This paper introduces a theory, an algorithm, and a tool that achieve this. Leveraging lightweight annotation, program analysis identifies an organ (interesting behavior to transplant); testing validates that the organ exhibits the desired behavior during its extraction and after its implantation into a host. While we do not claim automated transplantation is now a solved problem, our results are encouraging: we report that in 12 of 15 experiments, involving 5 donors and 3 hosts (all popular real-world systems), we successfully autotransplanted new functionality and passed all regression tests. Autotransplantation is also already useful: in 26 hours computation time we successfully autotransplanted the H.264 video encoding functionality from the x264 system to the VLC media player; compare this to upgrading x264 within VLC, a task that we estimate, from VLC's version history, took human programmers an average of 20 days of elapsed, as opposed to dedicated, time.},
  isbn = {978-1-4503-3620-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/2YVMHK7W/Barr et al. - 2015 - Automated software transplantation.pdf}
}

@article{bastaniEfficientTargetedCOVID192021,
  title = {Efficient and Targeted {{COVID-19}} Border Testing via Reinforcement Learning},
  author = {Bastani, Hamsa and Drakopoulos, Kimon and Gupta, Vishal and Vlachogiannis, Ioannis and Hadjicristodoulou, Christos and Lagiou, Pagona and Magiorkinis, Gkikas and Paraskevis, Dimitrios and Tsiodras, Sotirios},
  year = {2021},
  month = nov,
  journal = {Nature},
  volume = {599},
  number = {7883},
  pages = {108--113},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04014-z},
  abstract = {Throughout the coronavirus disease 2019 (COVID-19) pandemic, countries have~relied on a variety of ad hoc border control protocols to allow for non-essential travel while safeguarding public health, from quarantining all travellers to restricting entry from select nations on the basis of population-level epidemiological metrics such as cases, deaths or testing positivity rates1,2. Here we report the design and performance of a reinforcement learning system, nicknamed Eva. In the summer of 2020, Eva was deployed across all Greek borders to limit the influx of asymptomatic travellers infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and to inform border policies through real-time estimates of COVID-19 prevalence. In contrast to country-wide protocols, Eva allocated Greece's limited testing resources on the basis of incoming travellers' demographic information and testing results from previous travellers. By comparing Eva's performance against modelled counterfactual scenarios, we show that Eva identified 1.85 times as many asymptomatic, infected travellers as random surveillance testing, with up to 2\textendash 4 times as many during peak travel, and 1.25\textendash 1.45 times as many asymptomatic, infected travellers as testing policies that utilize only epidemiological metrics. We demonstrate that this latter benefit arises, at least partially, because population-level epidemiological metrics had limited predictive value for the actual prevalence of SARS-CoV-2 among asymptomatic travellers and exhibited strong country-specific idiosyncrasies in the summer of 2020. Our results raise serious concerns on the effectiveness of country-agnostic internationally proposed border control policies3 that are based on population-level epidemiological metrics. Instead, our work represents a successful example of the potential of reinforcement learning and real-time data for safeguarding public health.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Public health,SARS-CoV-2,Statistics},
  file = {/home/disc/p.templier/Zotero/storage/9YIU5A47/Bastani et al. - 2021 - Efficient and targeted COVID-19 border testing via.pdf;/home/disc/p.templier/Zotero/storage/PJW2NVIK/s41586-021-04014-z.html}
}

@article{beaulieuContinualLearningDomain2021,
  title = {Continual Learning under Domain Transfer with Sparse Synaptic Bursting},
  author = {Beaulieu, Shawn L. and Clune, Jeff and Cheney, Nick},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.12056 [cs]},
  eprint = {2108.12056},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning, and retaining, new information without repeated exposure to it. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using task-specific modules with constrained circumstances of application. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is accomplished by regulating the activity of weights in a convolutional neural network on the basis of inputs using top-down modulation generated by a second feed-forward neural network. We find that our method learns continually under domain transfer with sparse bursts of activity in weights that are recycled across tasks, rather than by maintaining task-specific modules. Sparse synaptic bursting is found to balance enhanced and diminished activity in a way that facilitates adaptation to new inputs without corrupting previously acquired functions. This behavior emerges during a prior meta-learning phase in which regulated synapses are selectively disinhibited, or grown, from an initial state of uniform suppression.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/TV8UQTR3/Beaulieu et al. - 2021 - Continual learning under domain transfer with spar.pdf}
}

@article{beaulieuLearningContinuallyLearn2020,
  title = {Learning to {{Continually Learn}}},
  author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
  year = {2020},
  month = mar,
  journal = {arXiv:2002.09571 [cs, stat]},
  eprint = {2002.09571},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables contextdependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/4XVD8JZR/Beaulieu et al. - 2020 - Learning to Continually Learn.pdf}
}

@article{beeching2021godotrlagents,
  title = {Godot Reinforcement Learning Agents},
  author = {Beeching, Edward and Dibangoye, Jilles and Simonin, Olivier and Wolf, Christian},
  journal = {arXiv preprint arXiv:2112.03636., year = 2021,}
}

@article{bellemareArcadeLearningEnvironment2012,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  year = {2012},
  journal = {CoRR},
  volume = {abs/1207.4708},
  annotation = {\_eprint: 1207.4708}
}

@article{bellmanMarkovianDecisionProcess1957,
  title = {A {{Markovian Decision Process}}},
  author = {Bellman, Richard},
  year = {1957},
  journal = {Indiana University Mathematics Journal},
  volume = {6},
  number = {4},
  pages = {679--684},
  issn = {0022-2518},
  file = {/home/disc/p.templier/Zotero/storage/T2FKXHQZ/2021 - A Markovian Decision Process.pdf}
}

@article{bertoinLocalFeatureSwapping2022,
  title = {Local Feature Swapping for Generalization in Reinforcement Learning},
  author = {Bertoin, David and Rachelson, Emmanuel},
  year = {2022},
  journal = {ICLR},
  pages = {25},
  abstract = {Over the past few years, the acceleration of computing resources and research in deep learning has led to significant practical successes in a range of tasks, including in particular in computer vision. Building on these advances, reinforcement learning has also seen a leap forward with the emergence of agents capable of making decisions directly from visual observations. Despite these successes, the over-parametrization of neural architectures leads to memorization of the data used during training and thus to a lack of generalization. Reinforcement learning agents based on visual inputs also suffer from this phenomenon by erroneously correlating rewards with unrelated visual features such as background elements. To alleviate this problem, we introduce a new regularization technique consisting of channel-consistent local permutations (CLOP) of the feature maps. The proposed permutations induce robustness to spatial correlations and help prevent overfitting behaviors in RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained with the CLOP method exhibit robustness to visual changes and better generalization properties than agents trained using other state-of-the-art regularization techniques. We also demonstrate the effectiveness of CLOP as a general regularization technique in supervised learning.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/EFQ7GMRK/Bertoin et Rachelson - 2022 - LOCAL FEATURE SWAPPING FOR GENERALIZATION IN REINF.pdf}
}

@article{beyerComprehensiveIntroduction,
  title = {A Comprehensive Introduction},
  author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
  pages = {50},
  abstract = {This article gives a comprehensive introduction into one of the main branches of evolutionary computation \textendash{} the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background is explained in order to make understandable why ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/44CF2T3I/Beyer et Schwefel - A comprehensive introduction.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  doi = {10.1137/141000671},
  annotation = {\_eprint: https://doi.org/10.1137/141000671}
}

@article{bhatiaEvolutionGymLargeScale2022,
  title = {Evolution {{Gym}}: {{A Large-Scale Benchmark}} for {{Evolving Soft Robots}}},
  shorttitle = {Evolution {{Gym}}},
  author = {Bhatia, Jagdeep Singh and Jackson, Holly and Tian, Yunsheng and Xu, Jie and Matusik, Wojciech},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.09863 [cs]},
  eprint = {2201.09863},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Both the design and control of a robot play equally important roles in its task performance. However, while optimal control is well studied in the machine learning and robotics community, less attention is placed on finding the optimal robot design. This is mainly because co-optimizing design and control in robotics is characterized as a challenging problem, and more importantly, a comprehensive evaluation benchmark for co-optimization does not exist. In this paper, we propose Evolution Gym, the first large-scale benchmark for co-optimizing the design and control of soft robots. In our benchmark, each robot is composed of different types of voxels (e.g., soft, rigid, actuators), resulting in a modular and expressive robot design space. Our benchmark environments span a wide range of tasks, including locomotion on various types of terrains and manipulation. Furthermore, we develop several robot co-evolution algorithms by combining state-of-the-art design optimization methods and deep reinforcement learning techniques. Evaluating the algorithms on our benchmark platform, we observe robots exhibiting increasingly complex behaviors as evolution progresses, with the best evolved designs solving many of our proposed tasks. Additionally, even though robot designs are evolved autonomously from scratch without prior knowledge, they often grow to resemble existing natural creatures while outperforming hand-designed robots. Nevertheless, all tested algorithms fail to find robots that succeed in our hardest environments. This suggests that more advanced algorithms are required to explore the high-dimensional design space and evolve increasingly intelligent robots \textendash{} an area of research in which we hope Evolution Gym will accelerate progress. Our website with code, environments, documentation, and tutorials is available at http://evogym.csail.mit.edu.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/disc/p.templier/Zotero/storage/SCXYJU2C/Bhatia et al. - 2022 - Evolution Gym A Large-Scale Benchmark for Evolvin.pdf}
}

@article{bodnarProximalDistilledEvolutionary2020,
  title = {Proximal {{Distilled Evolutionary Reinforcement Learning}}},
  author = {Bodnar, Cristian and Day, Ben and Li{\'o}, Pietro},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {3283--3290},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.5728},
  abstract = {Reinforcement Learning (RL) has achieved impressive performance in many complex environments due to the integration with Deep Neural Networks (DNNs). At the same time, Genetic Algorithms (GAs), often seen as a competing approach to RL, had limited success in scaling up to the DNNs required to solve challenging tasks. Contrary to this dichotomic view, in the physical world, evolution and learning are complementary processes that continuously interact. The recently proposed Evolutionary Reinforcement Learning (ERL) framework has demonstrated mutual benefits to performance when combining the two methods. However, ERL has not fully addressed the scalability problem of GAs. In this paper, we show that this problem is rooted in an unfortunate combination of a simple genetic encoding for DNNs and the use of traditional biologically-inspired variation operators. When applied to these encodings, the standard operators are destructive and cause catastrophic forgetting of the traits the networks acquired. We propose a novel algorithm called Proximal Distilled Evolutionary Reinforcement Learning (PDERL) that is characterised by a hierarchical integration between evolution and learning. The main innovation of PDERL is the use of learning-based variation operators that compensate for the simplicity of the genetic representation. Unlike traditional operators, our proposals meet the functional requirements of variation operators when applied on directly-encoded DNNs. We evaluate PDERL in five robot locomotion settings from the OpenAI gym. Our method outperforms ERL, as well as two state-of-the-art RL algorithms, PPO and TD3, in all tested environments.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/XCM9VGI7/Bodnar et al. - 2020 - Proximal Distilled Evolutionary Reinforcement Lear.pdf}
}

@incollection{botevCrossEntropyMethodOptimization2013,
  title = {The {{Cross-Entropy Method}} for {{Optimization}}},
  booktitle = {Handbook of {{Statistics}}},
  author = {Botev, Zdravko I. and Kroese, Dirk P. and Rubinstein, Reuven Y. and L'Ecuyer, Pierre},
  year = {2013},
  volume = {31},
  pages = {35--59},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-444-53859-8.00003-5},
  abstract = {The cross-entropy method is a versatile heuristic tool for solving difficult estimation and optimization problems, based on Kullback\textendash Leibler (or cross-entropy) minimization. As an optimization method it unifies many existing populationbased optimization heuristics. In this chapter we show how the cross-entropy method can be applied to a diverse range of combinatorial, continuous, and noisy optimization problems.},
  isbn = {978-0-444-53859-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/AS434W9V/Botev et al. - 2013 - The Cross-Entropy Method for Optimization.pdf}
}

@article{boydDistributedOptimizationStatistical2010,
  title = {Distributed {{Optimization}} and {{Statistical Learning}} via the {{Alternating Direction Method}} of {{Multipliers}}},
  author = {Boyd, Stephen},
  year = {2010},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {3},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000016},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/Q3BK9S83/Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf}
}

@article{braylanFrameSkipPowerful,
  title = {Frame {{Skip Is}} a {{Powerful Parameter}} for {{Learning}} to {{Play Atari}}},
  author = {Braylan, Alex and Hollenbeck, Mark and Meyerson, Elliot and Miikkulainen, Risto},
  pages = {2},
  abstract = {We show that setting a reasonable frame skip can be critical to the performance of agents learning to play Atari 2600 games. In all of the six games in our experiments, frame skip is a strong determinant of success. For two of these games, setting a large frame skip leads to state-of-the-art performance.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/JZEBIUZ4/Braylan et al. - Frame Skip Is a Powerful Parameter for Learning to.pdf}
}

@article{brePredictionWindPressure2017,
  title = {Prediction of Wind Pressure Coefficients on Building Surfaces Using {{Artificial Neural Networks}}},
  author = {Bre, Facundo and Gimenez, Juan and Fachinotti, V{\'i}ctor},
  year = {2017},
  month = nov,
  journal = {Energy and Buildings},
  volume = {158},
  doi = {10.1016/j.enbuild.2017.11.045}
}

@incollection{brockhoffMirroredSamplingSequential2010,
  title = {Mirrored {{Sampling}} and {{Sequential Selection}} for {{Evolution Strategies}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}, {{PPSN XI}}},
  author = {Brockhoff, Dimo and Auger, Anne and Hansen, Nikolaus and Arnold, Dirk V. and Hohm, Tim},
  editor = {Schaefer, Robert and Cotta, Carlos and Ko{\l}odziej, Joanna and Rudolph, G{\"u}nter},
  year = {2010},
  pages = {11--21},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15844-5_2},
  abstract = {This paper reveals the surprising result that a single-parent non-elitist evolution strategy (ES) can be locally faster than the (1+1)-ES. The result is brought about by mirrored sampling and sequential selection. With mirrored sampling, two offspring are generated symmetrically or mirrored with respect to their parent. In sequential selection, the offspring are evaluated sequentially and the iteration is concluded as soon as one offspring is better than the current parent. Both concepts complement each other well. We derive exact convergence rates of the (1, {$\lambda$})-ES with mirrored sampling and/or sequential selection on the sphere model. The log-linear convergence of the ES is preserved. Both methods lead to an improvement and in combination the (1,4)-ES becomes about 10\% faster than the (1+1)-ES. Naively implemented into the CMA-ES with recombination, mirrored sampling leads to a bias on the step-size. However, the (1,4)-CMA-ES with mirrored sampling and sequential selection is unbiased and appears to be faster, more robust, and as local as the (1+1)-CMA-ES.},
  isbn = {978-3-642-15843-8 978-3-642-15844-5},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/2LDNFFXW/Brockhoff et al. - 2010 - Mirrored Sampling and Sequential Selection for Evo.pdf}
}

@article{brockman2016openai,
  title = {Openai Gym},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  journal = {arXiv preprint arXiv:1606.01540},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{brockSMASHOneShotModel2017,
  title = {{{SMASH}}: {{One-Shot Model Architecture Search}} through {{HyperNetworks}}},
  shorttitle = {{{SMASH}}},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.05344 [cs]},
  eprint = {1708.05344},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized handdesigned networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/9MSML8B7/Brock et al. - 2017 - SMASH One-Shot Model Architecture Search through .pdf}
}

@article{bronsteinGeometricDeepLearning2017,
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  shorttitle = {Geometric Deep Learning},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  eprint = {1611.08097},
  eprinttype = {arxiv},
  pages = {18--42},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2017.2693418},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/disc/p.templier/Zotero/storage/QXI76WNL/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf}
}

@inproceedings{bubeck2013multiple,
  title = {Multiple Identifications in Multi-Armed Bandits},
  booktitle = {Proceedings of the \#1 International Conference on Machine Learning ({{ICML}} \#2){{30th2013}}},
  author = {Bubeck, S{\'e}bastian and Wang, Tengyao and Viswanathan, Nitin},
  year = {2013},
  pages = {258--265},
  organization = {{PMLR}}
}

@inproceedings{bukNEATHyperNEATSubstituted2009,
  title = {{{NEAT}} in {{HyperNEAT}} Substituted with Genetic Programming},
  author = {Buk, Zdenek and Koutn{\'i}k, Jan and Snorek, Miroslav},
  year = {2009},
  month = sep,
  volume = {5495},
  pages = {243--252},
  doi = {10.1007/978-3-642-04921-7_25},
  abstract = {In this paper we present application of genetic programming (GP) [1] to evolution of indirect encoding of neural network weights. We compare usage of original HyperNEAT algorithm with our implementation, in which we replaced the underlying NEAT with genetic programming. The algorithm was named HyperGP. The evolved neural networks were used as controllers of autonomous mobile agents (robots) in simulation. The agents were trained to drive with maximum average speed. This forces them to learn how to drive on roads and avoid collisions. The genetic programming lacking the NEAT complexification property shows better exploration ability and tends to generate more complex solutions in fewer generations. On the other hand, the basic genetic programming generates quite complex functions for weights generation. Both approaches generate neural controllers with similar abilities.},
  file = {/home/disc/p.templier/Zotero/storage/WU3VE8U7/Buk et al. - 2009 - NEAT in HyperNEAT substituted with genetic program.pdf}
}

@inproceedings{cazenilleEnsembleFeatureExtraction2021,
  title = {Ensemble Feature Extraction for Multi-Container Quality-Diversity Algorithms},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Cazenille, Leo},
  year = {2021},
  month = jun,
  pages = {75--83},
  publisher = {{ACM}},
  address = {{Lille France}},
  doi = {10.1145/3449639.3459392},
  abstract = {Quality-Diversity algorithms search for large collections of diverse and high-performing solutions, rather than just for a single solution like typical optimisation methods. They are specially adapted for multi-modal problems that can be solved in many different ways, such as complex reinforcement learning or robotics tasks. However, these approaches are highly dependent on the choice of feature descriptors (FDs) quantifying the similarity in behaviour of the solutions. While FDs usually needs to be hand-designed, recent studies have proposed ways to define them automatically by using feature extraction techniques, such as PCA or Auto-Encoders, to learn a representation of the problem from previously explored solutions. Here, we extend these approaches to more complex problems which cannot be efficiently explored by relying only on a single representation but require instead a set of diverse and complementary representations. We describe MC-AURORA, a Quality-Diversity approach that optimises simultaneously several collections of solutions, each with a different set of FDs, which are, in turn, defined automatically by an ensemble of modular auto-encoders. We show that this approach produces solutions that are more diverse than those produced by single-representation approaches.},
  isbn = {978-1-4503-8350-9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/IZ7N45MR/Cazenille - 2021 - Ensemble feature extraction for multi-container qu.pdf}
}

@article{chatzilygeroudisSurveyPolicySearch2019,
  title = {A Survey on Policy Search Algorithms for Learning Robot Controllers in a Handful of Trials},
  author = {Chatzilygeroudis, Konstantinos and Vassiliades, Vassilis and Stulp, Freek and Calinon, Sylvain and Mouret, Jean-Baptiste},
  year = {2019},
  month = dec,
  journal = {arXiv:1807.02303 [cs, stat]},
  eprint = {1807.02303},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible with a physical robot. This survey article focuses on the extreme other end of the spectrum: how can a robot adapt with only a handful of trials (a dozen) and a few minutes? By analogy with the word "big-data", we refer to this challenge as "micro-data reinforcement learning". We show that a first strategy is to leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators). A second strategy is to create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Overall, all successful micro-data algorithms combine these two strategies by varying the kind of model and prior knowledge. The current scientific challenges essentially revolve around scaling up to complex robots (e.g., humanoids), designing generic priors, and optimizing the computing time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/LBNRAB25/Chatzilygeroudis et al. - 2019 - A survey on policy search algorithms for learning .pdf;/home/disc/p.templier/Zotero/storage/ZZWU5YEE/1807.html}
}

@article{chavoyaCellPatternGeneration2008,
  title = {A Cell Pattern Generation Model Based on an Extended Artificialregulatory Network},
  shorttitle = {Doi},
  author = {Chavoya, Arturo and Duthen, Yves},
  year = {2008},
  doi = {10.1016/j.biosystems.2008.05.015},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/N24JVK7R/doi10.1016j.biosystems.2008.05.015  Elsevier En.pdf;/home/disc/p.templier/Zotero/storage/9F9MKE7I/S0303264708001317.html}
}

@article{chrabaszczBackBasicsBenchmarking2018,
  title = {Back to {{Basics}}: {{Benchmarking Canonical Evolution Strategies}} for {{Playing Atari}}},
  shorttitle = {Back to {{Basics}}},
  author = {Chrabaszcz, Patryk and Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  pages = {1419--1426},
  abstract = {Electronic proceedings of IJCAI 2018},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/NEBM7F34/Chrabaszcz et al. - 2018 - Back to Basics Benchmarking Canonical Evolution S.pdf;/home/disc/p.templier/Zotero/storage/PNUHGXET/197.html}
}

@article{cideronQDRLEfficientMixing2020,
  title = {{{QD-RL}}: {{Efficient Mixing}} of {{Quality}} and {{Diversity}} in {{Reinforcement Learning}}},
  shorttitle = {{{QD-RL}}},
  author = {Cideron, Geoffrey and Pierrot, Thomas and Perrin, Nicolas and Beguir, Karim and Sigaud, Olivier},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.08505 [cs]},
  eprint = {2006.08505},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a novel reinforcement learning algorithm,QD-RL, that incorporates the strengths of off-policy RL algorithms into Quality Diversity (QD) approaches. Quality-Diversity methods contribute structural biases by decoupling the search for diversity from the search for high return, resulting in efficient management of the exploration-exploitation trade-off. However, these approaches generally suffer from sample inefficiency as they call upon evolutionary techniques. QD-RL removes this limitation by relying on off-policy RL algorithms. More precisely, we train a population of off-policy deep RL agents to simultaneously maximize diversity inside the population and the return of the agents. QD-RL selects agents from the diversity-return Pareto Front, resulting in stable and efficient population updates. Our experiments on the Ant-Maze environment show that QD-RL can solve challenging exploration and control problems with deceptive rewards while being more than 15 times more sample efficient than its evolutionary counterparts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/GCBRZCVN/Cideron et al. - 2020 - QD-RL Efficient Mixing of Quality and Diversity i.pdf;/home/disc/p.templier/Zotero/storage/9TLVMKNB/2006.html}
}

@article{clunePerformanceIndirectEncoding2011,
  title = {On the {{Performance}} of {{Indirect Encoding Across}} the {{Continuum}} of {{Regularity}}},
  author = {Clune, J. and Stanley, K. O. and Pennock, R. T. and Ofria, C.},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {15},
  number = {3},
  pages = {346--367},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2010.2104157},
  abstract = {This paper investigates how an evolutionary algorithm with an indirect encoding exploits the property of phenotypic regularity, an important design principle found in natural organisms and engineered designs. We present the first comprehensive study showing that such phenotypic regularity enables an indirect encoding to outperform direct encoding controls as problem regularity increases. Such an ability to produce regular solutions that can exploit the regularity of problems is an important prerequisite if evolutionary algorithms are to scale to high-dimensional real-world problems, which typically contain many regularities, both known and unrecognized. The indirect encoding in this case study is HyperNEAT, which evolves artificial neural networks (ANNs) in a manner inspired by concepts from biological development. We demonstrate that, in contrast to two direct encoding controls, HyperNEAT produces both regular behaviors and regular ANNs, which enables HyperNEAT to significantly outperform the direct encodings as regularity increases in three problem domains. We also show that the types of regularities HyperNEAT produces can be biased, allowing domain knowledge and preferences to be injected into the search. Finally, we examine the downside of a bias toward regularity. Even when a solution is mainly regular, some irregularity may be needed to perfect its functionality. This insight is illustrated by a new algorithm called HybrID that hybridizes indirect and direct encodings, which matched HyperNEAT's performance on regular problems yet outperformed it on problems with some irregularity. HybrID's ability to improve upon the performance of HyperNEAT raises the question of whether indirect encodings may ultimately excel not as stand-alone algorithms, but by being hybridized with a further process of refinement, wherein the indirect encoding produces patterns that exploit problem regularity and the refining process modifies that pattern to capture irregularities. This paper thus paints a more complete picture of indirect encodings than prior studies because it analyzes the impact of the continuum between irregularity and regularity on the performance of such encodings, and ultimately suggests a path forward that combines indirect encodings with a separate process of refinement.},
  keywords = {artificial neural networks,Artificial neural networks,Bioinformatics,developmental encodings,Encoding,Evolution (biology),evolutionary algorithm,evolutionary computation,generative encodings,Genomics,HybrID algorithm,hybrid indirect,HyperNEAT,indirect encoding,indirect encodings,neural nets,Organisms,phenotypic regularity property,regularity,regularity continuum,Topology},
  file = {/home/disc/p.templier/Zotero/storage/K2YZCG5E/Clune et al. - 2011 - On the Performance of Indirect Encoding Across the.pdf;/home/disc/p.templier/Zotero/storage/LM4AGTNR/5910671.html}
}

@article{co-reyesEvolvingReinforcementLearning2021,
  title = {Evolving {{Reinforcement Learning Algorithms}}},
  author = {{Co-Reyes}, John D. and Miao, Yingjie and Peng, Daiyi and Real, Esteban and Levine, Sergey and Le, Quoc V. and Lee, Honglak and Faust, Aleksandra},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03958 [cs]},
  eprint = {2101.03958},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,rl},
  file = {/home/disc/p.templier/Zotero/storage/FAN8VKTC/Co-Reyes et al. - 2021 - Evolving Reinforcement Learning Algorithms.pdf}
}

@article{colasGEPPGDecouplingExploration,
  title = {{{GEP-PG}}: {{Decoupling Exploration}} and {{Exploitation}} in {{Deep Reinforcement Learning Algorithms}}},
  author = {Colas, Cedric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  pages = {10},
  abstract = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, QualityDiversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger HalfCheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/6BEK7DZC/Colas et al. - GEP-PG Decoupling Exploration and Exploitation in.pdf}
}

@article{colasScalingMAPElitesDeep2020,
  title = {Scaling {{MAP-Elites}} to {{Deep Neuroevolution}}},
  author = {Colas, C{\'e}dric and Huizinga, Joost and Madhavan, Vashisht and Clune, Jeff},
  year = {2020},
  month = jun,
  journal = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  eprint = {2003.01825},
  eprinttype = {arxiv},
  pages = {67--75},
  doi = {10.1145/3377930.3390217},
  abstract = {Quality-Diversity (QD) algorithms, and MAP-Elites (ME) in particular, have proven very useful for a broad range of applications including enabling real robots to recover quickly from joint damage, solving strongly deceptive maze tasks or evolving robot morphologies to discover new gaits. However, present implementations of ME and other QD algorithms seem to be limited to low-dimensional controllers with far fewer parameters than modern deep neural network models. In this paper, we propose to leverage the efficiency of Evolution Strategies (ES) to scale MAP-Elites to high-dimensional controllers parameterized by large neural networks. We design and evaluate a new hybrid algorithm called MAP-Elites with Evolution Strategies (ME-ES) for post-damage recovery in a difficult highdimensional control task where traditional ME fails. Additionally, we show that ME-ES performs efficient exploration, on par with state-of-the-art exploration algorithms in high-dimensional control tasks with strongly deceptive rewards.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/6HVR39J8/Colas et al. - 2020 - Scaling MAP-Elites to Deep Neuroevolution.pdf}
}

@misc{ComputerScienceBiology,
  title = {Computer {{Science}} and {{Biology Explore Algorithmic Evolution}}},
  journal = {Quanta Magazine},
  abstract = {Some researchers are using a complexity framework thought to be purely theoretical to understand evolutionary dynamics in biological and computational systems.},
  howpublished = {https://www.quantamagazine.org/computer-science-and-biology-explore-algorithmic-evolution-20181129/},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/S9KYL7L2/computer-science-and-biology-explore-algorithmic-evolution-20181129.html}
}

@article{contiImprovingExplorationEvolution2018,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty-Seeking Agents}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2018},
  month = oct,
  journal = {arXiv:1712.06560 [cs]},
  eprint = {1712.06560},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/disc/p.templier/Zotero/storage/M3FIJZPE/Conti et al. - 2018 - Improving Exploration in Evolution Strategies for .pdf}
}

@article{costaAdaptiveSharingElitist2003,
  title = {An {{Adaptive Sharing Elitist Evolution Strategy}} for {{Multiobjective Optimization}}},
  author = {Costa, Lino and Oliveira, Pedro},
  year = {2003},
  month = dec,
  journal = {Evolutionary Computation},
  volume = {11},
  number = {4},
  pages = {417--438},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365603322519297},
  abstract = {Almost all approaches to multiobjective optimization are based on Genetic Algorithms (GAs), and implementations based on Evolution Strategies (ESs) are very rare. Thus, it is crucial to investigate how ESs can be extended to multiobjective optimization, since they have, in the past, proven to be powerful single objective optimizers. In this paper, we present a new approach to multiobjective optimization, based on ESs. We call this approach the Multiobjective Elitist Evolution Strategy (MEES) as it incorporates several mechanisms, like elitism, that improve its performance. When compared with other algorithms, MEES shows very promising results in terms of performance.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/Z3ECKX5V/Costa et Oliveira - 2003 - An Adaptive Sharing Elitist Evolution Strategy for.pdf}
}

@article{craftonDirectFeedbackAlignment2019,
  title = {Direct {{Feedback Alignment With Sparse Connections}} for {{Local Learning}}},
  author = {Crafton, Brian and Parihar, Abhinav and Gebhardt, Evan and Raychowdhury, Arijit},
  year = {2019},
  journal = {Frontiers in Neuroscience},
  volume = {13},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00525},
  abstract = {Recent advances in deep neural networks (DNNs) owe their success to training algorithms that use backpropagation and gradient-descent. Backpropagation, while highly effective on von Neumann architectures, becomes inefficient when scaling to large networks. Commonly referred to as the weight transport problem, each neuron's dependence on the weights and errors located deeper in the network require exhaustive data movement which presents a key problem in enhancing the performance and energy-efficiency of machine-learning hardware. In this work, we propose a bio-plausible alternative to backpropagation drawing from advances in feedback alignment algorithms in which the error computation at a single synapse reduces to the product of three scalar values, satisfying the three factor rule. Using a sparse feedback matrix, we show that a neuron needs only a fraction of the information previously used by the feedback alignment algorithms to yield results which are competitive with backpropagation. Consequently, memory and compute can be partitioned and distributed whichever way produces the most efficient forward pass so long as a single error can be delivered to each neuron. We evaluate our algorithm using standard data sets, including ImageNet, to address the concern of scaling to challenging problems. Our results show orders of magnitude improvement in data movement and \$2\textbackslash times\$ improvement in multiply-and-accumulate operations over backpropagation. All the code and results are available under https://github.com/bcrafton/ssdfa.},
  langid = {english},
  keywords = {backpropagation,bio-plausible algorithms,feedback alignment,Hardware Acceleration,Local learning,sparse neural networks},
  file = {/home/disc/p.templier/Zotero/storage/RNKAFKUD/Crafton et al. - 2019 - Direct Feedback Alignment With Sparse Connections .pdf}
}

@article{crombachEvolutionEvolvabilityGene2008,
  title = {Evolution of {{Evolvability}} in {{Gene Regulatory Networks}}},
  author = {Crombach, Anton and Hogeweg, Paulien},
  editor = {Sauro, Herbert M.},
  year = {2008},
  month = jul,
  journal = {PLoS Computational Biology},
  volume = {4},
  number = {7},
  pages = {e1000112},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000112},
  abstract = {Gene regulatory networks are perhaps the most important organizational level in the cell where signals from the cell state and the outside environment are integrated in terms of activation and inhibition of genes. For the last decade, the study of such networks has been fueled by large-scale experiments and renewed attention from the theoretical field. Different models have been proposed to, for instance, investigate expression dynamics, explain the network topology we observe in bacteria and yeast, and for the analysis of evolvability and robustness of such networks. Yet how these gene regulatory networks evolve and become evolvable remains an open question. An individual-oriented evolutionary model is used to shed light on this matter. Each individual has a genome from which its gene regulatory network is derived. Mutations, such as gene duplications and deletions, alter the genome, while the resulting network determines the gene expression pattern and hence fitness. With this protocol we let a population of individuals evolve under Darwinian selection in an environment that changes through time.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/DFVGYXYY/pcbi.1000112.pdf}
}

@inproceedings{cullyAutonomousSkillDiscovery2019,
  title = {Autonomous Skill Discovery with Quality-Diversity and Unsupervised Descriptors},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Cully, Antoine},
  year = {2019},
  month = jul,
  pages = {81--89},
  publisher = {{ACM}},
  address = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321804},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/YVN8FWNM/Cully - 2019 - Autonomous skill discovery with quality-diversity .pdf}
}

@article{cullyMultiEmitterMAPElitesImproving2021,
  title = {Multi-{{Emitter MAP-Elites}}: {{Improving}} Quality, Diversity and Convergence Speed with Heterogeneous Sets of Emitters},
  shorttitle = {Multi-{{Emitter MAP-Elites}}},
  author = {Cully, Antoine},
  year = {2021},
  month = jun,
  journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
  eprint = {2007.05352},
  eprinttype = {arxiv},
  pages = {84--92},
  doi = {10.1145/3449639.3459326},
  abstract = {Quality-Diversity (QD) optimisation is a new family of learning algorithms that aims at generating collections of diverse and highperforming solutions. Among those algorithms, the recently introduced Covariance Matrix Adaptation MAP-Elites (CMA-ME) algorithm proposes the concept of emitters, which uses a predefined heuristic to drive the algorithm's exploration. This algorithm was shown to outperform MAP-Elites, a popular QD algorithm that has demonstrated promising results in numerous applications. In this paper, we introduce Multi-Emitter MAP-Elites (ME-MAP-Elites), an algorithm that directly extends CMA-ME and improves its quality, diversity and data efficiency. It leverages the diversity of a heterogeneous set of emitters, in which each emitter type improves the optimisation process in different ways. A bandit algorithm dynamically finds the best selection of emitters depending on the current situation. We evaluate the performance of ME-MAP-Elites on six tasks, ranging from standard optimisation problems (in 100 dimensions) to complex locomotion tasks in robotics. Our comparisons against CMA-ME and MAP-Elites show that ME-MAP-Elites is faster at providing collections of solutions that are significantly more diverse and higher performing. Moreover, in cases where no fruitful synergy can be found between the different emitters, MEMAP-Elites is equivalent to the best of the compared algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/MGLCSUW4/Cully - 2021 - Multi-Emitter MAP-Elites Improving quality, diver.pdf}
}

@article{cullyQualityDiversityOptimization2018,
  title = {Quality and {{Diversity Optimization}}: {{A Unifying Modular Framework}}},
  author = {Cully, A. and Demiris, Y.},
  year = {2018},
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {22},
  number = {2},
  pages = {245--259}
}

@article{cullyRobotsThatCan2015,
  title = {Robots That Can Adapt like Animals},
  author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {503--507},
  publisher = {{Nature Publishing Group}}
}

@article{cussat-blancArtificialGeneRegulatory2019,
  title = {Artificial {{Gene Regulatory Networks}} - {{A Review}}},
  author = {{Cussat-Blanc}, Sylvain and Harrington, Kyle and Banzhaf, Wolfgang},
  year = {2019},
  month = apr,
  journal = {Artificial Life},
  volume = {24},
  number = {4},
  pages = {296--328},
  publisher = {{Massachusetts Institute of Technology Press (MIT Press)}},
  doi = {10.1162/artl_a_00267},
  abstract = {In nature, gene regulatory networks are a key mediator between the information stored in the DNA of living organisms (their genotype) and the structural and behavioral expression this finds in their bodies, surviving in the world (their phenotype). They integrate environmental signals, steer development, buffer stochasticity, and allow evolution to proceed. In engineering, modeling and implementations of artificial gene regulatory networks have been an expanding field of research and development over the past few decades. This review discusses the concept of gene regulation, describes the current state of the art in gene regulatory networks, including modeling and simulation, and reviews their use in artificial evolutionary settings. We provide evidence forthe benefits of this concept in natural and the engineering domains.},
  keywords = {Control dynamics,Evolutionary algorithms,Gene regulatory networks,Morphogenesis,Neuromodulation},
  file = {/home/disc/p.templier/Zotero/storage/YGKMN7ZX/Cussat-Blanc et al. - 2019 - Artificial Gene Regulatory Networks - A Review.pdf}
}

@article{cussat-blancGeneRegulatoryNetwork2015,
  title = {Gene {{Regulatory Network Evolution Through Augmenting Topologies}}},
  author = {{Cussat-Blanc}, Sylvain and Harrington, Kyle and Pollack, Jordan},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {19},
  number = {6},
  pages = {823--837},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2015.2396199},
  abstract = {Artificial gene regulatory networks (GRNs) are biologically inspired dynamical systems used to control various kinds of agents, from the cells in developmental models to embodied robot swarms. Most recent work uses a genetic algorithm (GA) or an evolution strategy in order to optimize the network for a specific task. However, the empirical performances of these algorithms are unsatisfactory. This paper presents an algorithm that primarily exploits a network distance metric, which allows genetic similarity to be used for speciation and variation of GRNs. This algorithm, inspired by the successful neuroevolution of augmenting topologies algorithm's use in evolving neural networks and compositional pattern-producing networks, is based on a specific initialization method, a crossover operator based on gene alignment, and speciation based upon GRN structures. We demonstrate the effectiveness of this new algorithm by comparing our approach both to a standard GA and to evolutionary programming on four different experiments from three distinct problem domains, where the proposed algorithm excels on all experiments.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/N4A8LBXQ/Cussat-Blanc et al. - 2015 - Gene Regulatory Network Evolution Through Augmenti.pdf}
}

@article{d2014hyperneat,
  title = {{{HyperNEAT}}: {{The}} First Five Years},
  author = {D'Ambrosio, David B and Gauci, Jason and Stanley, Kenneth O},
  year = {2014},
  journal = {Growing adaptive machines},
  pages = {159--185},
  publisher = {{Springer}}
}

@article{dabneyImplicitQuantileNetworks2018,
  title = {Implicit {{Quantile Networks}} for {{Distributional Reinforcement Learning}}},
  author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.06923 [cs, stat]},
  eprint = {1806.06923},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/KXDMAYEF/Dabney et al. - 2018 - Implicit Quantile Networks for Distributional Rein.pdf}
}

@article{DALCIN20051108,
  title = {{{MPI}} for Python},
  author = {Dalc{\'i}n, Lisandro and Paz, Rodrigo and Storti, Mario},
  year = {2005},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {65},
  number = {9},
  pages = {1108--1115},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2005.03.010},
  abstract = {MPI for Python provides bindings of the Message Passing Interface (MPI) standard for the Python programming language and allows any Python program to exploit multiple processors. This package is constructed on top of the MPI-1 specification and defines an object-oriented interface which closely follows MPI-2 C++bindings. It supports point-to-point (sends, receives) and collective (broadcasts, scatters, gathers) communications of general Python objects. Efficiency has been tested in a Beowulf class cluster and satisfying results were obtained. MPI for Python is open source and available for download on the web (http://www.cimec.org.ar/python).},
  keywords = {High level languages,Message passing,MPI,Parallel Python},
  file = {/home/disc/p.templier/Zotero/storage/QJA6ILK6/11.pdf}
}

@inproceedings{dambrosioGenerativeEncodingMultiagent2008,
  title = {Generative Encoding for Multiagent Learning},
  booktitle = {{{GECCO}} '08},
  author = {D'Ambrosio, David B. and Stanley, K.},
  year = {2008},
  doi = {10.1145/1389095.1389256},
  abstract = {This paper argues that multiagent learning is a potential "killer application" for generative and developmental systems (GDS) because key challenges in learning to coordinate a team of agents are naturally addressed through indirect encodings and information reuse. For example, a significant problem for multiagent learning is that policies learned separately for different agent roles may nevertheless need to share a basic skill set, forcing the learning algorithm to reinvent the wheel for each agent. GDS is a good match for this kind of problem because it specializes in ways to encode patterns of related yet varying motifs. In this paper, to establish the promise of this capability, the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) generative approach to evolving neurocontrollers learns a set of coordinated policies encoded by a single genome representing a team of predator agents that work together to capture prey. Experimental results show that it is not only possible, but beneficial to encode a heterogeneous team of agents with an indirect encoding. The main contribution is thus to open up a significant new application domain for GDS.},
  file = {/home/disc/p.templier/Zotero/storage/SCBH5G64/download.pdf}
}

@book{darwinOriginSpeciesMeans1859,
  title = {The Origin of Species by Means of Natural Selection: Or, the Preservation of Favored Races in the Struggle for Life},
  author = {Darwin, Charles},
  year = {1859}
}

@article{davidGeneticAlgorithmsEvolving2014,
  title = {Genetic {{Algorithms}} for {{Evolving Computer Chess Programs}}},
  author = {David, Eli and van den Herik, H. Jaap and Koppel, Moshe and Netanyahu, Nathan S.},
  year = {2014},
  month = oct,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {18},
  number = {5},
  eprint = {1711.08337},
  eprinttype = {arxiv},
  pages = {779--789},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2013.2285111},
  abstract = {This paper demonstrates the use of genetic algorithms for evolving (1) a grandmaster-level evaluation function and (2) a search mechanism for a chess program, the parameter values of which are initialized randomly. The evaluation function of the program is evolved by learning from databases of (human) grandmaster games. At first the organisms are evolved to mimic the behavior of human grandmasters, and then these organisms are further improved upon by means of coevolution. The search mechanism is evolved by learning from tactical test suites. Our results show that the evolved program outperforms a two-time World Computer Chess Champion, and is on a par with other leading computer chess programs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/3YZ4UIIP/David et al. - 2014 - Genetic Algorithms for Evolving Computer Chess Pro.pdf}
}

@book{de2006evolutionary,
  title = {Evolutionary Computation: {{A}} Unified Approach},
  author = {De Jong, Kenneth A},
  year = {2006},
  publisher = {{MIT Press}}
}

@misc{DeepLearningNeural,
  title = {Deep Learning in Neural Networks: {{An}} Overview | {{Elsevier Enhanced Reader}}},
  shorttitle = {Deep Learning in Neural Networks},
  doi = {10.1016/j.neunet.2014.09.003},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0893608014002135?token=0AC9D7472AE0AAA1A41B151DACC9EA79F9182032ED8397666B525CE8C5ACFAA19EE980370C5E91782110CBE6F7BFAEB9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/2CCQJDAB/Deep learning in neural networks An overview  El.pdf;/home/disc/p.templier/Zotero/storage/7S68SA7K/S0893608014002135.html}
}

@misc{DeepMindAIReduces,
  title = {{{DeepMind AI Reduces Google Data Centre Cooling Bill}} by 40\%},
  abstract = {Reducing energy usage has been a major focus for us over the past 10 years: we have built our own super-efficient servers at Google, invented more efficient ways to cool our data centres and invested heavily in green energy sources, with the goal of being powered 100 percent by renewable energy.},
  howpublished = {https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/LHJS6HUZ/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40.html}
}

@article{degraveMagneticControlTokamak2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and {de las Casas}, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  year = {2022},
  month = feb,
  journal = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  abstract = {Abstract                            Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak \`a Configuration Variable               1,2               , including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/V3VI8UD7/Degrave et al. - 2022 - Magnetic control of tokamak plasmas through deep r.pdf}
}

@misc{DifferentialEvolutionMixed,
  title = {Differential Evolution with Mixed Mutation Strategy Based on Deep Reinforcement Learning - {{ScienceDirect-2021}}\_s1568494621005998},
  doi = {10.1016/j.asoc.2021.107678},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S1568494621005998?token=515EB8E262453C0C6A432B871A8C84E39237405B7ACAADAB940AE0130C7106423615C8520DBED22196D2F7D23BE9E9F7\&originRegion=eu-west-1\&originCreation=20220516112700},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/Z8LTDUDF/S1568494621005998.html}
}

@inproceedings{dissetComparisonGeneticRegulatory2017,
  title = {A Comparison of Genetic Regulatory Network Dynamics and Encoding},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Disset, Jean and Wilson, Dennis G and {Cussat-Blanc}, Sylvain and Sanchez, St{\'e}phane and Luga, Herv{\'e} and Duthen, Yves},
  year = {2017},
  month = jul,
  pages = {91--98},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3071178.3071322},
  isbn = {978-1-4503-4920-8},
  langid = {english},
  keywords = {fitness evaluations,genetic algorithms,representations},
  file = {/home/disc/p.templier/Zotero/storage/XWBFT3KU/Disset et al. - 2017 - A comparison of genetic regulatory network dynamic.pdf}
}

@inproceedings{doerrWhenResamplingCope2019,
  title = {When Resampling to Cope with Noise, Use Median, Not Mean},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Doerr, Benjamin and Sutton, Andrew M.},
  year = {2019},
  month = jul,
  pages = {242--248},
  publisher = {{ACM}},
  address = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321837},
  abstract = {Due to their randomized nature, many nature-inspired heuristics are robust to some level of noise in the fitness evaluations. A common strategy to increase the tolerance to noise is to re-evaluate the fitness of a solution candidate several times and to then work with the average of the sampled fitness values. In this work, we propose to use the median instead of the mean. Besides being invariant to rescalings of the fitness, the median in many situations turns out to be much more robust than the mean. We show that when the noisy fitness is {$\epsilon$}-concentrated, then a logarithmic number of samples suffice to discover the undisturbed fitness (via the median of the samples) with high probability. This gives a simple metaheuristic approach to transform a randomized optimization heuristics into one that is robust to this type of noise and that has a runtime higher than the original one only by a logarithmic factor. We show further that {$\epsilon$}-concentrated noise occurs frequently in standard situations. We also provide lower bounds showing that in two such situations, even with larger numbers of samples, the average-resample strategy cannot efficiently optimize the problem in polynomial time.},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/FS9QBRU5/Doerr et Sutton - 2019 - When resampling to cope with noise, use median, no.pdf}
}

@inproceedings{doncieuxNoveltySearchTheoretical2019,
  title = {Novelty Search: A Theoretical Perspective},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Doncieux, Stephane and Laflaqui{\`e}re, Alban and Coninx, Alexandre},
  year = {2019},
  pages = {99--106}
}

@inproceedings{droste2004analysis,
  title = {Analysis of the (1+ 1) {{EA}} for a Noisy {{OneMax}}},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference ({{GECCO}} \#1)2004},
  author = {Droste, Stefan},
  year = {2004},
  pages = {1088--1099},
  organization = {{Springer}}
}

@inproceedings{eibenIfItEvolves2020,
  title = {If It Evolves It Needs to Learn},
  booktitle = {Proceedings of the 2020 {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Eiben, A. E. and Hart, Emma},
  year = {2020},
  month = jul,
  pages = {1383--1384},
  publisher = {{ACM}},
  address = {{Canc\'un Mexico}},
  doi = {10.1145/3377929.3398151},
  abstract = {We elaborate on (future) evolutionary robot systems where morphologies and controllers of real robots are evolved in the real-world. We argue that such systems must contain a learning component where a newborn robot refines its inherited controller to align with its body, which will inevitably be different from its parents.},
  isbn = {978-1-4503-7127-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/H2X72UUY/Eiben et Hart - 2020 - If it evolves it needs to learn.pdf}
}

@article{elskenEfficientMultiobjectiveNeural2019,
  title = {Efficient {{Multi-objective Neural Architecture Search}} via {{Lamarckian Evolution}}},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  year = {2019},
  month = feb,
  journal = {arXiv:1804.09081 [cs, stat]},
  eprint = {1804.09081},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1)the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption, (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/4ZLCHEZC/Elsken et al. - 2019 - Efficient Multi-objective Neural Architecture Sear.pdf;/home/disc/p.templier/Zotero/storage/IDHCLMNR/1804.html}
}

@article{elskenNeuralArchitectureSearch,
  title = {Neural {{Architecture Search}}: {{A Survey}}},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  pages = {21},
  abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/IYV88829/Elsken et al. - Neural Architecture Search A Survey.pdf;/home/disc/p.templier/Zotero/storage/XCEIN392/Elsken et al. - Neural Architecture Search A Survey.pdf}
}

@article{espeholtIMPALAScalableDistributed,
  title = {{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymyr and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  pages = {10},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/9RSAYL3Z/Espeholt et al. - IMPALA Scalable Distributed Deep-RL with Importan.pdf}
}

@misc{evosax2022github,
  title = {{{evosax}}: {{JAX-based}} Evolution Strategies},
  author = {Lange, Robert Tjarko},
  year = {2022}
}

@article{fernandoConvolutionEvolutionDifferentiable2016,
  title = {Convolution by {{Evolution}}: {{Differentiable Pattern Producing Networks}}},
  shorttitle = {Convolution by {{Evolution}}},
  author = {Fernando, Chrisantha and Banarse, Dylan and Reynolds, Malcolm and Besse, Frederic and Pfau, David and Jaderberg, Max and Lanctot, Marc and Wierstra, Daan},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.02580 [cs]},
  eprint = {1606.02580},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,cppn,dppn},
  file = {/home/disc/p.templier/Zotero/storage/9S5WYUKG/Fernando et al. - 2016 - Convolution by Evolution Differentiable Pattern P.pdf}
}

@article{FitnessLandscapeAnalysis,
  title = {Fitness {{Landscape Analysis}} of {{Automated Machine Learning Search Spaces}}},
  file = {/home/disc/p.templier/Zotero/storage/9TPNQZF5/_.pdf}
}

@book{flageatFastStableMAPElites2020,
  title = {Fast and Stable {{MAP-Elites}} in Noisy Domains Using Deep Grids},
  author = {Flageat, Manon and Cully, Antoine},
  year = {2020},
  annotation = {\_eprint: 2006.14253}
}

@inproceedings{fontaineMappingHearthstoneDeck2019,
  title = {Mapping Hearthstone Deck Spaces through {{MAP-elites}} with Sliding Boundaries},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Fontaine, Matthew C. and Lee, Scott and Soros, L. B. and De Mesentier Silva, Fernando and Togelius, Julian and Hoover, Amy K.},
  year = {2019},
  month = jul,
  pages = {161--169},
  publisher = {{ACM}},
  address = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321794},
  abstract = {Quality diversity (QD) algorithms such as MAP-Elites have emerged as a powerful alternative to traditional single-objective optimization methods. They were initially applied to evolutionary robotics problems such as locomotion and maze navigation, but have yet to see widespread application. We argue that these algorithms are perfectly suited to the rich domain of video games, which contains many relevant problems with a multitude of successful strategies and often also multiple dimensions along which solutions can vary. This paper introduces a novel modification of the MAP-Elites algorithm called MAP-Elites with Sliding Boundaries (MESB) and applies it to the design and rebalancing of Hearthstone, a popular collectible card game chosen for its number of multidimensional behavior features relevant to particular styles of play. To avoid overpopulating cells with conflated behaviors, MESB slides the boundaries of cells based on the distribution of evolved individuals. Experiments in this paper demonstrate the performance of MESB in Hearthstone. Results suggest MESB finds diverse ways of playing the game well along the selected behavioral dimensions. Further analysis of the evolved strategies reveals common patterns that recur across behavioral dimensions and explores how MESB can help rebalance the game.},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/4JSWG3NW/Fontaine et al. - 2019 - Mapping hearthstone deck spaces through MAP-elites.pdf}
}

@article{fransSelectingSelectionLearning2021,
  title = {Selecting for {{Selection}}: {{Learning To Balance Adaptive}} and {{Diversifying Pressures}} in {{Evolutionary Search}}},
  shorttitle = {Selecting for {{Selection}}},
  author = {Frans, Kevin and Soros, L. B. and Witkowski, Olaf},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.09153 [cs]},
  eprint = {2106.09153},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Inspired by natural evolution, evolutionary search algorithms have proven remarkably capable due to their dual abilities to radiantly explore through diverse populations and to converge to adaptive pressures. A large part of this behavior comes from the selection function of an evolutionary algorithm, which is a metric for deciding which individuals survive to the next generation. In deceptive or hard-to-search fitness landscapes, greedy selection often fails, thus it is critical that selection functions strike the correct balance between gradient-exploiting adaptation and exploratory diversification. This paper introduces Sel4Sel, or Selecting for Selection, an algorithm that searches for high-performing neural-network-based selection functions through a meta-evolutionary loop. Results on three distinct bitstring domains indicate that Sel4Sel networks consistently match or exceed the performance of both fitness-based selection and benchmarks explicitly designed to encourage diversity. Analysis of the strongest Sel4Sel networks reveals a general tendency to favor highly novel individuals early on, with a gradual shift towards fitness-based selection as deceptive local optima are bypassed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/2WZQYNHJ/Frans et al. - 2021 - Selecting for Selection Learning To Balance Adapt.pdf;/home/disc/p.templier/Zotero/storage/EMYJWWHS/Frans et al. - 2021 - Selecting for Selection Learning To Balance Adapt.pdf}
}

@article{freemanLearningPredictLooking2019,
  title = {Learning to {{Predict Without Looking Ahead}}: {{World Models Without Forward Prediction}}},
  shorttitle = {Learning to {{Predict Without Looking Ahead}}},
  author = {Freeman, C. Daniel and Metz, Luke and Ha, David},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.13038 [cs]},
  eprint = {1910.13038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware---e.g., a brain---arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent. That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call observational dropout, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into learning a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment. Videos of our results available at https://learningtopredict.github.io/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/LV2BHP7P/Freeman et al. - 2019 - Learning to Predict Without Looking Ahead World M.pdf}
}

@inproceedings{friedrich2016graceful,
  title = {Graceful Scaling on Uniform versus Steep-Tailed Noise},
  booktitle = {Proceedings of the \#1 International Conference on Parallel Problem Solving from Nature ({{PPSN}} \#2){{14th2016}}},
  author = {Friedrich, Tobias and K{\"o}tzing, Timo and Krejca, Martin S. and Sutton, Andrew M.},
  year = {2016},
  pages = {761--770},
  organization = {{Springer}}
}

@article{friedrichCompactGeneticAlgorithm2017,
  title = {The {{Compact Genetic Algorithm}} Is {{Efficient Under Extreme Gaussian Noise}}},
  author = {Friedrich, Tobias and K{\"o}tzing, Timo and Krejca, Martin S. and Sutton, Andrew M.},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {21},
  number = {3},
  pages = {477--490},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2016.2613739},
  abstract = {Practical optimization problems frequently include uncertainty about the quality measure, for example, due to noisy evaluations. Thus, they do not allow for a straightforward application of traditional optimization techniques. In these settings, randomized search heuristics such as evolutionary algorithms are a popular choice because they are often assumed to exhibit some kind of resistance to noise. Empirical evidence suggests that some algorithms, such as estimation of distribution algorithms (EDAs) are robust against a scaling of the noise intensity, even without resorting to explicit noise-handling techniques such as resampling. In this paper, we want to support such claims with mathematical rigor. We introduce the concept of graceful scaling in which the run time of an algorithm scales polynomially with noise intensity. We study a monotone fitness function over binary strings with additive noise taken from a Gaussian distribution. We show that myopic heuristics cannot efficiently optimize the function under arbitrarily intense noise without any explicit noise-handling. Furthermore, we prove that using a population does not help. Finally, we show that a simple EDA called the compact genetic algorithm can overcome the shortsightedness of mutation-only heuristics to scale gracefully with noise. We conjecture that recombinative genetic algorithms also have this property.},
  keywords = {Evolutionary algorithms,Evolutionary computation,Gaussian noise,Genetic algorithms,Noise measurement,noisy optimization,Optimization,run time analysis,Sociology,Statistics},
  file = {/home/disc/p.templier/Zotero/storage/RK4L9YK2/Friedrich et al. - 2017 - The Compact Genetic Algorithm is Efficient Under E.pdf;/home/disc/p.templier/Zotero/storage/BM4N3YSX/7577782.html}
}

@misc{FrontiersDirectFeedback,
  title = {Frontiers | {{Direct Feedback Alignment With Sparse Connections}} for {{Local Learning}} | {{Neuroscience}}},
  howpublished = {https://www.frontiersin.org/articles/10.3389/fnins.2019.00525/full},
  file = {/home/disc/p.templier/Zotero/storage/MYAI3IDS/full.html}
}

@misc{FrontiersQualityDiversity,
  title = {Frontiers | {{Quality Diversity}}: {{A New Frontier}} for {{Evolutionary Computation}} | {{Robotics}} and {{AI}}},
  howpublished = {https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full},
  file = {/home/disc/p.templier/Zotero/storage/SG5IJ5QA/Frontiers  Quality Diversity A New Frontier for .pdf;/home/disc/p.templier/Zotero/storage/RHWK6BAW/full.html}
}

@article{fujimotoAddressingFunctionApproximation,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott},
  pages = {10},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/PGACTKK4/Fujimoto - Addressing Function Approximation Error in Actor-C.pdf}
}

@article{furutaPolicyInformationCapacity,
  title = {Policy {{Information Capacity}}: {{Information-Theoretic Measure}} for {{Task Complexity}} in {{Deep Reinforcement Learning}}},
  author = {Furuta, Hiroki and Matsushima, Tatsuya and Kozuno, Tadashi and Matsuo, Yutaka and Levine, Sergey and Nachum, Ofir and Gu, Shixiang Shane},
  pages = {12},
  abstract = {Progress in deep reinforcement learning (RL) research is largely enabled by benchmark task environments. However, analyzing the nature of those environments is often overlooked. In particular, we still do not have agreeable ways to measure the difficulty or solvability of a task, given that each has fundamentally different actions, observations, dynamics, rewards, and can be tackled with diverse RL algorithms. In this work, we propose policy information capacity (PIC) \textendash{} the mutual information between policy parameters and episodic return \textendash{} and policy-optimal information capacity (POIC) \textendash{} between policy parameters and episodic optimality \textendash{} as two environment-agnostic, algorithm-agnostic quantitative metrics for task difficulty. Evaluating our metrics across toy environments as well as continuous control benchmark tasks from OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that these information-theoretic metrics have higher correlations with normalized task solvability scores than a variety of alternatives. Lastly, we show that these metrics can also be used for fast and compute-efficient optimizations of key design parameters such as reward shaping, policy architectures, and MDP properties for better solvability by RL algorithms without ever running full RL experiments1.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/IQEBUGD4/Furuta et al. - Policy Information Capacity Information-Theoretic.pdf}
}

@article{gaier2018data,
  title = {Data-Efficient Design Exploration through Surrogate-Assisted Illumination},
  author = {Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
  year = {2018},
  journal = {Evolutionary Computation},
  volume = {26},
  number = {3},
  pages = {381--410},
  publisher = {{MIT Press}}
}

@inproceedings{gaierDataefficientNeuroevolutionKernelbased2018,
  title = {Data-Efficient Neuroevolution with Kernel-Based Surrogate Models},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
  year = {2018},
  month = jul,
  pages = {85--92},
  publisher = {{ACM}},
  address = {{Kyoto Japan}},
  doi = {10.1145/3205455.3205510},
  abstract = {Surrogate-assistance approaches have long been used in computationally expensive domains to improve the data-efficiency of optimization algorithms. Neuroevolution, however, has so far resisted the application of these techniques because it requires the surrogate model to make fitness predictions based on variable topologies, instead of a vector of parameters. Our main insight is that we can sidestep this problem by using kernel-based surrogate models, which require only the definition of a distance measure between individuals. Our second insight is that the well-established Neuroevolution of Augmenting Topologies (NEAT) algorithm provides a computationally efficient distance measure between dissimilar networks in the form of ``compatibility distance'', initially designed to maintain topological diversity. Combining these two ideas, we introduce a surrogate-assisted neuroevolution algorithm that combines NEAT and a surrogate model built using a compatibility distance kernel. We demonstrate the data-efficiency of this new algorithm on the low dimensional cart-pole swing-up problem, as well as the higher dimensional half-cheetah running task. In both tasks the surrogateassisted variant achieves the same or better results with several times fewer function evaluations as the original NEAT.},
  isbn = {978-1-4503-5618-3},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/BRNQTHQH/Gaier et al. - 2018 - Data-efficient neuroevolution with kernel-based su.pdf}
}

@incollection{gaierWeightAgnosticNeural2019,
  title = {Weight {{Agnostic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Gaier, Adam and Ha, David},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {5364--5378},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/disc/p.templier/Zotero/storage/LDXUFJ39/Gaier et Ha - 2019 - Weight Agnostic Neural Networks.pdf;/home/disc/p.templier/Zotero/storage/U8PVL579/8777-weight-agnostic-neural-networks.html}
}

@inproceedings{garivier2016optimal,
  title = {Optimal Best Arm Identification with Fixed Confidence},
  booktitle = {Proceedings of the \#1 Conference on Learning Theory ({{COLT}} \#2){{29th2016}}},
  author = {Garivier, Aur{\'e}lien and Kaufmann, Emilie},
  year = {2016},
  pages = {998--1027},
  organization = {{PMLR}}
}

@article{giessen2016robustness,
  title = {Robustness of Populations in Stochastic Environments},
  author = {Gie{\ss}en, Christian and K{\"o}tzing, Timo},
  year = {2016},
  journal = {Algorithmica. An International Journal in Computer Science},
  volume = {75},
  number = {3},
  pages = {462--489},
  publisher = {{Springer}}
}

@book{giuseppepaoloLearningSparseRewards,
  title = {Learning in {{Sparse Rewards}} Settings through {{Quality-Diversity}} Algorithms},
  author = {{Giuseppe Paolo}},
  file = {/home/disc/p.templier/Zotero/storage/K4TX6J9V/_.pdf}
}

@inproceedings{glorot2010understanding,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  pages = {249--256},
  organization = {{JMLR Workshop and Conference Proceedings}}
}

@article{gomez2008accelerated,
  title = {Accelerated Neural Evolution through Cooperatively Coevolved Synapses.},
  author = {Gomez, Faustino and Schmidhuber, J{\"u}rgen and Miikkulainen, Risto and Mitchell, Melanie},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {5}
}

@inproceedings{gonzalez-duqueFindingGameLevels2020,
  title = {Finding {{Game Levels}} with the {{Right Difficulty}} in a {{Few Trials}} through {{Intelligent Trial-and-Error}}},
  booktitle = {2020 {{IEEE Conference}} on {{Games}} ({{CoG}})},
  author = {{Gonz{\'a}lez-Duque}, Miguel and Palm, Rasmus Berg and Ha, David and Risi, Sebastian},
  year = {2020},
  month = aug,
  pages = {503--510},
  issn = {2325-4289},
  doi = {10.1109/CoG47356.2020.9231548},
  abstract = {Methods for dynamic difficulty adjustment allow games to be tailored to particular players to maximize their engagement. However, current methods often only modify a limited set of game features such as the difficulty of the opponents, or the availability of resources. Other approaches, such as experience-driven Procedural Content Generation (PCG), can generate complete levels with desired properties such as levels that are neither too hard nor too easy, but require many iterations. This paper presents a method that can generate and search for complete levels with a specific target difficulty in only a few trials. This advance is enabled by through an Intelligent Trial-and-Error algorithm, originally developed to allow robots to adapt quickly. Our algorithm first creates a large variety of different levels that vary across predefined dimensions such as leniency or map coverage. The performance of an AI playing agent on these maps gives a proxy for how difficult the level would be for another AI agent (e.g. one that employs Monte Carlo Tree Search instead of Greedy Tree Search); using this information, a Bayesian Optimization procedure is deployed, updating the difficulty of the prior map to reflect the ability of the agent. The approach can reliably find levels with a specific target difficulty for a variety of planning agents in only a few trials, while maintaining an understanding of their skill landscape.},
  keywords = {Dynamic Difficulty Adjustment,Intelligent Trial-and-Error,MAP-Elites,PCG,Planning Agents},
  file = {/home/disc/p.templier/Zotero/storage/HX3B8V64/González-Duque et al. - 2020 - Finding Game Levels with the Right Difficulty in a.pdf;/home/disc/p.templier/Zotero/storage/22ZK8279/9231548.html}
}

@article{gravinaQualityDiversitySurprise2019,
  title = {Quality {{Diversity Through Surprise}}},
  author = {Gravina, Daniele and Liapis, Antonios and Yannakakis, Georgios N.},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {23},
  number = {4},
  pages = {603--616},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2018.2877215},
  abstract = {Quality diversity (QD) is a recent family of evolutionary search algorithms which focus on finding several well-performing (quality) yet different (diversity) solutions with the aim to maintain an appropriate balance between divergence and convergence during search. While QD has already delivered promising results in complex problems, the capacity of divergent search variants for QD remains largely unexplored. Inspired by the notion of surprise as an effective driver of divergent search and its orthogonal nature to novelty this paper investigates the impact of the former to QD performance. For that purpose we introduce three new QD algorithms which employ surprise as a diversity measure, either on its own or combined with novelty, and compare their performance against novelty search with local competition, the state of the art QD algorithm. The algorithms are tested in a robot navigation task across 60 highly deceptive mazes. Our findings suggest that allowing surprise and novelty to operate synergistically for divergence and in combination with local competition leads to QD algorithms of significantly higher efficiency, speed, and robustness.},
  keywords = {Birds,Diversity reception,Local competition,Market research,maze navigation,Navigation,neuroevolution of augmenting topology (NEAT),novelty search,quality diversity (QD),Robustness,Search problems,surprise search (SS),Task analysis},
  file = {/home/disc/p.templier/Zotero/storage/3PSA4IGL/Gravina et al. - 2019 - Quality Diversity Through Surprise.pdf;/home/disc/p.templier/Zotero/storage/VCJ9HLXG/8501591.html}
}

@article{greenExploringOpenendedGameplay2021,
  title = {Exploring Open-Ended Gameplay Features with {{Micro RollerCoaster Tycoon}}},
  author = {Green, Michael Cerny and Yen, Victoria and Earle, Sam and Rajesh, Dipika and Edwards, Maria and Soros, L. B.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.04342 [cs]},
  eprint = {2105.04342},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper introduces MicroRCT, a novel open source simulator inspired by the theme park sandbox game RollerCoaster Tycoon. The goal in MicroRCT is to place rides and shops in an amusement park to maximize profit earned from park guests. Thus, the challenges for game AI include both selecting high-earning attractions and placing them in locations that are convenient to guests. In this paper, the MAPElites algorithm is used to generate a diversity of park layouts, exploring two theoretical questions about evolutionary algorithms and game design: 1) Is there a benefit to starting from a minimal starting point for evolution and complexifying incrementally? and 2) What are the effects of resource limitations on creativity and optimization? Results indicate that building from scratch with no costs results in the widest diversity of high-performing designs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/disc/p.templier/Zotero/storage/CW93ARCV/Green et al. - 2021 - Exploring open-ended gameplay features with Micro .pdf}
}

@inproceedings{greveEvolvingNeuralTuring2016,
  title = {Evolving {{Neural Turing Machines}} for {{Reward-based Learning}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} 2016},
  author = {Greve, Rasmus Boll and Jacobsen, Emil Juul and Risi, Sebastian},
  year = {2016},
  month = jul,
  pages = {117--124},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/2908812.2908930},
  abstract = {An unsolved problem in neuroevolution (NE) is to evolve artificial neural networks (ANN) that can store and use information to change their behavior online. While plastic neural networks have shown promise in this context, they have difficulties retaining information over longer periods of time and integrating new information without losing previously acquired skills. Here we build on recent work by Graves et al. [5] who extended the capabilities of an ANN by combining it with an external memory bank trained through gradient descent. In this paper, we introduce an evolvable version of their Neural Turing Machine (NTM) and show that such an approach greatly simplifies the neural model, generalizes better, and does not require accessing the entire memory content at each time-step. The Evolvable Neural Turing Machine (ENTM) is able to solve a simple copy tasks and for the first time, the continuous version of the double T-Maze, a complex reinforcement-like learning problem. In the T-Maze learning task the agent uses the memory bank to display adaptive behavior that normally requires a plastic ANN, thereby suggesting a complementary and effective mechanism for adaptive behavior in NE.},
  isbn = {978-1-4503-4206-3},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/BI69GASN/Greve et al. - 2016 - Evolving Neural Turing Machines for Reward-based L.pdf}
}

@article{grillBootstrapYourOwn,
  title = {Bootstrap {{Your Own Latent A New Approach}} to {{Self-Supervised Learning}}},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  pages = {14},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/VUP3Z5PN/Grill et al. - Bootstrap Your Own Latent A New Approach to Self-S.pdf}
}

@article{grillottiRelevanceguidedUnsupervisedDiscovery2022,
  title = {Relevance-Guided {{Unsupervised Discovery}} of {{Abilities}} with {{Quality-Diversity Algorithms}}},
  author = {Grillotti, Luca and Cully, Antoine},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.09828 [cs]},
  eprint = {2204.09828},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3512290.3528837},
  abstract = {Quality-Diversity algorithms provide efficient mechanisms to generate large collections of diverse and high-performing solutions, which have shown to be instrumental for solving downstream tasks. However, most of those algorithms rely on a behavioural descriptor to characterise the diversity that is hand-coded, hence requiring prior knowledge about the considered tasks. In this work, we introduce Relevance-guided Unsupervised Discovery of Abilities; a Quality-Diversity algorithm that autonomously finds a behavioural characterisation tailored to the task at hand. In particular, our method introduces a custom diversity metric that leads to higher densities of solutions near the areas of interest in the learnt behavioural descriptor space. We evaluate our approach on a simulated robotic environment, where the robot has to autonomously discover its abilities based on its full sensory data. We evaluated the algorithms on three tasks: navigation to random targets, moving forward with a high velocity, and performing half-rolls. The experimental results show that our method manages to discover collections of solutions that are not only diverse, but also well-adapted to the considered downstream task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/disc/p.templier/Zotero/storage/8SLUNAWB/Grillotti et Cully - 2022 - Relevance-guided Unsupervised Discovery of Abiliti.pdf}
}

@article{haEvolvingStableStrategies2017,
  title = {Evolving {{Stable Strategies}}},
  author = {Ha, David},
  year = {2017},
  journal = {blog.otoro.net}
}

@inproceedings{hagg2017hierarchical,
  title = {Hierarchical Surrogate Modeling for Illumination Algorithms},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion ({{GECCO}} Companion \#1)2017},
  author = {Hagg, Alexander},
  year = {2017},
  pages = {1407--1410}
}

@article{haHyperNetworks2016,
  title = {{{HyperNetworks}}},
  author = {Ha, David and Dai, Andrew M. and Le, Quoc V.},
  year = {2016},
  month = oct,
  abstract = {We train a small RNN to generate weights for a larger RNN, and train the system end-to-end.  We obtain state-of-the-art results on a variety of sequence modelling tasks.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/J8BKXH6T/Ha et al. - 2016 - HyperNetworks.pdf;/home/disc/p.templier/Zotero/storage/XLBFNMEP/forum.html}
}

@incollection{hansen2006eda,
  title = {The {{CMA}} Evolution Strategy: A Comparing Review},
  booktitle = {Towards a New Evolutionary Computation. {{Advances}} on Estimation of Distribution Algorithms},
  author = {Hansen, N.},
  editor = {Lozano, J.A. and Larranaga, P. and Inza, I. and Bengoetxea, E.},
  year = {2006},
  pages = {75--102},
  publisher = {{Springer}},
  file = {/home/disc/p.templier/Zotero/storage/SJY5LSF8/Hansen - The CMA Evolution Strategy A Comparing Review.pdf}
}

@incollection{hansen2015evolution,
  title = {Evolution Strategies},
  booktitle = {Springer Handbook of Computational Intelligence},
  author = {Hansen, Nikolaus and Arnold, Dirk V and Auger, Anne},
  year = {2015},
  pages = {871--898},
  publisher = {{Springer}},
  file = {/home/disc/p.templier/Zotero/storage/RZ5QQTTS/es-overview-2015.pdf}
}

@inproceedings{hansenAdaptingArbitraryNormal1996,
  title = {Adapting Arbitrary Normal Mutation Distributions in Evolution Strategies: The Covariance Matrix Adaptation},
  shorttitle = {Adapting Arbitrary Normal Mutation Distributions in Evolution Strategies},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Evolutionary Computation}}},
  author = {Hansen, N. and Ostermeier, A.},
  year = {1996},
  month = may,
  pages = {312--317},
  doi = {10.1109/ICEC.1996.542381},
  abstract = {A new formulation for coordinate system independent adaptation of arbitrary normal mutation distributions with zero mean is presented. This enables the evolution strategy (ES) to adapt the correct scaling of a given problem and also ensures invariance with respect to any rotation of the fitness function (or the coordinate system). Especially rotation invariance, here resulting directly from the coordinate system independent adaptation of the mutation distribution, is an essential feature of the ES with regard to its general applicability to complex fitness functions. Compared to previous work on this subject, the introduced formulation facilitates an interpretation of the resulting mutation distribution, making sensible manipulation by the user possible (if desired). Furthermore it enables a more effective control of the overall mutation variance (expected step length).},
  keywords = {arbitrary normal mutation distributions,complex fitness functions,covariance matrix,Covariance matrix,covariance matrix adaptation,derandomised adaptation,Electronic switching systems,evolution strategies,evolution strategy,evolutionary algorithms,Evolutionary computation,genetic algorithms,Genetic mutations,matrix algebra,mutation distribution,mutation distributions,rotation invariance,self-adaptation,Stochastic processes,strategy parameters},
  file = {/home/disc/p.templier/Zotero/storage/HVM3FAGW/Hansen et Ostermeier - 1996 - Adapting arbitrary normal mutation distributions i.pdf;/home/disc/p.templier/Zotero/storage/P39JAMVY/Hansen et Ostermeier - 1996 - Adapting arbitrary normal mutation distributions i.pdf;/home/disc/p.templier/Zotero/storage/PCBZFNZW/Hansen et Ostermeier - 1996 - Adapting arbitrary normal mutation distributions i.pdf;/home/disc/p.templier/Zotero/storage/7SJM5HMM/542381.html;/home/disc/p.templier/Zotero/storage/DUDP8JQQ/542381.html}
}

@book{hansenCMAESPycmaGithub2019,
  title = {{{CMA-ES}}/Pycma on {{Github}}},
  author = {Hansen, Nikolaus and Akimoto, Youhei and Baudis, Petr},
  year = {2019},
  month = feb,
  doi = {10.5281/zenodo.2559634},
  annotation = {Published: Zenodo, DOI:10.5281/zenodo.2559634}
}

@article{hansenCMAEvolutionStrategy2016,
  title = {The {{CMA Evolution Strategy}}: {{A Tutorial}}},
  shorttitle = {The {{CMA Evolution Strategy}}},
  author = {Hansen, Nikolaus},
  year = {2016},
  month = apr,
  journal = {arXiv:1604.00772 [cs, stat]},
  eprint = {1604.00772},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/UVCZVR4C/Hansen - 2016 - The CMA Evolution Strategy A Tutorial.pdf}
}

@article{hansenCompletelyDerandomizedSelfAdaptation2001,
  title = {Completely {{Derandomized Self-Adaptation}} in {{Evolution Strategies}}},
  author = {Hansen, Nikolaus and Ostermeier, Andreas},
  year = {2001},
  month = jun,
  journal = {Evolutionary Computation},
  volume = {9},
  number = {2},
  pages = {159--195},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365601750190398},
  abstract = {This paper puts forward two useful methods for self-adaptation of the mutation distribution \textendash{} the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding.},
  langid = {english},
  keywords = {cmaes},
  file = {/home/disc/p.templier/Zotero/storage/BMKAFBXD/Hansen et Ostermeier - 2001 - Completely Derandomized Self-Adaptation in Evoluti.pdf}
}

@article{hansenReducingTimeComplexity2003,
  title = {Reducing the {{Time Complexity}} of the {{Derandomized Evolution Strategy}} with {{Covariance Matrix Adaptation}} ({{CMA-ES}})},
  author = {Hansen, Nikolaus and M{\"u}ller, Sibylle and Koumoutsakos, Petros},
  year = {2003},
  month = feb,
  journal = {Evolutionary computation},
  volume = {11},
  pages = {1--18},
  doi = {10.1162/106365603321828970},
  abstract = {This paper presents a novel evolutionary optimization strategy based on the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). This new approach is intended to reduce the number of generations required for convergence to the optimum. Reducing the number of generations, i.e., the time complexity of the algorithm, is important if a large population size is desired: (1) to reduce the effect of noise; (2) to improve global search properties; and (3) to implement the algorithm on (highly) parallel machines. Our method results in a highly parallel algorithm which scales favorably with large numbers of processors. This is accomplished by efficiently incorporating the available information from a large population, thus significantly reducing the number of generations needed to adapt the covariance matrix. The original version of the CMA-ES was designed to reliably adapt the covariance matrix in small populations but it cannot exploit large populations efficiently. Our modifications scale up the efficiency to population sizes of up to 10n, where n is the problem dimension. This method has been applied to a large number of test problems, demonstrating that in many cases the CMA-ES can be advanced from quadratic to linear time complexity.},
  file = {/home/disc/p.templier/Zotero/storage/ZSIMQWTQ/Hansen et al. - 2003 - Reducing the Time Complexity of the Derandomized E.pdf}
}

@article{haRecurrentWorldModels,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  pages = {13},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/CRF6CWH4/Ha et Schmidhuber - Recurrent World Models Facilitate Policy Evolution.pdf}
}

@article{haReinforcementLearningImproving2019,
  title = {Reinforcement {{Learning}} for {{Improving Agent Design}}},
  author = {Ha, David},
  year = {2019},
  month = nov,
  journal = {Artificial Life},
  volume = {25},
  number = {4},
  pages = {352--365},
  issn = {1064-5462},
  doi = {10.1162/artl_a_00301},
  abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications.},
  file = {/home/disc/p.templier/Zotero/storage/6VSU94TS/Ha - 2019 - Reinforcement Learning for Improving Agent Design.pdf;/home/disc/p.templier/Zotero/storage/7QQ4638E/Reinforcement-Learning-for-Improving-Agent-Design.html}
}

@article{hassonDirectFitNature2020,
  title = {Direct {{Fit}} to {{Nature}}: {{An Evolutionary Perspective}} on {{Biological}} and {{Artificial Neural Networks}}},
  shorttitle = {Direct {{Fit}} to {{Nature}}},
  author = {Hasson, Uri and Nastase, Samuel and Goldstein, Ariel},
  year = {2020},
  month = feb,
  journal = {Neuron},
  volume = {105},
  pages = {416--434},
  doi = {10.1016/j.neuron.2019.12.002},
  abstract = {Evolution is a blind fitting process by which organisms become adapted to their environment. Does the brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in artificial neural networks have exposed the power of optimizing millions of synaptic weights over millions of observations to operate robustly in real-world contexts. These models do not learn simple, human-interpretable rules or representations of the world; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Counterintuitively, similar to evolutionary processes, over-parameterized models can be simple and parsimonious, as they provide a versatile, robust solution for learning a diverse set of functions. This new family of direct-fit models present a radical challenge to many of the theoretical assumptions in psychology and neuroscience. At the same time, this shift in perspective establishes unexpected links with developmental and ecological psychology.},
  file = {/home/disc/p.templier/Zotero/storage/9FDJC4RQ/Hasson et al. - 2020 - Direct Fit to Nature An Evolutionary Perspective .pdf}
}

@article{hausknechtNeuroevolutionApproachGeneral2014,
  title = {A {{Neuroevolution Approach}} to {{General Atari Game Playing}}},
  author = {Hausknecht, M. and Lehman, J. and Miikkulainen, R. and Stone, P.},
  year = {2014},
  month = mar,
  journal = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {6},
  number = {4},
  pages = {355--366},
  issn = {1943-0698},
  doi = {10.1109/TCIAIG.2013.2294713},
  abstract = {This paper addresses the challenge of learning to play many different video games with little domain-specific knowledge. Specifically, it introduces a neuroevolution approach to general Atari 2600 game playing. Four neuroevolution algorithms were paired with three different state representations and evaluated on a set of 61 Atari games. The neuroevolution agents represent different points along the spectrum of algorithmic sophistication - including weight evolution on topologically fixed neural networks (conventional neuroevolution), covariance matrix adaptation evolution strategy (CMA-ES), neuroevolution of augmenting topologies (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e., HyperNEAT) allow scaling to higher dimensional representations (i.e., the raw game screen). Previous approaches based on temporal-difference (TD) learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuroevolution is a promising approach to general video game playing (GVGP).},
  keywords = {Algorithm design and analysis,algorithmic sophistication,Algorithms,artificial neural networks,Artificial neural networks,CMA-ES,compact state representation,computer games,conventional neuroevolution,covariance matrices,covariance matrix adaptation evolution strategy,domain-specific knowledge,Encoding,evolutionary computation,Games,general Atari 2600 game playing,general video game playing,genetic algorithms,GVGP,higher dimensional representation,HyperNEAT,indirect network encoding,indirect-encoding method,learning (artificial intelligence),multi-agent systems,Network topology,neural nets,neural networks,neuroevolution agents,neuroevolution algorithm,neuroevolution approach,neuroevolution of augmenting topology,object representation,raw game screen,sparse reward gradient,state representations,state spaces,TD learning,temporal-difference learning,Topology,video games,weight evolution},
  file = {/home/disc/p.templier/Zotero/storage/F42W49TU/Hausknecht et al. - 2014 - A Neuroevolution Approach to General Atari Game Pl.pdf;/home/disc/p.templier/Zotero/storage/IKGW28QR/Hausknecht et al. - 2014 - A Neuroevolution Approach to General Atari Game Pl.pdf;/home/disc/p.templier/Zotero/storage/HUQ6RVTN/6756960.html;/home/disc/p.templier/Zotero/storage/ZNNWAEJ5/6756960.html}
}

@inproceedings{henderson2018deep,
  title = {Deep Reinforcement Learning That Matters},
  booktitle = {Proceedings of the \#1 {{AAAI}} Conference on Artificial Intelligence ({{AAAI}} \#2){{32nd2018}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  year = {2018},
  volume = {32}
}

@article{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.02298 [cs]},
  eprint = {1710.02298},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/E4SSG8TC/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/home/disc/p.templier/Zotero/storage/FZ2GHEJ7/1710.html}
}

@article{holland1975adaptation,
  title = {Adaptation in Natural and Artificial Systems},
  author = {Holland, JH},
  year = {1975},
  journal = {Ann Arbor: University of Michigan Press}
}

@inproceedings{hornbyAutomatedAntennaDesign2006,
  title = {Automated {{Antenna Design}} with {{Evolutionary Algorithms}}},
  booktitle = {Space 2006},
  author = {Hornby, Gregory and Globus, Al and Linden, Derek and Lohn, Jason},
  year = {2006},
  month = sep,
  publisher = {{American Institute of Aeronautics and Astronautics}},
  address = {{San Jose, California}},
  doi = {10.2514/6.2006-7242},
  isbn = {978-1-62410-049-9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/2MK2FMJZ/Hornby et al. - 2006 - Automated Antenna Design with Evolutionary Algorit.pdf}
}

@misc{hutsonComputersEvolveNew,
  title = {Computers {{Evolve}} a {{New Path Toward Human Intelligence}}},
  author = {Hutson, Matthew},
  journal = {Quanta Magazine},
  abstract = {By ignoring their goals, evolutionary algorithms have solved longstanding challenges in artificial intelligence.},
  howpublished = {https://www.quantamagazine.org/computers-evolve-a-new-path-toward-human-intelligence-20191106/},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/HLHV3JCE/computers-evolve-a-new-path-toward-human-intelligence-20191106.html}
}

@article{innesFluxElegantMachine,
  title = {Flux: {{Elegant}} Machine Learning with {{Julia}}},
  author = {Innes, Mike},
  pages = {1},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/D2EV2EZQ/Innes - Flux Elegant machine learning with Julia.pdf}
}

@article{irieModernSelfReferentialWeight,
  title = {A {{Modern Self-Referential Weight Matrix That Learns}} to {{Modify Itself}}},
  author = {Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  pages = {11},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/BLG524MT/Irie et al. - A Modern Self-Referential Weight Matrix That Learn.pdf}
}

@article{jaderbergOpenEndedLearningLeads,
  title = {Open-{{Ended}} Learning Leads to {{Generally Capable Agents}}},
  author = {Jaderberg, Max and Mathieu, Michael and McAleese, Nat and {Bradley-Schmieg}, Nathalie and Wong, Nathaniel and Porcel, Nicolas and {Hughes-Fitt}, Steph and Dalibard, Valentin and Czarnecki, Wojciech Marian},
  pages = {54},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/6IKU2KSU/Jaderberg et al. - Open-Ended learning leads to Generally Capable Age.pdf}
}

@article{jaderbergPopulationBasedTraining2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.09846 [cs]},
  eprint = {1711.09846},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present Population Based Training (PBT), a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/ZITZVA7L/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf}
}

@inproceedings{jakobi1995noise,
  title = {Noise and the Reality Gap: {{The}} Use of Simulation in Evolutionary Robotics},
  booktitle = {European Conference on Artificial Life},
  author = {Jakobi, Nick and Husbands, Phil and Harvey, Inman},
  year = {1995},
  pages = {704--720},
  organization = {{Springer}}
}

@book{janicekimLearnPlayGo,
  title = {Learn to Play {{Go}}, {{A Master}}'s {{Guide}} to the {{Ultimate Game}}, {{Vol}}. 1},
  author = {{Janice Kim} and {Jeong Soo-Hyun}},
  file = {/home/disc/p.templier/Zotero/storage/3JR7UV8U/_.pdf}
}

@book{jesseschellArtGameDesign,
  title = {The {{Art}} of {{Game Design}}},
  author = {{Jesse Schell}},
  file = {/home/disc/p.templier/Zotero/storage/9FICN2MQ/_.pdf}
}

@article{jinComprehensiveSurveyFitness2005,
  title = {A Comprehensive Survey of Fitness Approximation in Evolutionary Computation},
  author = {Jin, Y.},
  year = {2005},
  month = jan,
  journal = {Soft Computing},
  volume = {9},
  number = {1},
  pages = {3--12},
  issn = {1432-7643, 1433-7479},
  doi = {10.1007/s00500-003-0328-5},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/66E5YPRE/Jin - 2005 - A comprehensive survey of fitness approximation in.pdf}
}

@article{jinSurrogateassistedEvolutionaryComputation2011,
  title = {Surrogate-Assisted Evolutionary Computation: {{Recent}} Advances and Future Challenges},
  shorttitle = {Surrogate-Assisted Evolutionary Computation},
  author = {Jin, Yaochu},
  year = {2011},
  month = jun,
  journal = {Swarm and Evolutionary Computation},
  volume = {1},
  number = {2},
  pages = {61--70},
  issn = {22106502},
  doi = {10.1016/j.swevo.2011.05.001},
  abstract = {Surrogate-assisted, or meta-model based evolutionary computation uses efficient computational models, often known as surrogates or meta-models, for approximating the fitness function in evolutionary algorithms. Research on surrogate-assisted evolutionary computation began over a decade ago and has received considerably increasing interest in recent years. Very interestingly, surrogate-assisted evolutionary computation has found successful applications not only in solving computationally expensive single- or multi-objective optimization problems, but also in addressing dynamic optimization problems, constrained optimization problems and multi-modal optimization problems. This paper provides a concise overview of the history and recent developments in surrogate-assisted evolutionary computation and suggests a few future trends in this research area.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/N5XXKQPP/Jin - 2011 - Surrogate-assisted evolutionary computation Recen.pdf}
}

@misc{JuliaLangJulia,
  title = {{{JuliaLang}}/Julia},
  journal = {GitHub},
  abstract = {The Julia Programming Language. Contribute to JuliaLang/julia development by creating an account on GitHub.},
  howpublished = {https://github.com/JuliaLang/julia},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/XNX6FWWJ/CITATION.html}
}

@inproceedings{kaiserModelBasedReinforcement2019,
  title = {Model {{Based Reinforcement Learning}} for {{Atari}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kaiser, {\L}ukasz and Babaeizadeh, Mohammad and Mi{\l}os, Piotr and Osi{\'n}ski, B{\l}a{\.z}ej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2019},
  month = sep,
  abstract = {We use video prediction models, a model-based reinforcement learning algorithm and 2h of gameplay per game to train agents for 26 Atari games.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/AAI47CD9/Kaiser et al. - 2019 - Model Based Reinforcement Learning for Atari.pdf;/home/disc/p.templier/Zotero/storage/MN822RDV/forum.html}
}

@inproceedings{kalyanakrishnan2010efficient,
  title = {Efficient Selection of Multiple Bandit Arms: {{Theory}} and Practice},
  booktitle = {Proceedings of the \#1 International Conference on Machine Learning ({{ICML}} \#2){{27th2010}}},
  author = {Kalyanakrishnan, Shivaram and Stone, Peter},
  year = {2010}
}

@phdthesis{kalyanakrishnan2011learning,
  title = {Learning Methods for Sequential Decision Making with Imperfect Representations},
  author = {Kalyanakrishnan, Shivaram},
  year = {2011},
  school = {Citeseer}
}

@inproceedings{kalyanakrishnan2012pac,
  title = {{{PAC}} Subset Selection in Stochastic Multi-Armed Bandits},
  booktitle = {Proceedings of the \#1 International Conference on Machine Learning ({{ICML}} \#2){{29th2012}}},
  author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
  year = {2012},
  volume = {12},
  pages = {655--662}
}

@inproceedings{kellyEmergentTangledProgram2018,
  title = {Emergent {{Tangled Program Graphs}} in {{Multi-Task Learning}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}, {{IJCAI-18}}},
  author = {Kelly, Stephen and Heywood, Malcolm},
  year = {2018},
  month = jul,
  pages = {5294--5298},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2018/740}
}

@incollection{kernLocalMetamodelsOptimization2006,
  title = {Local {{Meta-models}} for {{Optimization Using Evolution Strategies}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} - {{PPSN IX}}},
  author = {Kern, Stefan and Hansen, Nikolaus and Koumoutsakos, Petros},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Runarsson, Thomas Philip and Beyer, Hans-Georg and Burke, Edmund and {Merelo-Guerv{\'o}s}, Juan J. and Whitley, L. Darrell and Yao, Xin},
  year = {2006},
  volume = {4193},
  pages = {939--948},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11844297_95},
  abstract = {We employ local meta-models to enhance the efficiency of evolution strategies in the optimization of computationally expensive problems. The method involves the combination of second order local regression meta-models with the Covariance Matrix Adaptation Evolution Strategy. Experiments on benchmark problems demonstrate that the proposed meta-models have the potential to reliably account for the ranking of the offspring population resulting in significant computational savings. The results show that the use of local meta-models significantly increases the efficiency of already competitive evolution strategies.},
  isbn = {978-3-540-38990-3 978-3-540-38991-0},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/KIRNG6LQ/Kern et al. - 2006 - Local Meta-models for Optimization Using Evolution.pdf}
}

@article{khadkaEvolutionGuidedPolicyGradient2018,
  title = {Evolution-{{Guided Policy Gradient}} in {{Reinforcement Learning}}},
  author = {Khadka, Shauharda and Tumer, Kagan},
  year = {2018},
  month = oct,
  journal = {arXiv:1805.07917 [cs, stat]},
  eprint = {1805.07917},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/RPT79CRF/Khadka et Tumer - 2018 - Evolution-Guided Policy Gradient in Reinforcement .pdf}
}

@article{khan2013fast,
  title = {Fast Learning Neural Networks Using Cartesian Genetic Programming},
  author = {Khan, Maryam Mahsal and Ahmad, Arbab Masood and Khan, Gul Muhammad and Miller, Julian F},
  year = {2013},
  journal = {Neurocomputing},
  volume = {121},
  pages = {274--289},
  publisher = {{Elsevier}}
}

@article{kimDeepClusteredConvolutional2015,
  title = {Deep {{Clustered Convolutional Kernels}}},
  author = {Kim, Minyoung and Rigazio, Luca},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.01824 [cs]},
  eprint = {1503.01824},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks have recently achieved state of the art performance thanks to new training algorithms for rapid parameter estimation and new regularization methods to reduce overfitting. However, in practice the network architecture has to be manually set by domain experts, generally by a costly trial and error procedure, which often accounts for a large portion of the final system performance. We view this as a limitation and propose a novel training algorithm that automatically optimizes network architecture, by progressively increasing model complexity and then eliminating model redundancy by selectively removing parameters at training time. For convolutional neural networks, our method relies on iterative split/merge clustering of convolutional kernels interleaved by stochastic gradient descent. We present a training algorithm and experimental results on three different vision tasks, showing improved performance compared to similarly sized hand-crafted architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/MJ66LKGJ/Kim et Rigazio - 2015 - Deep Clustered Convolutional Kernels.pdf}
}

@inproceedings{kimHybridEvolutionReinforcement2007,
  title = {Hybrid of {{Evolution}} and {{Reinforcement Learning}} for {{Othello Players}}},
  booktitle = {2007 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Games}}},
  author = {Kim, Kyung-Joong and Choi, Heejin and Cho, Sung-Bae},
  year = {2007},
  month = apr,
  pages = {203--209},
  issn = {2325-4289},
  doi = {10.1109/CIG.2007.368099},
  abstract = {Although the reinforcement learning and evolutionary algorithm show good results in board evaluation optimization, the hybrid of both approaches is rarely addressed in the literature. In this paper, the evolutionary algorithm is boosted using resources from the reinforcement learning. 1) The initialization of initial population using solution optimized by temporal difference learning 2) Exploitation of domain knowledge extracted from reinforcement learning. Experiments on Othello game strategies show that the proposed methods can effectively search the solution space and improve the performance},
  keywords = {Books,Computational intelligence,Computer science,Domain Knowledge,Evolutionary computation,Fluctuations,Learning systems,Optimization methods,Othello,Reinforcement Learning,Space exploration,Temporal Difference Learning},
  file = {/home/disc/p.templier/Zotero/storage/JAUPGL8V/Kim et al. - 2007 - Hybrid of Evolution and Reinforcement Learning for.pdf;/home/disc/p.templier/Zotero/storage/EMM4QN2D/4219044.html}
}

@article{kingma2013auto,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P and Welling, Max},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6114},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  file = {/home/disc/p.templier/Zotero/storage/C6FN465E/Kingma et Ba - 2015 - Adam A Method for Stochastic Optimization.pdf}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = jan,
  journal = {arXiv:1612.00796 [cs, stat]},
  eprint = {1612.00796},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/T5S9C99D/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf;/home/disc/p.templier/Zotero/storage/57EQ9WBD/1612.html}
}

@article{kitano1990designing,
  title = {Designing Neural Networks Using Genetic Algorithms with Graph Generation System},
  author = {Kitano, Hiroaki},
  year = {1990},
  journal = {Complex systems},
  volume = {4},
  pages = {461--476}
}

@inproceedings{koutnik2010evolving,
  title = {Evolving Neural Networks in Compressed Weight Space},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Koutnik, Jan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2010},
  pages = {619--626}
}

@inproceedings{koutnikEvolvingLargescaleNeural2013,
  title = {Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning},
  booktitle = {Proceedings of the 15th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Koutn{\'i}k, Jan and Cuccu, Giuseppe and Schmidhuber, J{\"u}rgen and Gomez, Faustino},
  year = {2013},
  month = jul,
  series = {{{GECCO}} '13},
  pages = {1061--1068},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2463372.2463509},
  abstract = {The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), for reinforcement learning (RL) tasks has now been around for over 20 years. However, as RL tasks become more challenging, the networks required become larger, as do their genomes. But, scaling NE to large nets (i.e. tens of thousands of weights) is infeasible using direct encodings that map genes one-to-one to network components. In this paper, we scale-up our compressed network encoding where network weight matrices are represented indirectly as a set of Fourier-type coefficients, to tasks that require very-large networks due to the high-dimensionality of their input space. The approach is demonstrated successfully on two reinforcement learning tasks in which the control networks receive visual input: (1) a vision-based version of the octopus control task requiring networks with over 3 thousand weights, and (2) a version of the TORCS driving game where networks with over 1 million weights are evolved to drive a car around a track using video images from the driver's perspective.},
  isbn = {978-1-4503-1963-8},
  keywords = {computer vision,neuroevolution,recurrent neural networks,reinforcement learning},
  file = {/home/disc/p.templier/Zotero/storage/R24523FB/Koutník et al. - 2013 - Evolving large-scale neural networks for vision-ba.pdf}
}

@inproceedings{kubotaRoleVirusInfection1996,
  title = {The Role of Virus Infection in Virus-Evolutionary Genetic Algorithm},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Evolutionary Computation}}},
  author = {Kubota, N. and Shimojima, K. and Fukuda, T.},
  year = {1996},
  pages = {182--187},
  publisher = {{IEEE}},
  address = {{Nagoya, Japan}},
  doi = {10.1109/ICEC.1996.542357},
  isbn = {978-0-7803-2902-7},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/VRMFA5UG/Kubota et al. - 1996 - The role of virus infection in virus-evolutionary .pdf}
}

@inproceedings{kubotaVirusevolutionaryGeneticAlgorithmecological1996,
  title = {Virus-Evolutionary Genetic Algorithm-Ecological Model on Planar Grid},
  booktitle = {Proceedings of {{North American Fuzzy Information Processing}}},
  author = {Kubota, N. and Shimojima, K. and Fukuda, T.},
  year = {1996},
  month = jun,
  pages = {505--509},
  doi = {10.1109/NAFIPS.1996.534786},
  abstract = {This paper deals with an ecological model on a planar gird of a genetic algorithm based on the virus theory of evolution (VEGA). VEGA assumes horizontal propagation and vertical inheritance of genetic information in a population with virus infection operators and generic operators. The main operator of VEGA is a reverse transcription operator, which plays the roles of crossover and selection simultaneously. The convergence and genetic diversity of the ecological model of VEGA (E-VEGA) depend on the frequency and localization of the virus infection. We apply E-VEGA to the traveling salesman problem and discuss its effectiveness through numerical simulation.},
  keywords = {Biological system modeling,Character generation,Computational modeling,Convergence,Evolution (biology),Frequency diversity,Genetic algorithms,Genetic engineering,Systems engineering and theory,Traveling salesman problems},
  file = {/home/disc/p.templier/Zotero/storage/QF8LHCZR/Kubota et al. - 1996 - Virus-evolutionary genetic algorithm-ecological mo.pdf;/home/disc/p.templier/Zotero/storage/7M3LM858/534786.html}
}

@article{kullback1951information,
  title = {On Information and Sufficiency},
  author = {Kullback, Solomon and Leibler, Richard A},
  year = {1951},
  journal = {The annals of mathematical statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {{JSTOR}}
}

@article{lairdHumanLevelAIKiller2001,
  title = {Human-{{Level AI}}'s {{Killer Application Interactive Computer Games}}},
  author = {Laird, John E. and Lent, Michael Van},
  year = {2001},
  journal = {AI Magazine Volume 22 Number 2 (2001) (\textcopyright{} AAAI)},
  abstract = {s Although one of the fundamental goals of AI is to understand and develop intelligent systems that have all the capabilities of humans, there is little active research directly pursuing this goal. We propose that AI for interactive computer games is an emerging application area in which this goal of human-level AI can successfully be pursued. Interactive computer games have increasingly complex and realistic worlds and increasingly complex and intelligent computer-controlled characters. In this article, we further motivate our proposal of using interactive computer games for AI research, review previous research on AI and games, and present the different game genres and the roles that human-level AI could play within these genres. We then describe the research issues and AI techniques that are relevant to each of these roles. Our conclusion is that interactive computer games provide a rich environment for incremental research on human-level AI. O ver the last 30 years, research in AI has fragmented into more and more specialized fields, working on more and more specialized problems, using more and more specialized algorithms. This approach has led to a long string of successes with important theoretical and practical advancements. However, these successes have made it easy for us to ignore our failure to make significant progress in building human-level AI systems. Human-level AI systems are the ones that you dreamed about when you first heard of AI: HAL from 2001, A Space Odyssey; DATA from Star Trek; or CP30 and R2D2 from Star Wars. They are smart enough to be both triumphant heroes and devious villains. They seamlessly integrate all the human-level capabilities: real-time response, robustness, autonomous intelligent interaction with their environment, planning, communication with natural language, com-monsense reasoning, creativity, and learning. If this is our dream, why isn't any progress being made? Ironically, one of the major reasons that almost nobody (see Brooks et al. [2000] for one high-profile exception) is working on this grand goal of AI is that current applications of AI do not need full-blown human-level AI. For almost all applications, the generality and adaptability of human thought is not needed-specialized, although more rigid and fragile, solutions are cheaper and easier to develop. Unfortunately, it is unclear whether the approaches that have been developed to solve specific problems are the right building blocks for creating human-level intelligence. The thesis of this article is that interactive computer games are the killer application for human-level AI. They are the application that will need human-level AI. Moreover, they can provide the environments for research on the right kinds of problem that lead to the type of incremental and integrative research needed to achieve human-level AI. Computer-Generated Forces Given that our personal goal is to build human-level AI systems, we have struggled to find the right application for our research that requires the breadth, depth, and flexibility of human-level intelligence. In 1991, we found computer-generated forces for large-scale distributed simulations as a potential application. Effective military training requires a complete battle space with tens if not hundreds or thousands of participants. The real world is too expensive and dangerous to use for continual training,}
}

@book{larranaga2001estimation,
  title = {Estimation of Distribution Algorithms: {{A}} New Tool for Evolutionary Computation},
  author = {Larra{\~n}aga, Pedro and Lozano, Jose A},
  year = {2001},
  volume = {2},
  publisher = {{Springer Science \& Business Media}}
}

@book{lattimore2020bandit,
  title = {Bandit Algorithms},
  author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year = {2020},
  publisher = {{Cambridge University Press}}
}

@inproceedings{lecarpentierLUCIEEvaluationSelection2022,
  title = {{{LUCIE}}: {{An Evaluation}} and {{Selection Method}} for {{Stochastic Problems}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} ({{GECCO}} 2022)},
  author = {Lecarpentier, Erwan and Templier, Paul and Rachelson, Emmanuel and Wilson, Dennis G.},
  year = {2022}
}

@article{lehmanAbandoningObjectivesEvolution2011,
  title = {Abandoning {{Objectives}}: {{Evolution}} through the {{Search}} for {{Novelty Alone}}},
  author = {Lehman, Joel and Stanley, Kenneth O},
  year = {2011},
  pages = {39},
  abstract = {In evolutionary computation, the fitness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/GZHBWATE/Lehman et Stanley - Abandoning Objectives Evolution through the Searc.pdf}
}

@inproceedings{lehmanESMoreJust2018,
  title = {{{ES}} Is More than Just a Traditional Finite-Difference Approximator},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2018},
  month = jul,
  pages = {450--457},
  publisher = {{ACM}},
  address = {{Kyoto Japan}},
  doi = {10.1145/3205455.3205474},
  abstract = {An evolution strategy (ES) variant based on a simplification of a natural evolution strategy recently attracted attention because it performs surprisingly well in challenging deep reinforcement learning domains. It searches for neural network parameters by generating perturbations to the current set of parameters, checking their performance, and moving in the aggregate direction of higher reward. Because it resembles a traditional finite-difference approximation of the reward gradient, it can naturally be confused with one. However, this ES optimizes for a different gradient than just reward: It optimizes for the average reward of the entire population, thereby seeking parameters that are robust to perturbation. This difference can channel ES into distinct areas of the search space relative to gradient descent, and also consequently to networks with distinct properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are significantly less robust to parameter perturbation than ES-based policies solving the same task. While the implications of such robustness and robustness-seeking remain open to further study, this work's main contribution is to highlight such differences and their potential importance.},
  isbn = {978-1-4503-5618-3},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/CSGZ7JUG/Lehman et al. - 2018 - ES is more than just a traditional finite-differen.pdf}
}

@misc{lehmanEvolutionLargeModels2022,
  title = {Evolution through {{Large Models}}},
  author = {Lehman, Joel and Gordon, Jonathan and Jain, Shawn and Ndousse, Kamal and Yeh, Cathy and Stanley, Kenneth O.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08896},
  eprint = {2206.08896},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This paper pursues the insight that large language models (LLMs) trained to generate code can vastly improve the effectiveness of mutation operators applied to programs in genetic programming (GP). Because such LLMs benefit from training data that includes sequential changes and modifications, they can approximate likely changes that humans would make. To highlight the breadth of implications of such evolution through large models (ELM), in the main experiment ELM combined with MAPElites generates hundreds of thousands of functional examples of Python programs that output working ambulating robots in the Sodarace domain, which the original LLM had never seen in pre-training. These examples then help to bootstrap training a new conditional language model that can output the right walker for a particular terrain. The ability to bootstrap new models that can output appropriate artifacts for a given context in a domain where zero training data was previously available carries implications for open-endedness, deep learning, and reinforcement learning. These implications are explored here in depth in the hope of inspiring new directions of research now opened up by ELM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/9X54Q5MZ/Lehman et al. - 2022 - Evolution through Large Models.pdf}
}

@inproceedings{lehmanEvolvingDiversityVirtual2011,
  title = {Evolving a Diversity of Virtual Creatures through Novelty Search and Local Competition},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Lehman, Joel and Stanley, Kenneth O},
  year = {2011},
  pages = {211--218}
}

@inproceedings{lehmanSafeMutationsDeep2018,
  title = {Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2018},
  month = jul,
  series = {{{GECCO}} '18},
  pages = {117--124},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3205455.3205473},
  abstract = {While neuroevolution (evolving neural networks) has been successful across a variety of domains from reinforcement learning, to artificial life, to evolutionary robotics, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights will likely break existing functionality. This paper proposes a solution: a family of safe mutation (SM) operators that facilitate exploration without dramatically altering network behavior or requiring additional interaction with the environment. The most effective SM variant scales the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks, including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
  isbn = {978-1-4503-5618-3},
  keywords = {deep learning,mutation,neuroevolution,recurrent networks},
  file = {/home/disc/p.templier/Zotero/storage/JTNKRYL7/Lehman et al. - 2018 - Safe mutations for deep and recurrent neural netwo.pdf}
}

@article{lehmanSurprisingCreativityDigital2018,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'e}noy, Antoine and Gagn{\'e}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c c}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  year = {2018},
  month = mar,
  abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.}
}

@article{lencNonDifferentiableSupervisedLearning2019,
  title = {Non-{{Differentiable Supervised Learning}} with {{Evolution Strategies}} and {{Hybrid Methods}}},
  author = {Lenc, Karel and Elsen, Erich and Schaul, Tom and Simonyan, Karen},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.03139 [cs, stat]},
  eprint = {1906.03139},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work we show that Evolution Strategies (ES) are a viable method for learning non-differentiable parameters of large supervised models. ES are black-box optimization algorithms that estimate distributions of model parameters; however they have only been used for relatively small problems so far. We show that it is possible to scale ES to more complex tasks and models with millions of parameters. While using ES for differentiable parameters is computationally impractical (although possible), we show that a hybrid approach is practically feasible in the case where the model has both differentiable and non-differentiable parameters. In this approach we use standard gradient-based methods for learning differentiable weights, while using ES for learning non-differentiable parameters - in our case sparsity masks of the weights. This proposed method is surprisingly competitive, and when parallelized over multiple devices has only negligible training time overhead compared to training with gradient descent. Additionally, this method allows to train sparse models from the first training step, so they can be much larger than when using methods that require training dense models first. We present results and analysis of supervised feed-forward models (such as MNIST and CIFAR-10 classification), as well as recurrent models, such as SparseWaveRNN for text-to-speech.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/SZ3B5JJ6/Lenc et al. - 2019 - Non-Differentiable Supervised Learning with Evolut.pdf;/home/disc/p.templier/Zotero/storage/RPUZEUGT/1906.html}
}

@article{lillicrapRandomSynapticFeedback2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2016},
  month = nov,
  journal = {Nature Communications},
  volume = {7},
  number = {1},
  pages = {13276},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
  copyright = {2016 The Author(s)},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/TEKJBLS8/Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf;/home/disc/p.templier/Zotero/storage/86RGRPER/ncomms13276.html}
}

@article{limDynamicsAwareQualityDiversityEfficient2021,
  title = {Dynamics-{{Aware Quality-Diversity}} for {{Efficient Learning}} of {{Skill Repertoires}}},
  author = {Lim, Bryan and Grillotti, Luca and Bernasconi, Lorenzo and Cully, Antoine},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.08522 [cs]},
  eprint = {2109.08522},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Quality-Diversity (QD) algorithms are powerful exploration algorithms that allow robots to discover large repertoires of diverse and high-performing skills. However, QD algorithms are sample inefficient and require millions of evaluations. In this paper, we propose Dynamics-Aware Quality-Diversity (DA-QD), a framework to improve the sample efficiency of QD algorithms through the use of dynamics models. We also show how DA-QD can then be used for continual acquisition of new skill repertoires. To do so, we incrementally train a deep dynamics model from experience obtained when performing skill discovery using QD. We can then perform QD exploration in imagination with an imagined skill repertoire. We evaluate our approach on three robotic experiments. First, our experiments show DA-QD is 20 times more sample efficient than existing QD approaches for skill discovery. Second, we demonstrate learning an entirely new skill repertoire in imagination to perform zero-shot learning. Finally, we show how DA-QD is useful and effective for solving a long horizon navigation task and for damage adaptation in the real world. Videos and source code are available at: https://sites.google.com/view/da-qd.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/disc/p.templier/Zotero/storage/MUB2AR9L/Lim et al. - 2021 - Dynamics-Aware Quality-Diversity for Efficient Lea.pdf}
}

@article{linTruthfulQAMeasuringHow,
  title = {{{TruthfulQA}}: {{Measuring How Models Mimic Human Falsehoods}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  pages = {35},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17\% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/NZFMNYSW/Lin et al. - TruthfulQA Measuring How Models Mimic Human False.pdf}
}

@article{liuHierarchicalRepresentationsEfficient2018,
  title = {Hierarchical {{Representations}} for {{Efficient Architecture Search}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2018},
  month = feb,
  journal = {arXiv:1711.00436 [cs, stat]},
  eprint = {1711.00436},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6\% on CIFAR-10 and 20.3\% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3\% less top-1 accuracy on CIFAR-10 and 0.1\% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/MTJ4P7IM/Liu et al. - 2018 - Hierarchical Representations for Efficient Archite.pdf;/home/disc/p.templier/Zotero/storage/ZNG7SPFV/1711.html}
}

@inproceedings{liuSelfGuidedEvolutionStrategies2020,
  title = {Self-{{Guided Evolution Strategies}} with {{Historical Estimated Gradients}}},
  booktitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Liu, Fei-Yu and Li, Zi-Niu and Qian, Chao},
  year = {2020},
  month = jul,
  volume = {2},
  pages = {1474--1480},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2020/205},
  abstract = {Electronic proceedings of IJCAI 2020},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/I72UGZSS/Liu et al. - 2020 - Self-Guided Evolution Strategies with Historical E.pdf;/home/disc/p.templier/Zotero/storage/L43QNWUT/205.html}
}

@article{liuSurveyEvolutionaryNeural2020,
  title = {A {{Survey}} on {{Evolutionary Neural Architecture Search}}},
  author = {Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.10937 [cs]},
  eprint = {2008.10937},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep Neural Networks (DNNs) have achieved great success in many applications, such as image classification, natural language processing and speech recognition. The architectures of DNNs have been proved to play a crucial role in its performance. However, designing architectures for different tasks is a difficult and time-consuming process of trial and error. Neural Architecture Search (NAS), which received great attention in recent years, can design the architecture automatically. Among different kinds of NAS methods, Evolutionary Computation (EC) based NAS methods have recently gained much attention and success. Unfortunately, there is not a comprehensive summary of the EC-based methods. This paper reviews 100+ papers of EC-based NAS methods in light of the common process. Four steps of the process have been covered in this paper including population initialization, population operators, evaluation and selection. Furthermore, current challenges and issues are also discussed to identify future research in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/TWIPGYT3/Liu et al. - 2020 - A Survey on Evolutionary Neural Architecture Searc.pdf;/home/disc/p.templier/Zotero/storage/4EQZUXUK/2008.html}
}

@article{liuTrustRegionEvolution2019,
  title = {Trust {{Region Evolution Strategies}}},
  author = {Liu, Guoqing and Zhao, Li and Yang, Feidiao and Bian, Jiang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {4352--4359},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33014352},
  abstract = {Evolution Strategies (ES), a class of black-box optimization algorithms, has recently been demonstrated to be a viable alternative to popular MDP-based RL techniques such as Qlearning and Policy Gradients. ES achieves fairly good performance on challenging reinforcement learning problems and is easier to scale in a distributed setting. However, standard ES algorithms perform one gradient update per data sample, which is not very efficient. In this paper, with the purpose of more efficient using of sampled data, we propose a novel iterative procedure that optimizes a surrogate objective function, enabling to reuse data sample for multiple epochs of updates. We prove monotonic improvement guarantee for such procedure. By making several approximations to the theoretically-justified procedure, we further develop a practical algorithm called Trust Region Evolution Strategies (TRES). Our experiments demonstrate the effectiveness of TRES on a range of popular MuJoCo locomotion tasks in the OpenAI Gym, achieving better performance than ES algorithm.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/DIDUSYRQ/Liu et al. - 2019 - Trust Region Evolution Strategies.pdf}
}

@article{lopez-ibanezIracePackageIterated2016,
  title = {The Irace Package: {{Iterated}} Racing for Automatic Algorithm Configuration},
  shorttitle = {The Irace Package},
  author = {{L{\'o}pez-Ib{\'a}{\~n}ez}, Manuel and {Dubois-Lacoste}, J{\'e}r{\'e}mie and P{\'e}rez C{\'a}ceres, Leslie and Birattari, Mauro and St{\"u}tzle, Thomas},
  year = {2016},
  journal = {Operations Research Perspectives},
  volume = {3},
  pages = {43--58},
  issn = {22147160},
  doi = {10.1016/j.orp.2016.09.002},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/8R8NMMRN/López-Ibáñez et al. - 2016 - The irace package Iterated racing for automatic a.pdf}
}

@article{loshchilovCMAESHyperparameterOptimization2016,
  title = {{{CMA-ES}} for Hyperparameter Optimization of Deep Neural Networks},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2016},
  journal = {arXiv preprint arXiv:1604.07269},
  eprint = {1604.07269},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@incollection{lucasFastEvolutionaryAdaptation2014,
  title = {Fast {{Evolutionary Adaptation}} for {{Monte Carlo Tree Search}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {Lucas, Simon M. and Samothrakis, Spyridon and P{\'e}rez, Diego},
  editor = {{Esparcia-Alc{\'a}zar}, Anna I. and Mora, Antonio M.},
  year = {2014},
  volume = {8602},
  pages = {349--360},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-45523-4_29},
  abstract = {This paper describes a new adaptive Monte Carlo Tree Search (MCTS) algorithm that uses evolution to rapidly optimise its performance. An evolutionary algorithm is used as a source of control parameters to modify the behaviour of each iteration (i.e. each simulation or roll-out) of the MCTS algorithm; in this paper we largely restrict this to modifying the behaviour of the random default policy, though it can also be applied to modify the tree policy.},
  isbn = {978-3-662-45522-7 978-3-662-45523-4},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/A7UFX6YJ/Lucas et al. - 2014 - Fast Evolutionary Adaptation for Monte Carlo Tree .pdf}
}

@inproceedings{luNSGANetNeuralArchitecture2019,
  title = {{{NSGA-Net}}: Neural Architecture Search Using Multi-Objective Genetic Algorithm},
  shorttitle = {{{NSGA-Net}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Lu, Zhichao and Whalen, Ian and Boddeti, Vishnu and Dhebar, Yashesh and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang},
  year = {2019},
  month = jul,
  pages = {419--427},
  publisher = {{ACM}},
  address = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321729},
  abstract = {This paper introduces NSGA-Net \textendash{} an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a procedure considering multiple and conflicting objectives, (2) an efficient procedure balancing exploration and exploitation of the space of potential neural network architectures, and (3) a procedure finding a diverse set of trade-off network architectures achieved in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures, and finally an exploitation step that utilizes the hidden useful knowledge stored in the entire history of evaluated neural architectures in the form of a Bayesian Network. Experimental results suggest that combining the dual objectives of minimizing an error metric and computational complexity, as measured by FLOPs, allows NSGA-Net to find competitive neural architectures. Moreover, NSGA-Net achieves error rate on the CIFAR-10 dataset on par with other state-of-the-art NAS methods while using orders of magnitude less computational resources. These results are encouraging and shows the promise to further use of EC methods in various deep-learning paradigms.},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/5HRGP6CU/Lu et al. - 2019 - NSGA-Net neural architecture search using multi-o.pdf}
}

@article{machadoEvolutionaryPerspectiveVirus2020,
  title = {An {{Evolutionary Perspective}} of {{Virus Propagation}}},
  author = {Machado, J.},
  year = {2020},
  month = may,
  journal = {Mathematics},
  volume = {8},
  pages = {779},
  doi = {10.3390/math8050779},
  abstract = {This paper presents an evolutionary algorithm that simulates simplified scenarios of the diffusion of an infectious disease within a given population. The proposed evolutionary epidemic diffusion (EED) computational model has a limited number of variables and parameters, but is still able to simulate a variety of configurations that have a good adherence to real-world cases. The use of two space distances and the calculation of spatial 2-dimensional entropy are also examined. Several simulations demonstrate the feasibility of the EED for testing distinct social, logistic and economy risks. The performance of the system dynamics is assessed by several variables and indices. The global information is efficiently condensed and visualized by means of multidimensional scaling.},
  file = {/home/disc/p.templier/Zotero/storage/7GG9PBPH/Machado - 2020 - An Evolutionary Perspective of Virus Propagation.pdf}
}

@article{machadoRevisitingArcadeLearning2017,
  title = {Revisiting the {{Arcade Learning Environment}}: {{Evaluation Protocols}} and {{Open Problems}} for {{General Agents}}},
  shorttitle = {Revisiting the {{Arcade Learning Environment}}},
  author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
  year = {2017},
  month = nov,
  journal = {arXiv:1709.06009 [cs]},
  eprint = {1709.06009},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/KKADI6SW/Machado et al. - 2017 - Revisiting the Arcade Learning Environment Evalua.pdf}
}

@article{maheswaranathanGuidedEvolutionaryStrategies,
  title = {Guided Evolutionary Strategies: {{Augmenting}} Random Search with Surrogate Gradients},
  author = {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and {Sohl-Dickstein}, Jascha},
  pages = {10},
  abstract = {Many applications in machine learning require optimizing a function whose true gradient is inaccessible, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the trade-offs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/44B59STS/Maheswaranathan et al. - Guided evolutionary strategies Augmenting random .pdf}
}

@article{maileConstrainedOptimizationDifferentiable2021,
  title = {On {{Constrained Optimization}} in {{Differentiable Neural Architecture Search}}},
  author = {Maile, Kaitlin and Lecarpentier, Erwan and Luga, Herv{\'e} and Wilson, Dennis G.},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.11655 [cs]},
  eprint = {2106.11655},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Differentiable Architecture Search (DARTS) is a recently proposed neural architecture search (NAS) method based on a differentiable relaxation. Due to its success, numerous variants analyzing and improving parts of the DARTS framework have recently been proposed. By considering the problem as a constrained bilevel optimization, we propose and analyze three improvements to architectural weight competition, update scheduling, and regularization towards discretization. First, we introduce a new approach to the activation of architecture weights, which prevents confounding competition within an edge and allows for fair comparison across edges to aid in discretization. Next, we propose a dynamic schedule based on per-minibatch network information to make architecture updates more informed. Finally, we consider two regularizations, based on proximity to discretization and the Alternating Directions Method of Multipliers (ADMM) algorithm, to promote early discretization. Our results show that this new activation scheme reduces final architecture size and the regularizations improve reliability in search results while maintaining comparable performance to state-of-the-art in NAS, especially when used with our new dynamic informed schedule.4.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/LI4M9M8I/2106.11655.pdf;/home/disc/p.templier/Zotero/storage/M5V3MDW6/Maile et al. - 2021 - On Constrained Optimization in Differentiable Neur.pdf}
}

@article{mandhaneMuZeroSelfcompetitionRate2022,
  title = {{{MuZero}} with {{Self-competition}} for {{Rate Control}} in {{VP9 Video Compression}}},
  author = {Mandhane, Amol and Zhernov, Anton and Rauh, Maribeth and Gu, Chenjie and Wang, Miaosen and Xue, Flora and Shang, Wendy and Pang, Derek and Claus, Rene and Chiang, Ching-Han and Chen, Cheng and Han, Jingning and Chen, Angie and Mankowitz, Daniel J. and Broshear, Jackson and Schrittwieser, Julian and Hubert, Thomas and Vinyals, Oriol and Mann, Timothy},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.06626 [cs, eess]},
  eprint = {2202.06626},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Video streaming usage has seen a significant rise as entertainment, education, and business increasingly rely on online video. Optimizing video compression has the potential to increase access and quality of content to users, and reduce energy use and costs overall. In this paper, we present an application of the MuZero algorithm to the challenge of video compression. Specifically, we target the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services. We treat this as a sequential decision making problem to maximize the video quality with an episodic constraint imposed by the target bitrate. Notably, we introduce a novel self-competition based reward mechanism to solve constrained RL with variable constraint satisfaction difficulty, which is challenging for existing constrained RL methods. We demonstrate that the MuZero-based rate control achieves an average 6.28\% reduction in size of the compressed videos for the same delivered video quality level (measured as PSNR BD-rate) compared to libvpx's two-pass VBR rate control policy, while having better constraint satisfaction behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/disc/p.templier/Zotero/storage/QNEYV2SU/Mandhane et al. - 2022 - MuZero with Self-competition for Rate Control in V.pdf;/home/disc/p.templier/Zotero/storage/W74CESRQ/2202.html}
}

@article{maniaSimpleRandomSearch2018,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.07055 [cs, math, stat]},
  eprint = {1803.07055},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-ofthe-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/C9RUYRFJ/Mania et al. - 2018 - Simple random search provides a competitive approa.pdf}
}

@inproceedings{maniaSimpleRandomSearch2018a,
  title = {Simple Random Search of Static Linear Policies Is Competitive for Reinforcement Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@article{marbachCoevolutionConfigurationControl2004,
  title = {Co-Evolution of {{Configuration}} and {{Control}} for {{Homogenous Modular Robots}}},
  author = {Marbach, D. and Ijspeert, A.J. and Groen, F.},
  year = {2004},
  month = jan,
  journal = {Proceedings of the Eighth Conference on Intelligent Autonomous Systems (IAS8)},
  file = {/home/disc/p.templier/Zotero/storage/GHBDZX3Y/Marbach et al. - 2004 - Co-evolution of Configuration and Control for Homo.pdf}
}

@book{marcalbinetConcevoirJeuVideo,
  title = {Concevoir Un {{Jeu Video}}},
  author = {{Marc Albinet}},
  file = {/home/disc/p.templier/Zotero/storage/5D48AVHA/Marc Albinet - Concevoir un Jeu Video.pdf}
}

@article{marchesiniGeneticDeepReinforcement2020,
  title = {Genetic {{Deep Reinforcement Learning}} for {{Mapless Navigation}}},
  author = {Marchesini, Enrico and Farinelli, Alessandro},
  year = {2020},
  journal = {New Zealand},
  pages = {3},
  abstract = {We consider Deep Reinforcement Learning (DRL) approaches to devise mapless navigation strategies for mobile platforms. We propose a Genetic Deep Reinforcement Learning (GDRL) method that combines Genetic Algorithms (GA) with discrete and continuous action space DRL approaches. The goal of GDRL is to reduce the sensitivity of DRL approaches to their hyper-parameter tuning and to provide robust exploration strategies. We evaluate GDRL in combination with Rainbow and Proximal Policy Optimization (PPO) in two navigation scenarios: i) a wheeled robot avoiding obstacles in an indoor environment and ii) a water drone that must reach a predefined location in presence of waves. Our empirical evaluation demonstrates that GDRL outperforms state-of-the-art DRL and GA methods as well as a previous hybrid approach.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/M7N3BTBJ/Marchesini et Farinelli - 2020 - Genetic Deep Reinforcement Learning for Mapless Na.pdf}
}

@article{marchesiniGENETICSOFTUPDATES2021,
  title = {{{GENETIC SOFT UPDATES FOR POLICY EVOLUTION IN DEEP REINFORCEMENT LEARNING}}},
  author = {Marchesini, Enrico and Corsi, Davide and Farinelli, Alessandro},
  year = {2021},
  pages = {15},
  abstract = {The combination of Evolutionary Algorithms (EAs) and Deep Reinforcement Learning (DRL) has been recently proposed to merge the benefits of both solutions. Existing mixed approaches, however, have been successfully applied only to actor-critic methods and present significant overhead. We address these issues by introducing a novel mixed framework that exploits a periodical genetic evaluation to soft update the weights of a DRL agent. The resulting approach is applicable with any DRL method and, in a worst-case scenario, it does not exhibit detrimental behaviours. Experiments in robotic applications and continuous control benchmarks demonstrate the versatility of our approach that significantly outperforms prior DRL, EAs, and mixed approaches. Finally, we employ formal verification to confirm the policy improvement, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/THTILBWX/Marchesini et al. - 2021 - GENETIC SOFT UPDATES FOR POLICY EVOLUTION IN DEEP .pdf}
}

@inproceedings{mcdermottGeneticProgrammingNeeds2012,
  title = {Genetic Programming Needs Better Benchmarks},
  booktitle = {Proceedings of the 14th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {McDermott, James and White, David R and Luke, Sean and Manzoni, Luca and Castelli, Mauro and Vanneschi, Leonardo and Jaskowski, Wojciech and Krawiec, Krzysztof and Harper, Robin and De Jong, Kenneth and others},
  year = {2012},
  pages = {791--798}
}

@article{meierImprovingGradientEstimation2019,
  title = {Improving {{Gradient Estimation}} in {{Evolutionary Strategies With Past Descent Directions}}},
  author = {Meier, Florian and Mujika, Asier and Gauy, Marcelo Matheus and Steger, Angelika},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.05268 [cs]},
  eprint = {1910.05268},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Evolutionary Strategies (ES) are known to be an effective black-box optimization technique for deep neural networks when the true gradients cannot be computed, such as in Reinforcement Learning. We continue a recent line of research that uses surrogate gradients to improve the gradient estimation of ES. We propose a novel method to optimally incorporate surrogate gradient information. Our approach, unlike previous work, needs no information about the quality of the surrogate gradients and is always guaranteed to find a descent direction that is better than the surrogate gradient. This allows to iteratively use the previous gradient estimate as surrogate gradient for the current search point. We theoretically prove that this yields fast convergence to the true gradient for linear functions and show under simplifying assumptions that it significantly improves gradient estimates for general functions. Finally, we evaluate our approach empirically on MNIST and reinforcement learning tasks and show that it considerably improves the gradient estimation of ES at no extra computational cost.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/3TZJNJSR/Meier et al. - 2019 - Improving Gradient Estimation in Evolutionary Stra.pdf}
}

@article{mendelEXPERIMENTSPLANTHYBRIDIZATION1865,
  title = {{{EXPERIMENTS IN PLANT HYBRIDIZATION}}},
  author = {Mendel, Gregor},
  year = {1865},
  journal = {Psychological Review}
}

@article{metzGradientsAreNot2021,
  title = {Gradients Are {{Not All You Need}}},
  author = {Metz, Luke and Freeman, C. Daniel and Schoenholz, Samuel S. and Kachman, Tal},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.05803 [cs, stat]},
  eprint = {2111.05803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/KNRTLV4U/Metz et al. - 2021 - Gradients are Not All You Need.pdf}
}

@article{millerCartesianGeneticProgramming2008,
  title = {Cartesian {{Genetic Programming}}},
  author = {Miller, Julian Francis and Harding, Simon},
  year = {2008},
  journal = {Cartesian Genetic Programming},
  pages = {25},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/7BANRW9N/Miller et Harding - 2008 - Cartesian Genetic Programming.pdf}
}

@inproceedings{millerDesigningNeuralNetworks1989,
  title = {Designing {{Neural Networks}} Using {{Genetic Algorithms}}.},
  author = {Miller, Geoffrey and Todd, Peter and Hegde, Shailesh},
  year = {1989},
  month = jan,
  pages = {379--384},
  file = {/home/disc/p.templier/Zotero/storage/MF9QLGU6/Miller et al. - 1989 - Designing Neural Networks using Genetic Algorithms.pdf}
}

@book{mitchellIntroductionGeneticAlgorithms1998,
  title = {An Introduction to Genetic Algorithms},
  author = {Mitchell, Melanie},
  year = {1998},
  publisher = {{MIT press}}
}

@article{mnihAsynchronousMethodsDeep,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
  pages = {10},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/IGFX2A24/Mnih et al. - Asynchronous Methods for Deep Reinforcement Learni.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  year = {2015},
  journal = {nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  keywords = {Computer science},
  file = {/home/disc/p.templier/Zotero/storage/WPMGDPGD/nature14236.html}
}

@article{mnihPlayingAtariDeep,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/H7LQWMZJ/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf}
}

@article{montanaTrainingFeedforwardNeural1989,
  title = {Training {{Feedforward Neural Networks Using Genetic Algorithms}}},
  author = {Montana, David J and Davis, Lawrence and St, Mouiton},
  year = {1989},
  month = aug,
  pages = {6},
  abstract = {Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both 1) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and 2) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/35DZS9IQ/Montana et al. - Training Feedforward Neural Networks Using Genetic.pdf}
}

@article{moriartyFormingNeuralNetworks,
  title = {Forming {{Neural Networks}} through {{Efficient}} and {{Adaptive Coevolution}}},
  author = {Moriarty, David E and Miikkulainen, Risto},
  pages = {28},
  abstract = {This article demonstrates the advantages of a cooperative, coevolutionary search in di cult control problems. The SANE system coevolves a population of neurons that cooperate to form a functioning neural network. In this process, neurons assume di erent but overlapping roles, resulting in a robust encoding of control behavior. SANE is shown to be more e cient, more adaptive, and maintain higher levels of diversity than the more common network-based population approaches. Further empirical studies illustrate the emergent neuron specializations and the di erent roles the neurons assume in the population.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/4FB38TRA/Moriarty et Miikkulainen - Forming Neural Networks through E cient and Adapti.pdf}
}

@inproceedings{mouretIlluminatingSearchSpaces2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  year = {2015},
  file = {/home/disc/p.templier/Zotero/storage/GMZ4Y2Z6/Mouret et Clune - 2015 - Illuminating search spaces by mapping elites.pdf}
}

@article{muravevNeuralArchitectureSearch2020,
  title = {Neural {{Architecture Search}} by {{Estimation}} of {{Network Structure Distributions}}},
  author = {Muravev, Anton and Raitoharju, Jenni and Gabbouj, Moncef},
  year = {2020},
  month = apr,
  journal = {arXiv:1908.06886 [cs]},
  eprint = {1908.06886},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The influence of deep learning is continuously expanding across different domains, and its new applications are ubiquitous. The question of neural network design thus increases in importance, as traditional empirical approaches are reaching their limits. Manual design of network architectures from scratch relies heavily on trial and error, while using existing pretrained models can introduce redundancies or vulnerabilities. Automated neural architecture design is able to overcome these problems, but the most successful algorithms operate on significantly constrained design spaces, assuming the target network to consist of identical repeating blocks. We propose a probabilistic representation of a neural network structure under the assumption of independence between layer types. A matrix of probabilities is equivalent to the population of models, but simpler to interpret and analyze. We construct an architecture search algorithm, inspired by the estimation of distribution algorithms, to take advantage of this representation. The probability matrix is tuned towards generating high-performance models by repeatedly sampling the architectures and evaluating the corresponding networks. Our algorithm is shown to discover models which are superior to handcrafted architectures and competitive with those produced by existing architecture search methods, both in accuracy and computational costs, while being conceptually simple and highly extensible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/I3L7JXP7/Muravev et al. - 2020 - Neural Architecture Search by Estimation of Networ.pdf;/home/disc/p.templier/Zotero/storage/96Y2JBDE/1908.html}
}

@misc{MuZeroFirstStep,
  title = {{{MuZero}}'s First Step from Research into the Real World},
  abstract = {Collaborating with YouTube to optimise video compression in the open source VP9 codec.},
  howpublished = {https://www.deepmind.com/blog/muzeros-first-step-from-research-into-the-real-world},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/TDAJLEXS/muzeros-first-step-from-research-into-the-real-world.html}
}

@article{mwu_test,
  title = {On a Test of Whether One of Two Random Variables Is Stochastically Larger than the Other},
  author = {Mann, H. B. and Whitney, D. R.},
  year = {1947},
  journal = {The Annals of Mathematical Statistics},
  volume = {18},
  number = {1},
  pages = {50--60},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/aoms/1177730491}
}

@misc{MyResume,
  title = {My {{Resume}}},
  howpublished = {http://0.0.0.0:4000/online-cv/},
  file = {/home/disc/p.templier/Zotero/storage/ICZ5W675/online-cv.html}
}

@article{nairRectifiedLinearUnits,
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  author = {Nair, Vinod and Hinton, Geoffrey E},
  pages = {8},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ``Stepped Sigmoid Units'' are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/UD4S8H57/Nair et Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf}
}

@misc{najarroHyperNCAGrowingDevelopmental2022,
  title = {{{HyperNCA}}: {{Growing Developmental Networks}} with {{Neural Cellular Automata}}},
  shorttitle = {{{HyperNCA}}},
  author = {Najarro, Elias and Sudhakaran, Shyam and Glanois, Claire and Risi, Sebastian},
  year = {2022},
  month = apr,
  number = {arXiv:2204.11674},
  eprint = {2204.11674},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.11674},
  abstract = {In contrast to deep reinforcement learning agents, biological neural networks are grown through a self-organized developmental process. Here we propose a new hypernetwork approach to grow artificial neural networks based on neural cellular automata (NCA). Inspired by self-organising systems and information-theoretic approaches to developmental biology, we show that our HyperNCA method can grow neural networks capable of solving common reinforcement learning tasks. Finally, we explore how the same approach can be used to build developmental metamorphosis networks capable of transforming their weights to solve variations of the initial RL task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/625NYBA3/Najarro et al. - 2022 - HyperNCA Growing Developmental Networks with Neur.pdf;/home/disc/p.templier/Zotero/storage/LA4YU8L8/Najarro et al. - 2022 - HyperNCA Growing Developmental Networks with Neur.pdf;/home/disc/p.templier/Zotero/storage/IKY5TR8V/2204.html}
}

@article{najarroMetaLearningHebbianPlasticity2021,
  title = {Meta-{{Learning}} through {{Hebbian Plasticity}} in {{Random Networks}}},
  author = {Najarro, Elias and Risi, Sebastian},
  year = {2021},
  month = mar,
  journal = {arXiv:2007.02686 [cs]},
  eprint = {2007.02686},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Lifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to morphological damage not seen during training and in the absence of any explicit reward or error signal in less than 100 timesteps. Code is available at https://github.com/enajx/HebbianMetaLearning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/C8Q7NQAU/Najarro et Risi - 2021 - Meta-Learning through Hebbian Plasticity in Random.pdf}
}

@article{nelsonEstimatesBranchingFactors,
  title = {Estimates for the {{Branching Factors}} of {{Atari Games}}},
  author = {Nelson, Mark J},
  pages = {5},
  abstract = {The branching factor of a game is the average number of new states reachable from a given state. It is a widely used metric in AI research on board games, but less often computed or discussed for videogames. This paper provides estimates for the branching factors of 103 Atari 2600 games, as implemented in the Arcade Learning Environment (ALE). Depending on the game, ALE exposes between 3 and 18 available actions per frame of gameplay, which is an upper bound on branching factor. This paper shows, based on an enumeration of the first 1 million distinct states reachable in each game, that the average branching factor is usually much lower, in many games barely above 1. In addition to reporting the branching factors, this paper aims to clarify what constitutes a distinct state in ALE.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/IXJCXTMA/Nelson - Estimates for the Branching Factors of Atari Games.pdf}
}

@incollection{neumannAnalysisEvolutionaryAlgorithms2020,
  title = {Analysis of {{Evolutionary Algorithms}} in {{Dynamic}} and {{Stochastic Environments}}},
  booktitle = {Theory of {{Evolutionary Computation}}: {{Recent Developments}} in {{Discrete Optimization}}},
  author = {Neumann, Frank and Pourhassan, Mojgan and Roostapour, Vahid},
  editor = {Doerr, Benjamin and Neumann, Frank},
  year = {2020},
  pages = {323--357},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-29414-4_7},
  abstract = {Many real-world optimization problems occur in environments that change dynamically or involve stochastic components. Evolutionary algorithms and other bio-inspired algorithms have been widely applied to dynamic and stochastic problems. This survey gives an overview of major theoretical developments in the area of runtime analysis for these problems. We review recent theoretical studies of evolutionary algorithms and ant colony optimization for problems where the objective functions or the constraints change over time. Furthermore, we consider stochastic problems with various noise models and point out some directions for future research.},
  isbn = {978-3-030-29414-4},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/WTY25KJU/Neumann et al. - 2020 - Analysis of Evolutionary Algorithms in Dynamic and.pdf}
}

@incollection{NEURIPS2019_9015,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}}
}

@misc{niko_cma-espycma_2020,
  title = {{{CMA-ES}}/Pycma: R3.0.3},
  shorttitle = {{{CMA-ES}}/Pycma},
  author = {{niko} and Akimoto, Youhei and {yoshihikoueno} and Brockhoff, Dimo and Chan, Matthew and {ARF1}},
  year = {2020},
  month = apr,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3764210},
  abstract = {Fix bug when passing args in OOOptimizer.optimize.},
  keywords = {cmaes,pycma}
}

@misc{nikoCMAESPycmaR32020,
  title = {{{CMA-ES}}/Pycma: R3.0.3},
  shorttitle = {{{CMA-ES}}/Pycma},
  author = {{niko} and Youhei Akimoto and {yoshihikoueno} and Dimo Brockhoff and Matthew Chan and ARF1},
  year = {2020},
  month = apr,
  doi = {10.5281/zenodo.3764210},
  abstract = {Fix bug when passing args in OOOptimizer.optimize.},
  howpublished = {Zenodo},
  keywords = {cmaes,pycma},
  file = {/home/disc/p.templier/Zotero/storage/CB9T5IEE/3764210.html}
}

@inproceedings{nilssonPolicyGradientAssisted2021,
  title = {Policy Gradient Assisted {{MAP-Elites}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Nilsson, Olle and Cully, Antoine},
  year = {2021},
  month = jun,
  pages = {866--875},
  publisher = {{ACM}},
  address = {{Lille France}},
  doi = {10.1145/3449639.3459304},
  abstract = {Quality-Diversity optimization algorithms such as MAP-Elites, aim to generate collections of both diverse and high-performing solutions to an optimization problem. MAP-Elites has shown promising results in a variety of applications. In particular in evolutionary robotics tasks targeting the generation of behavioral repertoires that highlight the versatility of robots. However, for most robotics applications MAP-Elites is limited to using simple open-loop or low-dimensional controllers. Here we present Policy Gradient Assisted MAP-Elites (PGA-MAP-Elites), a novel algorithm that enables MAP-Elites to efficiently evolve large neural network controllers by introducing a gradient-based variation operator inspired by Deep Reinforcement Learning. This operator leverages gradient estimates obtained from a critic neural network to rapidly find higher-performing solutions and is paired with a traditional genetic variation to maintain a divergent search behavior. The synergy of these operators makes PGA-MAP-Elites an efficient yet powerful algorithm for finding diverse and high-performing behaviors. We evaluate our method on four different tasks for building behavioral repertoires that use deep neural network controllers. The results show that PGA-MAP-Elites significantly improves the quality of the generated repertoires compared to existing methods.},
  isbn = {978-1-4503-8350-9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/8Z5FJ8G3/Nilsson et Cully - 2021 - Policy gradient assisted MAP-Elites.pdf;/home/disc/p.templier/Zotero/storage/PY5BKGU5/Nilsson et Cully - 2021 - Policy gradient assisted MAP-Elites.pdf;/home/disc/p.templier/Zotero/storage/XS8RXSPM/Nilsson et Cully - 2021 - Policy gradient assisted MAP-Elites.pdf}
}

@misc{noauthor_fluxmlfluxjl_2021,
  title = {{{FluxML}}/{{Flux}}.Jl},
  year = {2021},
  month = feb,
  publisher = {{Flux}},
  abstract = {Relax! Flux is the ML library that doesn't make you tensor},
  copyright = {View license , View license},
  keywords = {data-science,deep-learning,flux,machine-learning,neural-networks,the-human-brian}
}

@inproceedings{oreillyIntroductionGeneticProgramming2019,
  title = {Introduction to {{Genetic Programming}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {O'Reilly, Una-May and Hemberg, Erik},
  year = {2019},
  series = {{{GECCO}} '19},
  pages = {710--725},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3319619.3323398},
  isbn = {978-1-4503-6748-6}
}

@article{pagliucaEfficacyModernNeuroEvolutionary2020,
  title = {Efficacy of {{Modern Neuro-Evolutionary Strategies}} for {{Continuous Control Optimization}}},
  author = {Pagliuca, Paolo and Milano, Nicola and Nolfi, Stefano},
  year = {2020},
  month = jul,
  journal = {Frontiers in Robotics and AI},
  volume = {7},
  pages = {98},
  issn = {2296-9144},
  doi = {10.3389/frobt.2020.00098},
  abstract = {We analyze the efficacy of modern neuro-evolutionary strategies for continuous control optimization. Overall, the results collected on a wide variety of qualitatively different benchmark problems indicate that these methods are generally effective and scale well with respect to the number of parameters and the complexity of the problem. Moreover, they are relatively robust with respect to the setting of hyper-parameters. The comparison of the most promising methods indicates that the OpenAI-ES algorithm outperforms or equals the other algorithms on all considered problems. Moreover, we demonstrate how the reward functions optimized for reinforcement learning methods are not necessarily effective for evolutionary strategies and vice versa. This finding can lead to reconsideration of the relative efficacy of the two classes of algorithm since it implies that the comparisons performed to date are biased toward one or the other class.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/S5AVE8YT/Pagliuca et al. - 2020 - Efficacy of Modern Neuro-Evolutionary Strategies f.pdf}
}

@article{palmVariationalNeuralCellular2022,
  title = {Variational {{Neural Cellular Automata}}},
  author = {Palm, Rasmus Berg and {Gonz{\'a}lez-Duque}, Miguel and Sudhakaran, Shyam and Risi, Sebastian},
  year = {2022},
  month = feb,
  journal = {arXiv:2201.12360 [cs]},
  eprint = {2201.12360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms \textemdash{} algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/3BDHHPI6/Palm et al. - 2022 - Variational Neural Cellular Automata.pdf}
}

@inproceedings{paoloUnsupervisedLearningExploration2020,
  title = {Unsupervised {{Learning}} and {{Exploration}} of {{Reachable Outcome Space}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Paolo, Giuseppe and Laflaqui{\`e}re, Alban and Coninx, Alexandre and Doncieux, Stephane},
  year = {2020},
  month = may,
  pages = {2379--2385},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9196819},
  abstract = {Performing Reinforcement Learning in sparse rewards settings, with very little prior knowledge, is a challenging problem since there is no signal to properly guide the learning process. In such situations, a good search strategy is fundamental. At the same time, not having to adapt the algorithm to every single problem is very desirable. Here we introduce TAXONS, a Task Agnostic eXploration of Outcome spaces through Novelty and Surprise algorithm. Based on a population-based divergent-search approach, it learns a set of diverse policies directly from high-dimensional observations, without any task-specific information. TAXONS builds a repertoire of policies while training an autoencoder on the high-dimensional observation of the final state of the system to build a low-dimensional outcome space. The learned outcome space, combined with the reconstruction error, is used to drive the search for new policies. Results show that TAXONS can find a diverse set of controllers, covering a good part of the ground-truth outcome space, while having no information about such space.},
  keywords = {Aerospace electronics,Extraterrestrial measurements,Robots,Space exploration,Task analysis,Training},
  file = {/home/disc/p.templier/Zotero/storage/ILRCKDXW/Paolo et al. - 2020 - Unsupervised Learning and Exploration of Reachable.pdf;/home/disc/p.templier/Zotero/storage/68XPUWAC/9196819.html}
}

@article{picardTorchManualSeed2021,
  title = {Torch.Manual\_seed(3407) Is All You Need: {{On}} the Influence of Random Seeds in Deep Learning Architectures for Computer Vision},
  shorttitle = {Torch.Manual\_seed(3407) Is All You Need},
  author = {Picard, David},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.08203 [cs]},
  eprint = {2109.08203},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper I investigate the effect of random seed selection on the accuracy when using popular deep learning architectures for computer vision. I scan a large amount of seeds (up to 104) on CIFAR 10 and I also scan fewer seeds on Imagenet using pre-trained models to investigate large scale datasets. The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/disc/p.templier/Zotero/storage/7SKIVART/Picard - 2021 - Torch.manual_seed(3407) is all you need On the in.pdf}
}

@article{pierrotLearningCompositionalNeural2020,
  title = {Learning {{Compositional Neural Programs}} for {{Continuous Control}}},
  author = {Pierrot, Thomas and Perrin, Nicolas and Behbahani, Feryal and Laterre, Alexandre and Sigaud, Olivier and Beguir, Karim and {de Freitas}, Nando},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.13363 [cs]},
  eprint = {2007.13363},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a novel solution to challenging sparse-reward, continuous control problems that require hierarchical planning at multiple levels of abstraction. Our solution, dubbed AlphaNPI-X, involves three separate stages of learning. First, we use off-policy reinforcement learning algorithms with experience replay to learn a set of atomic goal-conditioned policies, which can be easily repurposed for many tasks. Second, we learn self-models describing the effect of the atomic policies on the environment. Third, the self-models are harnessed to learn recursive compositional programs with multiple levels of abstraction. The key insight is that the self-models enable planning by imagination, obviating the need for interaction with the world when learning higher-level compositional programs. To accomplish the third stage of learning, we extend the AlphaNPI algorithm, which applies AlphaZero to learn recursive neural programmer-interpreters. We empirically show that AlphaNPI-X can effectively learn to tackle challenging sparse manipulation tasks, such as stacking multiple blocks, where powerful model-free baselines fail.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/disc/p.templier/Zotero/storage/KIWAPD93/Pierrot et al. - 2020 - Learning Compositional Neural Programs for Continu.pdf}
}

@article{pigliucciEvolvabilityEvolvable2008,
  title = {Is Evolvability Evolvable?},
  author = {Pigliucci, Massimo},
  year = {2008},
  month = jan,
  journal = {Nature Reviews Genetics},
  volume = {9},
  number = {1},
  pages = {75--82},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg2278},
  abstract = {In recent years, biologists have increasingly been asking whether the ability to evolve \textemdash{} the evolvability \textemdash{} of biological systems, itself evolves, and whether this phenomenon is the result of natural selection or a by-product of other evolutionary processes. The concept of evolvability, and the increasing theoretical and empirical literature that refers to it, may constitute one of several pillars on which an extended evolutionary synthesis will take shape during the next few years, although much work remains to be done on how evolvability comes about.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/PP4FQUDQ/Pigliucci - 2008 - Is evolvability evolvable.pdf}
}

@article{pourchotCEMRLCombiningEvolutionary2019,
  title = {{{CEM-RL}}: {{Combining}} Evolutionary and Gradient-Based Methods for Policy Search},
  shorttitle = {{{CEM-RL}}},
  author = {Pourchot, Alo{\"i}s and Sigaud, Olivier},
  year = {2019},
  month = feb,
  journal = {arXiv:1810.01222 [cs, stat]},
  eprint = {1810.01222},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (ddpg) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (cem) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/38I9AM8X/Pourchot et Sigaud - 2019 - CEM-RL Combining evolutionary and gradient-based .pdf}
}

@misc{pourchotImportanceMixingImproving2018,
  title = {Importance Mixing: {{Improving}} Sample Reuse in Evolutionary Policy Search Methods},
  shorttitle = {Importance Mixing},
  author = {Pourchot, Alo{\"i}s and Perrin, Nicolas and Sigaud, Olivier},
  year = {2018},
  month = aug,
  number = {arXiv:1808.05832},
  eprint = {1808.05832},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as ''importance mixing'' can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/2ATLW9TR/Pourchot et al. - 2018 - Importance mixing Improving sample reuse in evolu.pdf}
}

@inproceedings{pretorius2017transferability,
  title = {The Transferability of Evolved Hexapod Locomotion Controllers from Simulation to Real Hardware},
  booktitle = {2017 {{IEEE}} International Conference on Real-Time Computing and Robotics ({{RCAR}})},
  author = {Pretorius, Christiaan J and {du Plessis}, Mathys C and Gonsalves, John W},
  year = {2017},
  pages = {567--574},
  organization = {{IEEE}}
}

@article{procgen,
  title = {Leveraging {{Procedural Generation}} to {{Benchmark Reinforcement Learning}}},
  author = {Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
  year = {2020},
  journal = {International conference on machine learning},
  pages = {9},
  abstract = {We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/MAXUKGYT/Cobbe et al. - Leveraging Procedural Generation to Benchmark Rein.pdf}
}

@book{puterman1994markov,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author = {Puterman, Martin L},
  year = {1994},
  publisher = {{John Wiley \& Sons}}
}

@article{rakicevicPolicyManifoldSearch2021,
  title = {Policy {{Manifold Search}}: {{Exploring}} the {{Manifold Hypothesis}} for {{Diversity-based Neuroevolution}}},
  shorttitle = {Policy {{Manifold Search}}},
  author = {Rakicevic, Nemanja and Cully, Antoine and Kormushev, Petar},
  year = {2021},
  month = jun,
  journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
  eprint = {2104.13424},
  eprinttype = {arxiv},
  pages = {901--909},
  doi = {10.1145/3449639.3459320},
  abstract = {Neuroevolution is an alternative to gradient-based optimisation that has the potential to avoid local minima and allows parallelisation. The main limiting factor is that usually it does not scale well with parameter space dimensionality. Inspired by recent work examining neural network intrinsic dimension and loss landscapes, we hypothesise that there exists a low-dimensional manifold, embedded in the policy network parameter space, around which a high-density of diverse and useful policies are located. This paper proposes a novel method for diversity-based policy search via Neuroevolution, that leverages learned representations of the policy network parameters, by performing policy search in this learned representation space. Our method relies on the Quality-Diversity (QD) framework which provides a principled approach to policy search, and maintains a collection of diverse policies, used as a dataset for learning policy representations. Further, we use the Jacobian of the inversemapping function to guide the search in the representation space. This ensures that the generated samples remain in the high-density regions, after mapping back to the original space. Finally, we evaluate our contributions on four continuous-control tasks in simulated environments, and compare to diversity-based baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/RMPTJX6X/Rakicevic et al. - 2021 - Policy Manifold Search Exploring the Manifold Hyp.pdf}
}

@book{rasmussen2006gaussian,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K.},
  year = {2006},
  volume = {2},
  publisher = {{MIT press Cambridge, MA}}
}

@article{realAutoMLZeroEvolvingMachine,
  title = {{{AutoML-Zero}}: {{Evolving Machine Learning Algorithms From Scratch}}},
  author = {Real, Esteban and Liang, Chen and So, David R and Le, Quoc V},
  pages = {13},
  abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expertdesigned layers as building blocks\textemdash or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/RYDNGADX/Real et al. - AutoML-Zero Evolving Machine Learning Algorithms .pdf}
}

@article{realLargeScaleEvolutionImage2017,
  title = {Large-{{Scale Evolution}} of {{Image Classifiers}}},
  author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.01041 [cs]},
  eprint = {1703.01041},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {and Cluster Computing,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,I.5.2,Parallel},
  file = {/home/disc/p.templier/Zotero/storage/4ENFFZGU/Real et al. - 2017 - Large-Scale Evolution of Image Classifiers.pdf}
}

@article{realRegularizedEvolutionImage2019,
  title = {Regularized {{Evolution}} for {{Image Classifier Architecture Search}}},
  author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  year = {2019},
  month = feb,
  journal = {arXiv:1802.01548 [cs]},
  eprint = {1802.01548},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier\textemdash AmoebaNet-A\textemdash that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9\% top-1 / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {and Cluster Computing,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,I.5.2,Parallel},
  file = {/home/disc/p.templier/Zotero/storage/IGC6AFFV/Real et al. - 2019 - Regularized Evolution for Image Classifier Archite.pdf}
}

@incollection{rechenberg1978evolutionsstrategien,
  title = {Evolutionsstrategien},
  booktitle = {Simulationsmethoden in Der Medizin Und Biologie},
  author = {Rechenberg, Ingo},
  year = {1978},
  pages = {83--114},
  publisher = {{Springer}}
}

@inproceedings{rechenbergEvolutionStrategyNature1989,
  title = {Evolution {{Strategy}}: {{Nature}}'s {{Way}} of {{Optimization}}},
  booktitle = {Optimization: {{Methods}} and {{Applications}}, {{Possibilities}} and {{Limitations}}},
  author = {Rechenberg, Ingo},
  editor = {Bergmann, H. W.},
  year = {1989},
  pages = {106--126},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {Biological Evolution has done development work on animated matter for more than three billion years. The method used in this process is the equivalent of an astute optimization procedure which is proverable by the theory of Evolution Strategy.},
  isbn = {978-3-642-83814-9}
}

@inproceedings{risi2011enhancing,
  title = {Enhancing Es-Hyperneat to Evolve More Complex Regular Neural Networks},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Risi, Sebastian and Stanley, Kenneth O},
  year = {2011},
  pages = {1539--1546}
}

@inproceedings{risiDeepNeuroevolutionRecurrent2019,
  title = {Deep Neuroevolution of Recurrent and Discrete World Models},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Risi, Sebastian and Stanley, Kenneth O.},
  year = {2019},
  month = jul,
  series = {{{GECCO}} '19},
  pages = {456--462},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3321707.3321817},
  abstract = {Neural architectures inspired by our own human cognitive system, such as the recently introduced world models, have been shown to outperform traditional deep reinforcement learning (RL) methods in a variety of different domains. Instead of the relatively simple architectures employed in most RL experiments, world models rely on multiple different neural components that are responsible for visual information processing, memory, and decision-making. However, so far the components of these models have to be trained separately and through a variety of specialized training methods. This paper demonstrates the surprising finding that models with the same precise parts can be instead efficiently trained end-to-end through a genetic algorithm (GA), reaching a comparable performance to the original world model by solving a challenging car racing task. An analysis of the evolved visual and memory system indicates that they include a similar effective representation to the system trained through gradient descent. Additionally, in contrast to gradient descent methods that struggle with discrete variables, GAs also work directly with such representations, opening up opportunities for classical planning in latent space. This paper adds additional evidence on the effectiveness of deep neuroevolution for tasks that require the intricate orchestration of multiple components in complex heterogeneous architectures.},
  isbn = {978-1-4503-6111-8},
  file = {/home/disc/p.templier/Zotero/storage/VG53QE7T/Risi et Stanley - 2019 - Deep neuroevolution of recurrent and discrete worl.pdf}
}

@inproceedings{risiEvolvingPlacementDensity2010,
  title = {Evolving the Placement and Density of Neurons in the Hyperneat Substrate},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '10},
  author = {Risi, Sebastian and Lehman, Joel and Stanley, Kenneth O.},
  year = {2010},
  pages = {563},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/1830483.1830589},
  abstract = {The Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) approach demonstrated that the pattern of weights across the connectivity of an artificial neural network (ANN) can be generated as a function of its geometry, thereby allowing large ANNs to be evolved for high-dimensional problems. Yet it left to the user the question of where hidden nodes should be placed in a geometry that is potentially infinitely dense. To relieve the user from this decision, this paper introduces an extension called evolvable-substrate HyperNEAT (ES-HyperNEAT) that determines the placement and density of the hidden nodes based on a quadtree-like decomposition of the hypercube of weights and a novel insight about the relationship between connectivity and node placement. The idea is that the representation in HyperNEAT that encodes the pattern of connectivity across the ANN contains implicit information on where the nodes should be placed and can therefore be exploited to avoid the need to evolve explicit placement. In this paper, as a proof of concept, ES-HyperNEAT discovers working placements of hidden nodes for a simple navigation domain on its own, thereby eliminating the need to configure the HyperNEAT substrate by hand and suggesting the potential power of the new approach.},
  isbn = {978-1-4503-0072-8},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/N8PVZ5PU/Risi et al. - 2010 - Evolving the placement and density of neurons in t.pdf}
}

@article{risiIncreasingGeneralityMachine2020,
  title = {Increasing Generality in Machine Learning through Procedural Content Generation},
  author = {Risi, Sebastian and Togelius, Julian},
  year = {2020},
  month = aug,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {8},
  pages = {428--436},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0208-z},
  abstract = {Procedural content generation (PCG) refers to the practice of generating game content, such as levels, quests or characters, algorithmically. Motivated by the need to make games replayable, as well as to reduce authoring burden and enable particular aesthetics, many PCG methods have been devised. At the same time that researchers are adapting methods from machine learning (ML) to PCG problems, the ML community has become more interested in PCG-inspired methods. One reason for this development is that ML algorithms often only work for a particular version of a particular task with particular initial parameters. In response, researchers have begun exploring randomization of problem parameters to counteract such overfitting and to allow trained policies to more easily transfer from one environment to another, such as from a simulated robot to a robot in the real world. Here we review existing work on PCG, its overlap with current efforts in ML, and promising new research directions such as procedurally generated learning environments. Although originating in games, we believe PCG algorithms are critical to creating more general machine intelligence.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/home/disc/p.templier/Zotero/storage/Q9IJ35LX/Risi et Togelius - 2020 - Increasing generality in machine learning through .pdf;/home/disc/p.templier/Zotero/storage/LEAKW6ZN/s42256-020-0208-z.html}
}

@article{risiNeuroevolutionGamesState2015,
  title = {Neuroevolution in {{Games}}: {{State}} of the {{Art}} and {{Open Challenges}}},
  shorttitle = {Neuroevolution in {{Games}}},
  author = {Risi, Sebastian and Togelius, Julian},
  year = {2015},
  month = nov,
  journal = {arXiv:1410.7326 [cs]},
  eprint = {1410.7326},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/QRHXQ9DG/Risi et Togelius - 2015 - Neuroevolution in Games State of the Art and Open.pdf;/home/disc/p.templier/Zotero/storage/DJHQVP9Y/1410.html}
}

@article{robbinsStochasticApproximationMethod1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  journal = {Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {{The Institute of Mathematical Statistics}},
  doi = {10.1214/aoms/1177729586},
  file = {/home/disc/p.templier/Zotero/storage/4E2DMH68/Robbins et Monro - 1951 - A Stochastic Approximation Method.pdf}
}

@misc{RoleVirusInfection,
  title = {The Role of Virus Infection in Virus-Evolutionary Genetic Algorithm | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  howpublished = {https://ieeexplore-ieee-org.gorgone.univ-toulouse.fr/document/542357}
}

@incollection{ronaldGeneticLanderExperiment1994,
  title = {Genetic Lander: {{An}} Experiment in Accurate Neuro-Genetic Control},
  shorttitle = {Genetic Lander},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} \textemdash{} {{PPSN III}}},
  author = {Ronald, Edmund and Schoenauer, Marc},
  year = {1994},
  volume = {866},
  pages = {452--461},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-58484-6_288},
  abstract = {The control problem of soft-landing a toy lunar module simulation is investigated in the context of neural nets. While traditional supervised back-propagation training is inappropriate for lack of training exemplars, genetic algorithms allow a controller to be evolved without di culty: Evolution is a form of unsupervised learning. A novelty introduced in this paper is the presentation of additional renormalized inputs to the net; experiments indicate that the presence of such inputs allows precision of control to be attained faster, when learning time is measured by the number of generations for which the GA must run to attain a certain mean performance.},
  isbn = {978-3-540-58484-1 978-3-540-49001-2},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/JNJN3FS2/Ronald et Schoenauer - 1994 - Genetic lander An experiment in accurate neuro-ge.pdf}
}

@book{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  annotation = {\_eprint: 1505.04597}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  pages = {65--386}
}

@incollection{rosSimpleModificationCMAES2008,
  title = {A {{Simple Modification}} in {{CMA-ES Achieving Linear Time}} and {{Space Complexity}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} \textendash{} {{PPSN X}}},
  author = {Ros, Raymond and Hansen, Nikolaus},
  editor = {Rudolph, G{\"u}nter and Jansen, Thomas and Beume, Nicola and Lucas, Simon and Poloni, Carlo},
  year = {2008},
  volume = {5199},
  pages = {296--305},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87700-4_30},
  abstract = {This report proposes a simple modification of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for high dimensional objective functions that reduces the internal time and space complexity from quadratic to linear. The covariance matrix is constrained to be diagonal and the resulting algorithm, sep-CMA-ES, samples each coordinate independently. Because the model complexity is reduced, the learning rate for the covariance matrix can be increased. Consequently, on essentially separable functions, sep-CMAES significantly outperforms CMA-ES. For dimension larger than 100, even on the non-separable Rosenbrock function, the sep-CMA-ES needs fewer function evaluations than CMA-ES.},
  isbn = {978-3-540-87699-1 978-3-540-87700-4},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/3XFJQLVL/Ros et Hansen - 2008 - A Simple Modification in CMA-ES Achieving Linear T.pdf}
}

@incollection{rosSimpleModificationCMAES2008a,
  title = {A {{Simple Modification}} in {{CMA-ES Achieving Linear Time}} and {{Space Complexity}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} \textendash{} {{PPSN X}}},
  author = {Ros, Raymond and Hansen, Nikolaus},
  editor = {Rudolph, G{\"u}nter and Jansen, Thomas and Beume, Nicola and Lucas, Simon and Poloni, Carlo},
  year = {2008},
  volume = {5199},
  pages = {296--305},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-87700-4_30},
  abstract = {This paper proposes a simple modification of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for high dimensional objective functions, reducing the internal time and space complexity from quadratic to linear. The covariance matrix is constrained to be diagonal and the resulting algorithm, sep-CMA-ES, samples each coordinate independently. Because the model complexity is reduced, the learning rate for the covariance matrix can be increased. Consequently, on essentially separable functions, sep-CMA-ES significantly outperforms CMA-ES. For dimensions larger than a hundred, even on the non-separable Rosenbrock function, the sep-CMA-ES needs fewer function evaluations than CMA-ES.},
  isbn = {978-3-540-87699-1 978-3-540-87700-4},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/GJE7532Q/Ros et Hansen - 2008 - A Simple Modification in CMA-ES Achieving Linear T.pdf}
}

@inproceedings{rowe2012choice,
  title = {The Choice of the Offspring Population Size in the (1, {{$\lambda$}}) {{EA}}},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference ({{GECCO}} \#1)2012},
  author = {Rowe, Jonathan E. and Sudholt, Dirk},
  year = {2012},
  pages = {1349--1356}
}

@article{rusuProgressiveNeuralNetworks2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  year = {2016},
  month = sep,
  journal = {arXiv:1606.04671 [cs]},
  eprint = {1606.04671},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/633UP9KE/Rusu et al. - 2016 - Progressive Neural Networks.pdf;/home/disc/p.templier/Zotero/storage/G4GZV72V/1606.html}
}

@inproceedings{ryanGrammaticalEvolutionEvolving14,
  title = {Grammatical {{Evolution}}: {{Evolving Programs}} for an {{Arbitrary Language}}},
  booktitle = {Proceedings of the {{First European Workshop}} on {{Genetic Programming}}},
  author = {Ryan, Conor and Collins, J. J. and O'Neill, Michael},
  editor = {Banzhaf, Wolfgang and Poli, Riccardo and Schoenauer, Marc and Fogarty, Terence C.},
  year = {14},
  series = {{{LNCS}}},
  volume = {1391},
  pages = {83--96},
  publisher = {{Springer-Verlag}},
  address = {{Paris}},
  doi = {doi:10.1007/BFb0055930},
  abstract = {We describe a Genetic Algorithm that can evolve complete programs. Using a variable length linear genome to govern how a Backus Naur Form grammar definition is mapped to a program, expressions and programs of arbitrary complexity may be evolved. Other automatic programming methods are described, before our system, Grammatical Evolution, is applied to a symbolic regression problem.},
  isbn = {3-540-64360-5},
  keywords = {genetic algorithms,genetic programming,grammatical evolution}
}

@article{salimansEvolutionStrategiesScalable2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = mar,
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/5AHGG4Q6/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf;/home/disc/p.templier/Zotero/storage/JNKZWBBF/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf;/home/disc/p.templier/Zotero/storage/9EA4ZHXF/1703.html}
}

@article{salimansImprovedTechniquesTraining,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi and Chen, Xi},
  pages = {9},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/SKLHELJ5/Salimans et al. - Improved Techniques for Training GANs.pdf}
}

@inproceedings{samothrakisNeuroevolutionGeneralVideo2015,
  title = {Neuroevolution for {{General Video Game Playing}}},
  author = {Samothrakis, Spyridon and Perez Liebana, Diego and Lucas, Simon and Fasli, Maria},
  year = {2015},
  month = aug,
  pages = {200--207},
  doi = {10.1109/CIG.2015.7317943},
  file = {/home/disc/p.templier/Zotero/storage/LJKDX9AV/Samothrakis et al. - 2015 - Neuroevolution for General Video Game Playing.pdf}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  shorttitle = {Deep {{Learning}} in {{Neural Networks}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  eprint = {1404.7828},
  eprinttype = {arxiv},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/587PAVT2/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf}
}

@misc{scipy,
  title = {{{SciPy}}: {{Open}} Source Scientific Tools for {{Python}}},
  author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and others},
  year = {2001}
}

@article{sigaudCombiningEvolutionDeep2022,
  title = {Combining {{Evolution}} and {{Deep Reinforcement Learning}} for {{Policy Search}}: A {{Survey}}},
  shorttitle = {Combining {{Evolution}} and {{Deep Reinforcement Learning}} for {{Policy Search}}},
  author = {Sigaud, Olivier},
  year = {2022},
  month = apr,
  journal = {arXiv:2203.14009 [cs]},
  eprint = {2203.14009},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neuroevolution and deep Reinforcement Learning have received a lot of attention in the last years. Some works have compared them, highlighting theirs pros and cons, but an emerging trend consists in combining them so as to benefit from the best of both worlds. In this paper, we provide a survey of this emerging trend by organizing the literature into related groups of works and casting all the existing combinations in each group into a generic framework. We systematically cover all easily available papers irrespective of their publication status, focusing on the combination mechanisms rather than on the experimental results. In total, we cover 45 algorithms more recent than 2017. We hope this effort will favor the growth of the domain by facilitating the understanding of the relationships between the methods, leading to deeper analyses, outlining missing useful comparisons and suggesting new combinations of mechanisms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/TL6YZI8H/Sigaud - 2022 - Combining Evolution and Deep Reinforcement Learnin.pdf;/home/disc/p.templier/Zotero/storage/EBS6C7G4/2203.html}
}

@article{silverGeneralReinforcementLearning2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  journal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aar6404},
  file = {/home/disc/p.templier/Zotero/storage/9GETNTMN/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf}
}

@book{sinhaEvolvingNeuralArchitecture2020,
  title = {Evolving {{Neural Architecture Using One Shot Model}}},
  author = {Sinha, Nilotpal and Chen, Kuan-Wen},
  year = {2020},
  month = dec,
  abstract = {Neural Architecture Search (NAS) is emerging as a new research direction which has the potential to replace the hand-crafted neural architectures designed for specific tasks. Previous evolution based architecture search requires high computational resources resulting in high search time. In this work, we propose a novel way of applying a simple genetic algorithm to the NAS problem called EvNAS (Evolving Neural Architecture using One Shot Model) which reduces the search time significantly while still achieving better result than previous evolution based methods. The architectures are represented by using the architecture parameter of the one shot model which results in the weight sharing among the architectures for a given population of architectures and also weight inheritance from one generation to the next generation of architectures. We propose a decoding technique for the architecture parameter which is used to divert majority of the gradient information towards the given architecture and is also used for improving the performance prediction of the given architecture from the one shot model during the search process. Furthermore, we use the accuracy of the partially trained architecture on the validation data as a prediction of its fitness in order to reduce the search time. EvNAS searches for the architecture on the proxy dataset i.e. CIFAR-10 for 4.4 GPU day on a single GPU and achieves top-1 test error of 2.47\% with 3.63M parameters which is then transferred to CIFAR-100 and ImageNet achieving top-1 error of 16.37\% and top-5 error of 7.4\% respectively. All of these results show the potential of evolutionary methods in solving the architecture search problem.},
  file = {/home/disc/p.templier/Zotero/storage/MMSI7WKI/Sinha et Chen - 2020 - Evolving Neural Architecture Using One Shot Model.pdf}
}

@article{slivkinsIntroductionMultiArmedBandits2022,
  title = {Introduction to {{Multi-Armed Bandits}}},
  author = {Slivkins, Aleksandrs},
  year = {2022},
  month = jan,
  journal = {arXiv:1904.07272 [cs, stat]},
  eprint = {1904.07272},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/YB5T884U/Slivkins - 2022 - Introduction to Multi-Armed Bandits.pdf}
}

@article{smithEvolvingDotaShadow2019,
  title = {Evolving {{Dota}} 2 {{Shadow Fiend Bots}} Using {{Genetic Programming}} with {{External Memory}}},
  author = {Smith, Robert J. and Heywood, Malcolm I.},
  year = {2019},
  publisher = {{ACM}},
  doi = {10.1145/3321707.3321866},
  abstract = {The capacity of genetic programming (GP) to evolve a 'hero' character in the Dota 2 video game is investigated. A reinforcement learning context is assumed in which the only input is a 320-dimensional state vector and performance is expressed in terms of kills and net worth. Minimal assumptions are made to initialize the GP game playing agents-evolution from a tabula rasa starting point-implying that: 1) the instruction set is not task specific; 2) end of game performance feedback reflects quantitive properties a player experiences ; 3) no attempt is made to impart game specific knowledge into GP, such as heuristics for improving navigation, minimizing partial observability, improving team work or prioritizing the protection of specific strategically important structures. In short, GP has to actively develop its own strategies for all aspects of the game. We are able to demonstrate competitive play with the built in game opponents assuming 1-on-1 competitions using the 'Shadow Fiend' hero. The single most important contributing factor to this result is the provision of external memory to GP. Without this, the resulting Dota 2 bots are not able to identify strategies that match those of the built-in game bot. CCS CONCEPTS \textbullet{} Computing methodologies \textrightarrow{} Reinforcement learning; Genetic programming.},
  isbn = {9781450361118},
  keywords = {Coevolution,Dota 2,External Memory,Genetic programming,Partial Observability,Reinforcement learning}
}

@article{songESENASCombiningEvolution2021,
  title = {{{ES-ENAS}}: {{Combining Evolution Strategies}} with {{Neural Architecture Search}} at {{No Extra Cost}} for {{Reinforcement Learning}}},
  shorttitle = {{{ES-ENAS}}},
  author = {Song, Xingyou and Choromanski, Krzysztof and {Parker-Holder}, Jack and Tang, Yunhao and Peng, Daiyi and Jain, Deepali and Gao, Wenbo and Pacchiano, Aldo and Sarlos, Tamas and Yang, Yuxiang},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.07415 [cs]},
  eprint = {2101.07415},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce ES-ENAS, a simple neural architecture search (NAS) algorithm for the purpose of reinforcement learning (RL) policy design, by combining Evolutionary Strategies (ES) [30, 40] and Efficient NAS (ENAS) [36, 54, 61] in a highly scalable and intuitive way. Our main insight is noticing that ES is already a distributed blackbox algorithm, and thus we may simply insert a model controller from ENAS into the central aggregator in ES and obtain weight sharing properties for free. By doing so, we bridge the gap from NAS research in supervised learning settings to the reinforcement learning scenario through this relatively simple marriage between two different lines of research, and are one of the first to apply controller-based NAS techniques to RL. We demonstrate the utility of our method by training combinatorial neural network architectures for RL problems in continuous control, via edge pruning and weight sharing. We also incorporate a wide variety of popular techniques from modern NAS literature, including multiobjective optimization and varying controller methods, to showcase their promise in the RL field and discuss possible extensions. We achieve {$>$} 90\% network compression for multiple tasks, which may be special interest in mobile robotics [15] with limited storage and computational resources.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/disc/p.templier/Zotero/storage/QKKWEDAG/Song et al. - 2021 - ES-ENAS Combining Evolution Strategies with Neura.pdf}
}

@inproceedings{spectorEvolutionEvolvesAutoconstruction2016,
  title = {Evolution {{Evolves}} with {{Autoconstruction}}},
  booktitle = {Proceedings of the 2016 on {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Spector, Lee and McPhee, Nicholas Freitag and Helmuth, Thomas and Casale, Maggie M. and Oks, Julian},
  year = {2016},
  month = jul,
  pages = {1349--1356},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/2908961.2931727},
  abstract = {In autoconstructive evolutionary algorithms, individuals implement not only candidate solutions to specified computational problems, but also their own methods for variation of offspring. This makes it possible for the variation methods to themselves evolve, which could, in principle, produce a system with an enhanced capacity for adaptation and superior problem solving power. Prior work on autoconsruction has explored a range of system designs and their evolutionary dynamics, but it has not solved hard problems. Here we describe a new approach that can indeed solve at least some hard problems. We present the key components of this approach, including the use of linear genomes for hierarchically structured programs, a diversity-maintaining parent selection algorithm, and the enforcement of diversification constraints on offspring. We describe a software synthesis benchmark problem that our new approach can solve, and we present visualizations of data from single successful runs of autoconstructive vs. non-autoconstructive systems on this problem. While anecdotal, the data suggests that variation methods, and therefore significant aspects of the evolutionary process, evolve over the course of the autoconstructive runs.},
  isbn = {978-1-4503-4323-7},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/K75SYKIB/Spector et al. - 2016 - Evolution Evolves with Autoconstruction.pdf}
}

@article{spong1995swing,
  title = {The Swing up Control Problem for the Acrobot},
  author = {Spong, Mark W.},
  year = {1995},
  journal = {IEEE control systems magazine},
  volume = {15},
  number = {1},
  pages = {49--55},
  publisher = {{IEEE}}
}

@techreport{stanleyCompetitiveCoevolutionEvolutionary2004,
  title = {Competitive {{Coevolution}} through {{Evolutionary Complexification}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2004},
  journal = {Journal of Artificial Intelligence Research},
  volume = {21},
  pages = {63--100},
  abstract = {Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure , complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize.},
  keywords = {Changes and compilation copyright © The American Association for Artificial Intelligence. All rights reserved.,Copyright © 2004 AI Access}
}

@article{stanleyCompositionalPatternProducing2007,
  title = {Compositional Pattern Producing Networks: {{A}} Novel Abstraction of Development},
  author = {Stanley, Kenneth O.},
  year = {2007},
  file = {/home/disc/p.templier/Zotero/storage/SARZ3XJV/Stanley - Compositional Pattern Producing Networks A Novel .pdf}
}

@article{stanleyDesigningNeuralNetworks2019,
  title = {Designing Neural Networks through Neuroevolution},
  author = {Stanley, Kenneth O},
  year = {2019},
  volume = {1},
  pages = {12},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/ISCVUVNX/Stanley - 2019 - Designing neural networks through neuroevolution.pdf}
}

@article{stanleyDesigningNeuralNetworks2019a,
  title = {Designing Neural Networks through Neuroevolution},
  author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
  year = {2019},
  month = jan,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {1},
  pages = {24--35},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0006-z},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/UGHV9XUL/Stanley et al. - 2019 - Designing neural networks through neuroevolution.pdf}
}

@inproceedings{stanleyEfficientEvolutionNeural2002,
  title = {Efficient Evolution of Neural Network Topologies},
  booktitle = {Proceedings of the 2002 {{Congress}} on {{Evolutionary Computation}}. {{CEC}}'02 ({{Cat}}. {{No}}. {{02TH8600}})},
  author = {Stanley, Kenneth O and Miikkulainen, Risto},
  year = {2002},
  volume = {2},
  pages = {1757--1762},
  publisher = {{IEEE}}
}

@article{stanleyEvolvingNeuralNetworks2002,
  title = {Evolving Neural Networks through Augmenting Topologies},
  author = {Stanley, Kenneth O and Miikkulainen, Risto},
  year = {2002},
  journal = {Evolutionary computation},
  volume = {10},
  number = {2},
  pages = {99--127},
  publisher = {{MIT Press}},
  keywords = {competing conventions,genetic algorithms,network topologies,neural networks,neuroevolution,speciation},
  file = {/home/disc/p.templier/Zotero/storage/2JLIRFYV/Stanley et Miikkulainen - Evolving neural networks through augmenting topolo.pdf}
}

@article{stanleyHypercubeBasedIndirectEncoding2009,
  title = {A {{Hypercube-Based Indirect Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author = {Stanley, Kenneth O and D'Ambrosio, David and Gauci, Jason},
  year = {2009},
  pages = {39},
  abstract = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algorithms, is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/XMRXX88B/Stanley et al. - A Hypercube-Based Indirect Encoding for Evolving L.pdf}
}

@techreport{stanleyRealTimeNeuroevolutionNERO2005,
  title = {Real-{{Time Neuroevolution}} in the {{NERO Video Game}}},
  author = {Stanley, Kenneth O. and Bryant, Bobby D. and Miikkulainen, Risto},
  year = {2005},
  journal = {IEEE Transactions on Evolutionary Computation (Special Issue on Evolutionary Computation and Games)},
  volume = {9},
  number = {6},
  abstract = {In most modern video games, character behavior is scripted; no matter how many times the player exploits a weakness, that weakness is never repaired. Yet if game characters could learn through interacting with the player, behavior could improve as the game is played, keeping it interesting. This paper introduces the real-time NeuroEvolution of Augmenting Topologies (rtNEAT) method for evolving increasingly complex artificial neural networks in real time, as a game is being played. The rtNEAT method allows agents to change and improve during the game. In fact, rtNEAT makes possible an entirely new genre of video games in which the player trains a team of agents through a series of customized exercises. To demonstrate this concept, the NeuroEvolving Robotic Operatives (NERO) game was built based on rtNEAT. In NERO, the player trains a team of virtual robots for combat against other players' teams. This paper describes results from this novel application of machine learning, and demonstrates that rtNEAT makes possible video games like NERO where agents evolve and adapt in real time. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games.}
}

@article{stanleyTaxonomyArtificialEmbryogeny2003,
  title = {A {{Taxonomy}} for {{Artificial Embryogeny}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2003},
  month = apr,
  journal = {Artificial Life},
  volume = {9},
  number = {2},
  pages = {93--130},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/106454603322221487},
  abstract = {A major challenge for evolutionary computation is to evolve phenotypes such as neural networks, sensory systems, or motor controllers at the same level of complexity as found in biological organisms. In order to meet this challenge, many researchers are proposing indirect encodings, that is, evolutionary mechanisms where the same genes are used multiple times in the process of building a phenotype. Such gene reuse allows compact representations of very complex phenotypes. Development is a natural choice for implementing indirect encodings, if only because nature itself uses this very process. Motivated by the development of embryos in nature, we define artificial embryogeny (AE) as the subdiscipline of evolutionary computation (EC) in which phenotypes undergo a developmental phase. An increasing number of AE systems are currently being developed, and a need has arisen for a principled approach to comparing and contrasting, and ultimately building, such systems. Thus, in this paper, we develop a principled taxonomy for AE. This taxonomy provides a unified context for long-term research in AE, so that implementation decisions can be compared and contrasted along known dimensions in the design space of embryogenic systems. It also allows predicting how the settings of various AE parameters affect the capacity to efficiently evolve complex phenotypes.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/L4MIF8R2/Stanley et Miikkulainen - 2003 - A Taxonomy for Artificial Embryogeny.pdf}
}

@article{stulpPathIntegralPolicy,
  title = {Path {{Integral Policy Improvement}} with {{Covariance Matrix Adaptation}}},
  author = {Stulp, Freek and Sigaud, Olivier},
  pages = {8},
  abstract = {There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family \textendash{} Cross-Entropy Methods and CMAES \textendash{} at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for ``Path Integral Policy Improvement with Covariance Matrix Adaptation''. PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/DDQAIF3J/Stulp et Sigaud - Path Integral Policy Improvement with Covariance M.pdf}
}

@article{suchDeepNeuroevolutionGenetic2017,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2017},
  journal = {CoRR},
  volume = {abs/1712.06567},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {\_eprint: 1712.06567},
  file = {/home/disc/p.templier/Zotero/storage/86HMLZQ5/Such et al. - 2017 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf;/home/disc/p.templier/Zotero/storage/Q7BTDJVP/Such et al. - 2018 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf;/home/disc/p.templier/Zotero/storage/PABB2YCT/1712.html}
}

@misc{sudhakaranGoalGuidedNeuralCellular2022,
  title = {Goal-{{Guided Neural Cellular Automata}}: {{Learning}} to {{Control Self-Organising Systems}}},
  shorttitle = {Goal-{{Guided Neural Cellular Automata}}},
  author = {Sudhakaran, Shyam and Najarro, Elias and Risi, Sebastian},
  year = {2022},
  month = apr,
  number = {arXiv:2205.06806},
  eprint = {2205.06806},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.06806},
  abstract = {Inspired by cellular growth and self-organization, Neural Cellular Automata (NCAs) have been capable of "growing" artificial cells into images, 3D structures, and even functional machines. NCAs are flexible and robust computational systems but -- similarly to many other self-organizing systems -- inherently uncontrollable during and after their growth process. We present an approach to control these type of systems called Goal-Guided Neural Cellular Automata (GoalNCA), which leverages goal encodings to control cell behavior dynamically at every step of cellular growth. This approach enables the NCA to continually change behavior, and in some cases, generalize its behavior to unseen scenarios. We also demonstrate the robustness of the NCA with its ability to preserve task performance, even when only a portion of cells receive goal information.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/YEBHYHCW/Sudhakaran et al. - 2022 - Goal-Guided Neural Cellular Automata Learning to .pdf;/home/disc/p.templier/Zotero/storage/QVDD7IV4/2205.html}
}

@article{suganumaGeneticProgrammingApproach2017,
  title = {A {{Genetic Programming Approach}} to {{Designing Convolutional Neural Network Architectures}}},
  author = {Suganuma, Masanori and Shirakawa, Shinichi and Nagao, Tomoharu},
  year = {2017},
  month = aug,
  journal = {arXiv:1704.00764 [cs]},
  eprint = {1704.00764},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The convolutional neural network (CNN), which is one of the deep learning models, has seen much success in a variety of computer vision tasks. However, designing CNN architectures still requires expert knowledge and a lot of trial and error. In this paper, we attempt to automatically construct CNN architectures for an image classification task based on Cartesian genetic programming (CGP). In our method, we adopt highly functional modules, such as convolutional blocks and tensor concatenation, as the node functions in CGP. The CNN structure and connectivity represented by the CGP encoding method are optimized to maximize the validation accuracy. To evaluate the proposed method, we constructed a CNN architecture for the image classification task with the CIFAR-10 dataset. The experimental result shows that the proposed method can be used to automatically find the competitive CNN architecture compared with state-of-the-art models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {cgp,Computer Science - Neural and Evolutionary Computing,nas},
  file = {/home/disc/p.templier/Zotero/storage/E6MDI86C/Suganuma et al. - 2017 - A Genetic Programming Approach to Designing Convol.pdf;/home/disc/p.templier/Zotero/storage/Y28CR5L5/Suganuma et al. - 2017 - A Genetic Programming Approach to Designing Convol.pdf}
}

@article{sunAutomaticallyDesigningCNN2020,
  title = {Automatically {{Designing CNN Architectures Using}} the {{Genetic Algorithm}} for {{Image Classification}}},
  author = {Sun, Y. and Xue, B. and Zhang, M. and Yen, G. G. and Lv, J.},
  year = {2020},
  month = sep,
  journal = {IEEE Transactions on Cybernetics},
  volume = {50},
  number = {9},
  pages = {3840--3854},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2020.2983860},
  abstract = {Convolutional neural networks (CNNs) have gained remarkable success on many image classification tasks in recent years. However, the performance of CNNs highly relies upon their architectures. For the most state-of-the-art CNNs, their architectures are often manually designed with expertise in both CNNs and the investigated problems. Therefore, it is difficult for users, who have no extended expertise in CNNs, to design optimal CNN architectures for their own image classification problems of interest. In this article, we propose an automatic CNN architecture design method by using genetic algorithms, to effectively address the image classification tasks. The most merit of the proposed algorithm remains in its ``automatic'' characteristic that users do not need domain knowledge of CNNs when using the proposed algorithm, while they can still obtain a promising CNN architecture for the given images. The proposed algorithm is validated on widely used benchmark image classification datasets, compared to the state-of-the-art peer competitors covering eight manually designed CNNs, seven automatic + manually tuning, and five automatic CNN architecture design algorithms. The experimental results indicate the proposed algorithm outperforms the existing automatic CNN architecture design algorithms in terms of classification accuracy, parameter numbers, and consumed computational resources. The proposed algorithm also shows the very comparable classification accuracy to the best one from manually designed and automatic + manually tuning CNNs, while consuming fewer computational resources.},
  keywords = {automatic CNN architecture design algorithms,automatic CNN architecture design method,benchmark image classification datasets,Computer architecture,convolutional neural nets,Convolutional neural networks (CNNs),Evolution (biology),Evolutionary computation,evolutionary deep learning,genetic algorithm,genetic algorithms,Genetic algorithms,genetic algorithms (GAs),Genetics,image classification,image classification tasks,learning (artificial intelligence),Manuals,neural-network architecture optimization,optimal CNN architectures,peer competitors,Tuning},
  file = {/home/disc/p.templier/Zotero/storage/A9KXYP9R/Sun et al. - 2020 - Automatically Designing CNN Architectures Using th.pdf;/home/disc/p.templier/Zotero/storage/QP5EJBRG/Sun et al. - 2020 - Automatically Designing CNN Architectures Using th.pdf;/home/disc/p.templier/Zotero/storage/SS4BZFY2/9075201.html;/home/disc/p.templier/Zotero/storage/XEJRYC37/9075201.html}
}

@inproceedings{sunEfficientNaturalEvolution2009,
  title = {Efficient Natural Evolution Strategies},
  booktitle = {Proceedings of the 11th {{Annual}} Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '09},
  author = {Sun, Yi and Wierstra, Daan and Schaul, Tom and Schmidhuber, Juergen},
  year = {2009},
  pages = {539},
  publisher = {{ACM Press}},
  address = {{Montreal, Qu\&\#233;bec, Canada}},
  doi = {10.1145/1569901.1569976},
  abstract = {Efficient Natural Evolution Strategies (eNES) is a novel alternative to conventional evolutionary algorithms, using the natural gradient to adapt the mutation distribution. Unlike previous methods based on natural gradients, eNES uses a fast algorithm to calculate the inverse of the exact Fisher information matrix, thus increasing both robustness and performance of its evolution gradient estimation, even in higher dimensions. Additional novel aspects of eNES include optimal fitness baselines and importance mixing (a procedure for updating the population with very few fitness evaluations). The algorithm yields competitive results on both unimodal and multimodal benchmarks.},
  isbn = {978-1-60558-325-9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/6VP6UWRX/Sun et al. - 2009 - Efficient natural evolution strategies.pdf}
}

@article{sunESDQNBasedVerticalHandoff2020,
  title = {{{ES-DQN-Based Vertical Handoff Algorithm}} for {{Heterogeneous Wireless Networks}}},
  author = {Sun, Jiani and Qian, Zhihong},
  year = {2020},
  journal = {IEEE WIRELESS COMMUNICATIONS LETTERS},
  volume = {9},
  number = {8},
  pages = {4},
  abstract = {One of the most challenging topics for next generation wireless networks is vertical handoff concept since several wireless technologies are supposed to cooperate. In this letter, evolution strategy and deep Q-network (ES-DQN) has been proposed to make vertical handoff simple and effective. The algorithm executes the deep Q-network (DQN) with the objective of maximizing the benefit of the system, and combined with evolution strategy (ES) algorithm with global optimization capability to set initial parameters of the back propagation (BP) network in order to improve the convergence speed and precision of parameter learning. The results show that the proposed method not only outperforms existing schemes with handoff latency and throughput, but also reduce handoff failure probability and call blocking probability.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/NBJHLKR7/Sun et Qian - 2020 - ES-DQN-Based Vertical Handoff Algorithm for Hetero.pdf}
}

@article{sunEvolvingDeepConvolutional2020,
  title = {Evolving {{Deep Convolutional Neural Networks}} for {{Image Classification}}},
  author = {Sun, Y. and Xue, B. and Zhang, M. and Yen, G. G.},
  year = {2020},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {24},
  number = {2},
  pages = {394--407},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2019.2916183},
  abstract = {Evolutionary paradigms have been successfully applied to neural network designs for two decades. Unfortunately, these methods cannot scale well to the modern deep neural networks due to the complicated architectures and large quantities of connection weights. In this paper, we propose a new method using genetic algorithms for evolving the architectures and connection weight initialization values of a deep convolutional neural network to address image classification problems. In the proposed algorithm, an efficient variable-length gene encoding strategy is designed to represent the different building blocks and the potentially optimal depth in convolutional neural networks. In addition, a new representation scheme is developed for effectively initializing connection weights of deep convolutional neural networks, which is expected to avoid networks getting stuck into local minimum that is typically a major issue in the backward gradient-based optimization. Furthermore, a novel fitness evaluation method is proposed to speed up the heuristic search with substantially less computational resource. The proposed algorithm is examined and compared with 22 existing algorithms on nine widely used image classification tasks, including the state-of-the-art methods. The experimental results demonstrate the remarkable superiority of the proposed algorithm over the state-of-the-art designs in terms of classification error rate and the number of parameters (weights).},
  keywords = {Architecture,backward gradient-based optimization,Computer architecture,connection weights,convolutional neural nets,Convolutional neural network (CNN),Convolutional neural networks,deep convolutional neural networks,deep learning,Encoding,evolutionary paradigms,genetic algorithms,Genetic algorithms,genetic algorithms (GAs),gradient methods,image classification,image classification problems,Optimization,Task analysis,variable-length gene encoding strategy},
  file = {/home/disc/p.templier/Zotero/storage/HEN7IWZP/Sun et al. - 2020 - Evolving Deep Convolutional Neural Networks for Im.pdf;/home/disc/p.templier/Zotero/storage/PJD4ZSG6/Sun et al. - 2020 - Evolving Deep Convolutional Neural Networks for Im.pdf;/home/disc/p.templier/Zotero/storage/XA7RADF9/8712430.html;/home/disc/p.templier/Zotero/storage/XY449VH3/8712430.html}
}

@misc{suriMaximumMutationReinforcement2021,
  title = {Maximum {{Mutation Reinforcement Learning}} for {{Scalable Control}}},
  author = {Suri, Karush and Shi, Xiao Qi and Plataniotis, Konstantinos N. and Lawryshyn, Yuri A.},
  year = {2021},
  month = jan,
  number = {arXiv:2007.13690},
  eprint = {2007.13690},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Advances in Reinforcement Learning (RL) have demonstrated data efficiency and optimal control over large state spaces at the cost of scalable performance. Genetic methods, on the other hand, provide scalability but depict hyperparameter sensitivity towards evolutionary operations. However, a combination of the two methods has recently demonstrated success in scaling RL agents to high-dimensional action spaces. Parallel to recent developments, we present the Evolutionbased Soft Actor-Critic (ESAC), a scalable RL algorithm. We abstract exploration from exploitation by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC). Through this lens, we enable dominant skill transfer between offsprings by making use of soft winner selections and genetic crossovers in hindsight and simultaneously improve hyperparameter sensitivity in evolutions using the novel Automatic Mutation Tuning (AMT). AMT gradually replaces the entropy framework of SAC allowing the population to succeed at the task while acting as randomly as possible, without making use of backpropagation updates. In a study of challenging locomotion tasks consisting of high-dimensional action spaces and sparse rewards, ESAC demonstrates improved performance and sample efficiency in comparison to the Maximum Entropy framework. Additionally, ESAC presents efficacious use of hardware resources and algorithm overhead. A complete implementation of ESAC can be found at karush17.github.io/esac-web/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/C2W4MV2Y/Suri et al. - 2021 - Maximum Mutation Reinforcement Learning for Scalab.pdf}
}

@inproceedings{tangNeuroevolutionSelfInterpretableAgents2020,
  title = {Neuroevolution of {{Self-Interpretable Agents}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Tang, Yujin and Nguyen, Duong and Ha, David},
  year = {2020},
  file = {/home/disc/p.templier/Zotero/storage/KLJZTAAG/Tang et al. - 2020 - Neuroevolution of self-interpretable agents.pdf}
}

@article{tangSensoryNeuronTransformer2021,
  title = {The {{Sensory Neuron}} as a {{Transformer}}: {{Permutation-Invariant Neural Networks}} for {{Reinforcement Learning}}},
  shorttitle = {The {{Sensory Neuron}} as a {{Transformer}}},
  author = {Tang, Yujin and Ha, David},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.02869 [cs]},
  eprint = {2109.02869},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/4SWTUVLP/Tang et Ha - 2021 - The Sensory Neuron as a Transformer Permutation-I.pdf;/home/disc/p.templier/Zotero/storage/L35SSWIF/Tang et Ha - 2021 - The Sensory Neuron as a Transformer Permutation-I.pdf;/home/disc/p.templier/Zotero/storage/D9UHWUFT/2109.html}
}

@inproceedings{taraporeHowDifferentEncodings2016,
  title = {How Do {{Different Encodings Influence}} the {{Performance}} of the {{MAP-Elites Algorithm}}?},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} 2016},
  author = {Tarapore, Danesh and Clune, Jeff and Cully, Antoine and Mouret, Jean-Baptiste},
  year = {2016},
  month = jul,
  pages = {173--180},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/2908812.2908875},
  abstract = {The recently introduced Intelligent Trial and Error algorithm (IT\&E) both improves the ability to automatically generate controllers that transfer to real robots, and enables robots to creatively adapt to damage in less than 2 minutes. A key component of IT\&E is a new evolutionary algorithm called MAP-Elites, which creates a behavior-performance map that is provided as a set of ``creative'' ideas to an online learning algorithm. To date, all experiments with MAPElites have been performed with a directly encoded list of parameters: it is therefore unknown how MAP-Elites would behave with more advanced encodings, like HyperNeat and SUPG. In addition, because we ultimately want robots that respond to their environments via sensors, we investigate the ability of MAP-Elites to evolve closed-loop controllers, which are more complicated, but also more powerful. Our results show that the encoding critically impacts the quality of the results of MAP-Elites, and that the differences are likely linked to the locality of the encoding (the likelihood of generating a similar behavior after a single mutation). Overall, these results improve our understanding of both the dynamics of the MAP-Elites algorithm and how to best harness MAP-Elites to evolve effective and adaptable robotic controllers.},
  isbn = {978-1-4503-4206-3},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/NYDZ3FDN/Tarapore et al. - 2016 - How do Different Encodings Influence the Performan.pdf;/home/disc/p.templier/Zotero/storage/SI9L9IZZ/Tarapore et al. - 2016 - How do Different Encodings Influence the Performan.pdf}
}

@article{templierGeometricEncodingNeural2021,
  title = {A {{Geometric Encoding}} for {{Neural Network Evolution}}},
  author = {Templier, Paul and Rachelson, Emmanuel and Wilson, Dennis G},
  year = {2021},
  pages = {9},
  abstract = {A major limitation to the optimization of artificial neural networks (ANN) with evolutionary methods lies in the high dimensionality of the search space, the number of weights growing quadratically with the size of the network. This leads to expensive training costs, especially in evolution strategies which rely on matrices whose sizes grow with the number of genes. We introduce a geometric encoding for neural network evolution (GENE) as a representation of ANN parameters in a smaller space that scales linearly with the number of neurons, allowing for efficient parameter search. Each neuron of the network is encoded as a point in a latent space and the weight of a connection between two neurons is computed as the distance between them. The coordinates of all neurons are then optimized with evolution strategies in a reduced search space while not limiting network fitness and possibly improving search.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/UJHRZLYE/Templier et al. - 2021 - A Geometric Encoding for Neural Network Evolution.pdf}
}

@misc{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@inproceedings{thierens1994convergence,
  title = {Convergence Models of Genetic Algorithm Selection Schemes},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  author = {Thierens, Dirk and Goldberg, David},
  year = {1994},
  pages = {119--129},
  organization = {{Springer}}
}

@article{tianModernEvolutionStrategies2021,
  title = {Modern {{Evolution Strategies}} for {{Creativity}}: {{Fitting Concrete Images}} and {{Abstract Concepts}}},
  shorttitle = {Modern {{Evolution Strategies}} for {{Creativity}}},
  author = {Tian, Yingtao and Ha, David},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.08857 [cs]},
  eprint = {2109.08857},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Evolutionary algorithms have been used in the digital art scene since the 1970s. A popular application of genetic algorithms is to optimize the procedural placement of vector graphic primitives to resemble a given painting. In recent years, deep learning-based approaches have also been proposed to generate procedural drawings, which can be optimized using gradient descent. In this work, we revisit the use of evolutionary algorithms for computational creativity. We find that modern evolution strategies (ES) algorithms, when tasked with the placement of shapes, offer large improvements in both quality and efficiency compared to traditional genetic algorithms, and even comparable to gradient-based methods. We demonstrate that ES is also well suited at optimizing the placement of shapes to fit the CLIP model, and can produce diverse, distinct geometric abstractions that are aligned with human interpretation of language. Videos and demo: https://es-clip.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/BZBEC98D/Tian et Ha - 2021 - Modern Evolution Strategies for Creativity Fittin.pdf;/home/disc/p.templier/Zotero/storage/F4XQJSPF/Tian et Ha - 2021 - Modern Evolution Strategies for Creativity Fittin.pdf;/home/disc/p.templier/Zotero/storage/R6L84FT8/Tian et Ha - 2021 - Modern Evolution Strategies for Creativity Fittin.pdf;/home/disc/p.templier/Zotero/storage/SZPLJFSR/Tian et Ha - 2021 - Modern Evolution Strategies for Creativity Fittin.pdf}
}

@inproceedings{todorov2012mujoco,
  title = {{{MuJoCo}}: {{A}} Physics Engine for Model-Based Control},
  booktitle = {2012 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  year = {2012},
  pages = {5026--5033},
  doi = {10.1109/IROS.2012.6386109},
  organization = {{IEEE}}
}

@inproceedings{togeliusSuperMarioEvolution2009,
  title = {Super Mario Evolution},
  booktitle = {2009 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Games}}},
  author = {Togelius, J. and Karakovskiy, S. and Koutnik, J. and Schmidhuber, J.},
  year = {2009},
  month = sep,
  pages = {156--161},
  issn = {2325-4289},
  doi = {10.1109/CIG.2009.5286481},
  abstract = {We introduce a new reinforcement learning benchmark based on the classic platform game Super Mario Bros. The benchmark has a high-dimensional input space, and achieving a good score requires sophisticated and varied strategies. However, it has tunable difficulty, and at the lowest difficulty setting decent score can be achieved using rudimentary strategies and a small fraction of the input space. To investigate the properties of the benchmark, we evolve neural network-based controllers using different network architectures and input spaces. We show that it is relatively easy to learn basic strategies capable of clearing individual levels of low difficulty, but that these controllers have problems with generalization to unseen levels and with taking larger parts of the input space into account. A number of directions worth exploring for learning better-performing strategies are discussed.},
  keywords = {Artificial intelligence,Automatic control,Benchmark testing,classic platform game,computer games,Games,Humans,input representation,Learning,learning (artificial intelligence),Multidimensional systems,network architectures,neural net architecture,neural network-based controllers,Neural networks,neuroevolution,Observability,Platform games,reinforcement learning benchmark,rudimentary strategy,State-space methods,Super Mario Bros},
  file = {/home/disc/p.templier/Zotero/storage/U25PNPNR/Togelius et al. - 2009 - Super mario evolution.pdf;/home/disc/p.templier/Zotero/storage/XY8XMUNH/5286481.html}
}

@article{toromanoffDeepReinforcementLearning,
  title = {Is {{Deep Reinforcement Learning Really Superhuman}} on {{Atari}}?},
  author = {Toromanoff, Marin and Wirbel, Emilie and Moutarde, Fabien},
  pages = {23},
  abstract = {Consistent and reproducible evaluation of Deep Reinforcement Learning (DRL) is not straightforward. In the Arcade Learning Environment (ALE), small changes in environment parameters such as stochasticity or the maximum allowed play time can lead to very different performance. In this work, we discuss the difficulties of comparing different agents trained on ALE. In order to take a step further towards reproducible and comparable DRL, we introduce SABER, a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Our methodology extends previous recommendations and contains a complete set of environment parameters as well as train and test procedures. We then use SABER to evaluate the current state of the art, Rainbow. Furthermore, we introduce a human world records baseline, and argue that previous claims of expert or superhuman performance of DRL might not be accurate. Finally, we propose Rainbow-IQN by extending Rainbow with Implicit Quantile Networks (IQN) leading to new stateof-the-art performance. Source code is available for reproducibility.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/SL9UZ2EE/Toromanoff et al. - Is Deep Reinforcement Learning Really Superhuman o.pdf}
}

@inproceedings{ulmer2003evolution,
  title = {Evolution Strategies Assisted by {{Gaussian}} Processes with Improved Preselection Criterion},
  booktitle = {The 2003 Congress on Evolutionary Computation, 2003. {{CEC}}'03.},
  author = {Ulmer, Holger and Streichert, Felix and Zell, Andreas},
  year = {2003},
  volume = {1},
  pages = {692--699},
  organization = {{IEEE}}
}

@article{valiant1984theory,
  title = {A Theory of the Learnable},
  author = {Valiant, Leslie G.},
  year = {1984},
  journal = {Communications of the ACM},
  volume = {27},
  number = {11},
  pages = {1134--1142},
  publisher = {{ACM New York, NY, USA}}
}

@article{vansteenkisteWaveletbasedEncodingNeuroevolution2016,
  title = {A {{Wavelet-based Encoding}} for {{Neuroevolution}}},
  author = {{van Steenkiste}, Sjoerd and Koutn{\'i}k, Jan and Driessens, Kurt and {Schmidhuber, Jurgen}},
  year = {2016},
  pages = {8},
  abstract = {A new indirect scheme for encoding neural network connection weights as sets of wavelet-domain coefficients is proposed in this paper. It exploits spatial regularities in the weight-space to reduce the gene-space dimension by considering the low-frequency wavelet coefficients only. The wavelet-based encoding builds on top of a frequency-domain encoding, but unlike when using a Fourier-type transform, it offers gene locality while preserving continuity of the genotypephenotype mapping. We argue that this added property allows for more efficient evolutionary search and demonstrate this on the octopus-arm control task, where superior solutions were found in fewer generations. The scalability of the wavelet-based encoding is shown by evolving networks with many parameters to control game-playing agents in the Arcade Learning Environment.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/3LJC9ZL8/van Steenkiste et al. - A Wavelet-based Encoding for Neuroevolution.pdf}
}

@article{vassiliadesDiscoveringEliteHypervolume2018,
  title = {Discovering the {{Elite Hypervolume}} by {{Leveraging Interspecies Correlation}}},
  author = {Vassiliades, Vassilis and Mouret, Jean-Baptiste},
  year = {2018},
  month = jul,
  journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
  eprint = {1804.03906},
  eprinttype = {arxiv},
  pages = {149--156},
  doi = {10.1145/3205455.3205602},
  abstract = {Evolution has produced an astonishing diversity of species, each filling a different niche. Algorithms like MAP-Elites mimic this divergent evolutionary process to find a set of behaviorally diverse but high-performing solutions, called the elites. Our key insight is that species in nature often share a surprisingly large part of their genome, in spite of occupying very different niches; similarly, the elites are likely to be concentrated in a specific ``elite hypervolume'' whose shape is defined by their common features. In this paper, we first introduce the elite hypervolume concept and propose two metrics to characterize it: the genotypic spread and the genotypic similarity. We then introduce a new variation operator, called ``directional variation'', that exploits interspecies (or inter-elites) correlations to accelerate the MAP-Elites algorithm. We demonstrate the effectiveness of this operator in three problems (a toy function, a redundant robotic arm, and a hexapod robot).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/KQ5273RE/Vassiliades et Mouret - 2018 - Discovering the Elite Hypervolume by Leveraging In.pdf}
}

@inproceedings{viswambaranEvolvingDeepRecurrent2020,
  title = {Evolving {{Deep Recurrent Neural Networks Using A New Variable-Length Genetic Algorithm}}},
  booktitle = {2020 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  author = {Viswambaran, R. A. and Chen, G. and Xue, B. and Nekooei, M.},
  year = {2020},
  month = jul,
  pages = {1--8},
  doi = {10.1109/CEC48606.2020.9185851},
  abstract = {Deep Recurrent Neural Network (DRNN) is an effective deep learning method with a wide variety of applications. Manually designing the architecture of a DRNN for any specific task requires expert knowledge and the optimal DRNN architecture can vary substantially for different tasks. This paper focuses on developing an algorithm to automatically evolve task-specific DRNN architectures by using a Genetic Algorithm (GA). A variable-length encoding strategy is developed to represent DRNNs of different depths because it is not possible to determine the required depth of a DRNN in advance. Activation functions play an important role in the performance of DRNNs and must be carefully used in these networks. Driven by this understanding, knowledge-driven crossover and mutation operators will be proposed to carefully control the use of activation functions in GA in order for the algorithm to evolve best performing DRNNs. Our algorithm focuses particularly on evolving DRNN architectures that use Long Short Term Memory (LSTM) units. As a leading type of DRNN, LSTM-based DRNN can effectively handle long-term dependencies, achieving cutting-edge performance while processing various sequential data. Three different types of publicly available benchmark datasets for both classification and regression tasks have been considered in our experiments. The obtained results show that the proposed variable-length GA can evolve DRNN architectures that significantly outperform many state-of-the-art systems on most of the datasets.},
  keywords = {activation functions,classification,Computer architecture,Deep Recurrent Neural Network,Deep Recurrent Neural Networks,effective deep learning method,Encoding,evolutionary computation,evolving deep recurrent neural networks,expert knowledge,Genetic Algorithm,genetic algorithms,Genetic algorithms,knowledge-driven crossover,learning (artificial intelligence),Long Short Term Memory,Long Short Term Memory units,LSTM-based DRNN,Machine learning,mutation operators,neural nets,neurocontrollers,optimal DRNN architecture,performing DRNNs,recurrent neural nets,Recurrent neural networks,regression tasks,required depth,Task analysis,task-specific DRNN architectures,Training,variable-length encoding strategy,variable-length GA,variable-length genetic algorithm},
  file = {/home/disc/p.templier/Zotero/storage/7ZFE64UW/Viswambaran et al. - 2020 - Evolving Deep Recurrent Neural Networks Using A Ne.pdf;/home/disc/p.templier/Zotero/storage/JW48XHYJ/9185851.html;/home/disc/p.templier/Zotero/storage/RPJMB5UE/9185851.html}
}

@article{walkerPhysicalNeuralCellular2022,
  title = {Physical {{Neural Cellular Automata}} for {{2D Shape Classification}}},
  author = {Walker, Kathryn and Palm, Rasmus Berg and Garcia, Rodrigo Moreno and Faina, Andres and Stoy, Kasper and Risi, Sebastian},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.07548 [cs]},
  eprint = {2203.07548},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Materials with the ability to self-classify their own shape have the potential to advance a wide range of engineering applications and industries. Biological systems possess the ability not only to self-reconfigure but also to self-classify themselves to determine a general shape and function. Previous work into modular robotics systems have only enabled self-recognition and self-reconfiguration into a specific target shape, missing the inherent robustness present in nature to self-classify. In this paper we therefore take advantage of recent advances in deep learning and neural cellular automata, and present a simple modular 2D robotic system that can infer its own class of shape through the local communication of its components. Furthermore, we show that our system can be successfully transferred to hardware which thus opens opportunities for future self-classifying machines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/home/disc/p.templier/Zotero/storage/KF89QAIZ/Walker et al. - 2022 - Physical Neural Cellular Automata for 2D Shape Cla.pdf;/home/disc/p.templier/Zotero/storage/KCED72SK/2203.html}
}

@article{wangBenchmarkingModelBasedReinforcement2019,
  title = {Benchmarking {{Model-Based Reinforcement Learning}}},
  author = {Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.02057 [cs, stat]},
  eprint = {1907.02057},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. However, research in model-based RL has not been very standardized. It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research, which are sometimes closed-sourced or not reproducible. Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other. To facilitate research in MBRL, in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL. We benchmark these algorithms with unified problem settings, including noisy environments. Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms. We characterize three key research challenges for future MBRL research: the dynamics bottleneck, the planning horizon dilemma, and the early-termination dilemma. Finally, to maximally facilitate future research on MBRL, we open-source our benchmark in http://www.cs.toronto.edu/\textasciitilde tingwuwang/mbrl.html.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/A8CHZSCI/Wang et al. - 2019 - Benchmarking Model-Based Reinforcement Learning.pdf}
}

@article{wangDuelingNetworkArchitectures2016,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {van Hasselt}, Hado and Lanctot, Marc and {de Freitas}, Nando},
  year = {2016},
  month = apr,
  journal = {arXiv:1511.06581 [cs]},
  eprint = {1511.06581},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/J8Z87M76/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf}
}

@article{wangEVOLUTIONARYDIVERSITYOPTIMIZATION2022,
  title = {{{EVOLUTIONARY DIVERSITY OPTIMIZATION WITH CLUSTERING-BASED SELECTION FOR REINFORCEMENT LEARNING}}},
  author = {Wang, Yutong and Xue, Ke and Qian, Chao},
  year = {2022},
  pages = {17},
  abstract = {Reinforcement Learning (RL) has achieved significant successes, which aims to obtain a single policy maximizing the expected cumulative rewards for a given task. However, in many real-world scenarios, e.g., navigating in complex environments and controlling robots, one may need to find a set of policies having both high rewards and diverse behaviors, which can bring better exploration and robust few-shot adaptation. Recently, some methods have been developed by using evolutionary techniques, including iterative reproduction and selection of policies. However, due to the inefficient selection mechanisms, these methods cannot fully guarantee both high quality and diversity. In this paper, we propose EDO-CS, a new Evolutionary Diversity Optimization algorithm with Clusteringbased Selection. In each iteration, the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. EDO-CS also adaptively balances the importance between quality and diversity in the reproduction process. Experiments on various (i.e., deceptive and multi-modal) continuous control tasks, show the superior performance of EDO-CS over previous methods, i.e., EDO-CS can achieve a set of policies with both high quality and diversity efficiently while previous methods cannot.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/6FPQRF7E/Wang et al. - 2022 - EVOLUTIONARY DIVERSITY OPTIMIZATION WITH CLUSTERIN.pdf}
}

@article{wangNeuralArchitectureSearchBasedMultiobjectiveCognitive2020,
  title = {Neural-{{Architecture-Search-Based Multiobjective Cognitive Automation System}}},
  author = {Wang, Eric Ke and Xu, Ship Peng and Chen, Chien-Ming and Kumar, Neeraj},
  year = {2020},
  journal = {IEEE Systems Journal},
  pages = {1--8},
  issn = {1937-9234},
  doi = {10.1109/JSYST.2020.3002428},
  abstract = {Currently, deep-learning-based cognitive automation for decision-making in industrial informatics is a new hot topic in the field of cognitive computing, among which multiobjective architecture optimization is of great difficulty in the research area. When the existing algorithms face multiobjective cognitive model problems, it often takes a lot of time to continuously set different search preference parameters to generate a new search process. This article mainly aims to solve the problem in a multiobjective neural architecture search process, and the key issue is how to adapt user preferences during architectural search. We propose a new algorithm: linear-prefer coevolutionary algorithm. Compared to the original user-constrained method and the Pareto-dominant NSGA-II algorithm, we have faster adaptation time and better quality of adaptation. At the same time, it can respond to user's needs at a relatively faster pace during the reasoning phase. Based on a large number of comparative test results, our algorithm is superior to the traditional cognitive automation algorithms for the multiobjective problem in search quality.},
  keywords = {Automation,Cognitive automation,Cognitive systems,Computer architecture,evolutional algorithm,Evolutionary computation,multiobjective,neural architecture search (NAS),Neural networks,Optimization,Pareto dominant,Search problems},
  file = {/home/disc/p.templier/Zotero/storage/UYVQXBLY/Wang et al. - 2020 - Neural-Architecture-Search-Based Multiobjective Co.pdf;/home/disc/p.templier/Zotero/storage/WY87NI4D/9127493.html}
}

@misc{wangSurrogateAssistedControllerExpensive2022,
  title = {A {{Surrogate-Assisted Controller}} for {{Expensive Evolutionary Reinforcement Learning}}},
  author = {Wang, Yuxing and Zhang, Tiantian and Chang, Yongzhe and Liang, Bin and Wang, Xueqian and Yuan, Bo},
  year = {2022},
  month = apr,
  number = {arXiv:2201.00129},
  eprint = {2201.00129},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The integration of Reinforcement Learning (RL) and Evolutionary Algorithms (EAs) aims at simultaneously exploiting the sample efficiency as well as the diversity and robustness of the two paradigms. Recently, hybrid learning frameworks based on this principle have achieved great success in various challenging tasks. However, in these methods, policies from the genetic population are evaluated via interactions with the real environments, limiting their applicability when such interactions are prohibitively costly. In this work, we propose Surrogate-assisted Controller (SC), a novel and efficient module that can be integrated into existing frameworks to alleviate the burden of EAs by partially replacing the expensive fitness evaluation. The key challenge in applying this module is to prevent the optimization process from being misled by the possible false minima introduced by the surrogate. To address this issue, we present two strategies for SC to control the workflow of hybrid frameworks. Experiments on six continuous control tasks from the OpenAI Gym platform show that SC can not only significantly reduce the cost of interacting with the environment, but also boost the performance of the original hybrid frameworks with collaborative learning and evolutionary processes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/XVESQ89Y/Wang et al. - 2022 - A Surrogate-Assisted Controller for Expensive Evol.pdf}
}

@article{whitelamCorrespondenceNeuroevolutionGradient2021,
  title = {Correspondence between Neuroevolution and Gradient Descent},
  author = {Whitelam, Stephen and Selin, Viktor and Park, Sang-Won and Tamblyn, Isaac},
  year = {2021},
  month = nov,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {6317},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-26568-2},
  abstract = {We show analytically that training a neural network by conditioned stochastic mutation or neuroevolution of its weights is equivalent, in the limit of small mutations, to gradient descent on the loss function in the presence of Gaussian white noise. Averaged over independent realizations of the learning process, neuroevolution is equivalent to gradient descent on the loss function. We use numerical simulation to show that this correspondence can be observed for finite mutations, for shallow and deep neural networks. Our results provide a connection between two families of neural-network training methods that are usually considered to be fundamentally different.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Statistical physics},
  file = {/home/disc/p.templier/Zotero/storage/59WH3T7G/Whitelam et al. - 2021 - Correspondence between neuroevolution and gradient.pdf}
}

@inproceedings{whitesonAutomaticFeatureSelection2005a,
  title = {Automatic Feature Selection in Neuroevolution},
  booktitle = {Proceedings of the 2005 Conference on {{Genetic}} and Evolutionary Computation  - {{GECCO}} '05},
  author = {Whiteson, Shimon and Stone, Peter and Stanley, Kenneth O. and Miikkulainen, Risto and Kohl, Nate},
  year = {2005},
  pages = {1225},
  publisher = {{ACM Press}},
  address = {{Washington DC, USA}},
  doi = {10.1145/1068009.1068210},
  abstract = {Feature selection is the process of finding the set of inputs to a machine learning algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to automate feature selection rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously, FS-NEAT addresses the feature selection problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the feature selection task it faces is made increasingly difficult.},
  isbn = {978-1-59593-010-1},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/MP4SGXHN/Whiteson et al. - 2005 - Automatic feature selection in neuroevolution.pdf}
}

@article{whitleyCellularEncodingApplied1970,
  title = {Cellular {{Encoding Applied}} to {{Neurocontrol}}},
  author = {Whitley, Darrell and Gruau, Fr{\'e}d{\'e}ric and Pyeatt, Larry},
  year = {1970},
  month = feb,
  abstract = {Neural networks are trained for balancing 1 and 2 poles attached to a cart on a fixed track. For one variant of the single pole system, only pole angle and cart position variables are supplied as inputs; the network must learn to compute velocities. All of the problems are solved using a fixed architecture and using a new version of cellular encoding that evolves an application specific architecture with real-valued weights. The learning times and generalization capabilities are compared for neural networks developed using both methods. After a post processing simplification, topologies produced by cellular encoding were very simple and could be analyzed. Architectures with no hidden units were produced for the single pole and the two pole problem when velocity information is supplied as an input. Moreover, these linear solutions display good generalization. For all the control problems, cellular encoding can automatically generate architectures whose complexity a...},
  file = {/home/disc/p.templier/Zotero/storage/TM2LP4SJ/Whitley et al. - 1970 - Cellular Encoding Applied to Neurocontrol.pdf}
}

@incollection{whitleyHybridGeneticAlgorithm2010,
  title = {A {{Hybrid Genetic Algorithm}} for the {{Traveling Salesman Problem Using Generalized Partition Crossover}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}}, {{PPSN XI}}},
  author = {Whitley, Darrell and Hains, Doug and Howe, Adele},
  editor = {Schaefer, Robert and Cotta, Carlos and Ko{\l}odziej, Joanna and Rudolph, G{\"u}nter},
  year = {2010},
  pages = {566--575},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15844-5_57},
  abstract = {We present a hybrid Genetic Algorithm that incorporates the Generalized Partition Crossover (GPX) operator to produce an algorithm that is competitive with the state of the art for the Traveling Salesman Problem (TSP). GPX is respectful, transmits alleles and is capable of tunneling directly to new local optima. Our results show that the Hybrid Genetic Algorithm quickly finds optimal and near optimal solution on problems ranging from 500 to 1817 cities using a population size of 10. It is also superior to Chained-LK given similar computational effort. Additional analysis shows that all the edges found in the globally optimal solution are present in a population after only a few generations in almost every run. Furthermore the number of unique edges in the population is also less than twice the problem size.},
  isbn = {978-3-642-15843-8 978-3-642-15844-5},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/92GJEGAP/Whitley et al. - 2010 - A Hybrid Genetic Algorithm for the Traveling Sales.pdf}
}

@inproceedings{wierstraNaturalEvolutionStrategies2008,
  title = {Natural {{Evolution Strategies}}},
  booktitle = {2008 {{IEEE Congress}} on {{Evolutionary Computation}} ({{IEEE World Congress}} on {{Computational Intelligence}})},
  author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Juergen},
  year = {2008},
  month = jun,
  pages = {3381--3387},
  publisher = {{IEEE}},
  address = {{Hong Kong, China}},
  doi = {10.1109/CEC.2008.4631255},
  abstract = {This paper presents Natural Evolution Strategies (NES), a novel algorithm for performing real-valued `black box' function optimization: optimizing an unknown objective function where algorithm-selected function measurements constitute the only information accessible to the method. Natural Evolution Strategies search the fitness landscape using a multivariate normal distribution with a self-adapting mutation matrix to generate correlated mutations in promising regions. NES shares this property with Covariance Matrix Adaption (CMA), an Evolution Strategy (ES) which has been shown to perform well on a variety of high-precision optimization tasks. The Natural Evolution Strategies algorithm, however, is simpler, less ad-hoc and more principled. Self-adaptation of the mutation matrix is derived using a Monte Carlo estimate of the natural gradient towards better expected fitness. By following the natural gradient instead of the `vanilla' gradient, we can ensure efficient update steps while preventing early convergence due to overly greedy updates, resulting in reduced sensitivity to local suboptima. We show NES has competitive performance with CMA on unimodal tasks, while outperforming it on several multimodal tasks that are rich in deceptive local optima.},
  isbn = {978-1-4244-1822-0},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/AJG2ACSF/Wierstra et al. - 2008 - Natural Evolution Strategies.pdf}
}

@inproceedings{wierstraNaturalEvolutionStrategies2014,
  title = {Natural {{Evolution Strategies}}},
  booktitle = {Journal of {{Machine Learning Research}}},
  author = {Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Juergen},
  year = {2014},
  publisher = {{IEEE}},
  doi = {10.1109/CEC.2008.4631255},
  abstract = {This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others.},
  isbn = {978-1-4244-1822-0},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/GSW2Q289/Wierstra et al. - 2014 - Natural Evolution Strategies.pdf}
}

@article{wilson_evolving_2018,
  title = {Evolving Simple Programs for Playing {{Atari}} Games},
  author = {Wilson, Dennis G. and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Miller, Julian F.},
  year = {2018},
  journal = {CoRR},
  volume = {abs/1806.05695}
}

@book{wilsonBERLLjBenchmarking,
  title = {{{BERL}}.Lj: {{Benchmarking Evolutionary Reinforcement Learning}}},
  author = {Wilson, Dennis}
}

@article{wilsonEvolvingDifferentiableGene2018,
  title = {Evolving {{Differentiable Gene Regulatory Networks}}},
  author = {Wilson, Dennis G. and Harrington, Kyle and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e}},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.05948 [cs]},
  eprint = {1807.05948},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Over the past twenty years, artificial Gene Regulatory Networks (GRNs) have shown their capacity to solve real-world problems in various domains such as agent control, signal processing and artificial life experiments. They have also benefited from new evolutionary approaches and improvements to dynamic which have increased their optimization efficiency. In this paper, we present an additional step toward their usability in machine learning applications. We detail an GPU-based implementation of differentiable GRNs, allowing for local optimization of GRN architectures with stochastic gradient descent (SGD). Using a standard machine learning dataset, we evaluate the ways in which evolution and SGD can be combined to further GRN optimization. We compare these approaches with neural network models trained by SGD and with support vector machines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/LCL647D7/Wilson et al. - 2018 - Evolving Differentiable Gene Regulatory Networks.pdf;/home/disc/p.templier/Zotero/storage/YYI4KNK6/1807.html}
}

@article{wilsonNeuromodulatedLearningDeep2018,
  title = {Neuromodulated {{Learning}} in {{Deep Neural Networks}}},
  author = {Wilson, Dennis G. and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Harrington, Kyle},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.03365 [cs, stat]},
  eprint = {1812.03365},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In the brain, learning signals change over time and synaptic location, and are applied based on the learning history at the synapse, in the complex process of neuromodulation. Learning in artificial neural networks, on the other hand, is shaped by hyper-parameters set before learning starts, which remain static throughout learning, and which are uniform for the entire network. In this work, we propose a method of deep artificial neuromodulation which applies the concepts of biological neuromodulation to stochastic gradient descent. Evolved neuromodulatory dynamics modify learning parameters at each layer in a deep neural network over the course of the network's training. We show that the same neuromodulatory dynamics can be applied to different models and can scale to new problems not encountered during evolution. Finally, we examine the evolved neuromodulation, showing that evolution found dynamic, location-specific learning strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/N5FETR84/Wilson et al. - 2018 - Neuromodulated Learning in Deep Neural Networks.pdf;/home/disc/p.templier/Zotero/storage/PT3PCBRZ/1812.html}
}

@book{wilsonPositionalCartesianGenetic2018,
  title = {Positional {{Cartesian Genetic Programming}}},
  author = {Wilson, D. G. and Miller, Julian F. and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e}},
  year = {2018},
  annotation = {\_eprint: 1810.04119}
}

@article{xinyaoEvolvingArtificialNeural1999,
  title = {Evolving Artificial Neural Networks},
  author = {{Xin Yao}},
  year = {1999},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {87},
  number = {9},
  pages = {1423--1447},
  issn = {1558-2256},
  doi = {10.1109/5.784219},
  abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone.},
  keywords = {Adaptive systems,Algorithm design and analysis,Artificial intelligence,Artificial neural networks,Competitive intelligence,Computer networks,connection weights,evolutionary algorithms,Evolutionary computation,genetic algorithms,Intelligent networks,intelligent systems,Intelligent systems,learning,learning (artificial intelligence),neural nets,neural networks,search operators,search problems,technological forecasting,Transfer functions},
  file = {/home/disc/p.templier/Zotero/storage/TN9ZTKG9/Xin Yao - 1999 - Evolving artificial neural networks.pdf}
}

@article{yamanLimitedEvaluationCooperative2018,
  title = {Limited {{Evaluation Cooperative Co-evolutionary Differential Evolution}} for {{Large-scale Neuroevolution}}},
  author = {Yaman, Anil and Mocanu, Decebal Constantin and Iacca, Giovanni and Fletcher, George and Pechenizkiy, Mykola},
  year = {2018},
  month = jul,
  journal = {Proceedings of the Genetic and Evolutionary Computation Conference},
  eprint = {1804.07234},
  eprinttype = {arxiv},
  pages = {569--576},
  doi = {10.1145/3205455.3205555},
  abstract = {Many real-world control and classification tasks involve a large number of features. When artificial neural networks (ANNs) are used for modeling these tasks, the network architectures tend to be large. Neuroevolution is an effective approach for optimizing ANNs; however, there are two bottlenecks that make their application challenging in case of high-dimensional networks using direct encoding. First, classic evolutionary algorithms tend not to scale well for searching large parameter spaces; second, the network evaluation over a large number of training instances is in general time-consuming. In this work, we propose an approach called the Limited Evaluation Cooperative Co-evolutionary Differential Evolution algorithm (LECCDE) to optimize high-dimensional ANNs. The proposed method aims to optimize the pre-synaptic weights of each post-synaptic neuron in different subpopulations using a Cooperative Co-evolutionary Differential Evolution algorithm, and employs a limited evaluation scheme where fitness evaluation is performed on a relatively small number of training instances based on fitness inheritance. We test LECCDE on three datasets with various sizes, and our results show that cooperative co-evolution significantly improves the test error comparing to standard Differential Evolution, while the limited evaluation scheme facilitates a significant reduction in computing time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/VU9PJK4H/Yaman et al. - 2018 - Limited Evaluation Cooperative Co-evolutionary Dif.pdf;/home/disc/p.templier/Zotero/storage/J8JYTKC4/1804.html}
}

@article{yangGeneralizedAlgorithmMultiObjective2019,
  title = {A {{Generalized Algorithm}} for {{Multi-Objective Reinforcement Learning}} and {{Policy Adaptation}}},
  author = {Yang, Runzhe and Sun, Xingyuan and Narasimhan, Karthik},
  year = {2019},
  month = nov,
  journal = {arXiv:1908.08342 [cs]},
  eprint = {1908.08342},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new algorithm for multi-objective reinforcement learning (MORL) with linear preferences, with the goal of enabling few-shot adaptation to new tasks. In MORL, the aim is to learn policies over multiple competing objectives whose relative importance (preferences) is unknown to the agent. While this alleviates dependence on scalar reward design, the expected return of a policy can change significantly with varying preferences, making it challenging to learn a single model to produce optimal policies under different preference conditions. We propose a generalized version of the Bellman equation to learn a single parametric representation for optimal policies over the space of all possible preferences. After an initial learning phase, our agent can execute the optimal policy under any given preference, or automatically infer an underlying preference with very few samples. Experiments across four different domains demonstrate the effectiveness of our approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/PZSNNYQF/Yang et al. - 2019 - A Generalized Algorithm for Multi-Objective Reinfo.pdf}
}

@misc{youngMinAtarAtariInspiredTestbed2019,
  title = {{{MinAtar}}: {{An Atari-Inspired Testbed}} for {{Thorough}} and {{Reproducible Reinforcement Learning Experiments}}},
  shorttitle = {{{MinAtar}}},
  author = {Young, Kenny and Tian, Tian},
  year = {2019},
  month = jun,
  number = {arXiv:1903.03176},
  eprint = {1903.03176},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1903.03176},
  abstract = {The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/disc/p.templier/Zotero/storage/U92JWKEL/Young et Tian - 2019 - MinAtar An Atari-Inspired Testbed for Thorough an.pdf;/home/disc/p.templier/Zotero/storage/AEIQH2QT/1903.html}
}

@inproceedings{zhongPracticalBlockWiseNeural2018,
  title = {Practical {{Block-Wise Neural Network Architecture Generation}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
  year = {2018},
  month = jun,
  pages = {2423--2432},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00257},
  abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54\% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/ES24HM49/Zhong et al. - 2018 - Practical Block-Wise Neural Network Architecture G.pdf}
}

@inproceedings{zhuEvolutionaryReinforcementLearning2021,
  title = {Evolutionary Reinforcement Learning for Sparse Rewards},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Zhu, Shibei and Belardinelli, Francesco and Le{\'o}n, Borja Gonz{\'a}lez},
  year = {2021},
  month = jul,
  pages = {1508--1512},
  publisher = {{ACM}},
  address = {{Lille France}},
  doi = {10.1145/3449726.3463142},
  abstract = {Temporal logic (TL) is an expressive way of specifying complex goals in reinforcement learning (RL), which facilitates the design of reward functions. However, the combination of these two techniques is prone to generate sparse rewards, which might hinder the learning process. Evolutionary algorithms (EAs) hold promise in tackling this problem by encouraging the diversification of policies through exploration in the parameter space. In this paper, we present \dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend{} \dbend\dbend\dbend\dbend\dbend\dbend, the first hybrid on-policy evolutionary-based algorithm that combines the advantages of gradient learning in deep RL with the exploration ability of evolutionary algorithms, in order to solve the sparse reward problem pertaining to TL specifications. We test our approach in a delayed reward scenario. Differently from previous baselines combining RL and TL, we show that \dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend{} \dbend\dbend\dbend\dbend\dbend\dbend{} is able to tackle complex TL specifications even in sparse-reward settings.},
  isbn = {978-1-4503-8351-6},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/QF7YSP2I/Zhu et al. - 2021 - Evolutionary reinforcement learning for sparse rew.pdf}
}

@article{zophLearningTransferableArchitectures,
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  pages = {14},
  abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the ``NASNet search space'') which enables transferability. In our experiments, we search for the best convolutional layer (or ``cell'') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a ``NASNet architecture''. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4\% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS \textendash{} a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  langid = {english},
  file = {/home/disc/p.templier/Zotero/storage/XY5YHYJF/Zoph et al. - Learning Transferable Architectures for Scalable I.pdf}
}

@article{zophNeuralArchitectureSearch2017,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  author = {Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = feb,
  journal = {arXiv:1611.01578 [cs]},
  eprint = {1611.01578},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/disc/p.templier/Zotero/storage/AWB3URYH/Zoph et Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf}
}

@article{zotero-213,
  type = {Article}
}


